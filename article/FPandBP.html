<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpeg">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.2',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: true,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="深度学习中的的前向传播（Forward Propagation）与反向传播（Back Propagation）的目的就是，寻找一组最优的参数，使得损失函数（Lost Function）或称为成本函数（Cost Function）获取到最小值。">
<meta name="keywords" content="Deep Learning,反向传播,Back Propagation">
<meta property="og:type" content="article">
<meta property="og:title" content="前向传播(Forward Propagation)与反向传播(Back Propagation)个人理解与总结">
<meta property="og:url" content="https://wangcong.net/article/FPandBP.html">
<meta property="og:site_name" content="旋律信">
<meta property="og:description" content="深度学习中的的前向传播（Forward Propagation）与反向传播（Back Propagation）的目的就是，寻找一组最优的参数，使得损失函数（Lost Function）或称为成本函数（Cost Function）获取到最小值。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-03-10T14:12:28.714Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="前向传播(Forward Propagation)与反向传播(Back Propagation)个人理解与总结">
<meta name="twitter:description" content="深度学习中的的前向传播（Forward Propagation）与反向传播（Back Propagation）的目的就是，寻找一组最优的参数，使得损失函数（Lost Function）或称为成本函数（Cost Function）获取到最小值。">

<link rel="canonical" href="https://wangcong.net/article/FPandBP.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>前向传播(Forward Propagation)与反向传播(Back Propagation)个人理解与总结 | 旋律信</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">旋律信</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">The value of knowledge lies not in possession, but in share.</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>时间轴</a>

  </li>
        <li class="menu-item menu-item-specialization">

    <a href="/specialization/" rel="section"><i class="fa fa-fw fa-university"></i>专题</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        
            
  <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>


      
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wangcong.net/article/FPandBP.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="旋律信">
      <meta itemprop="description" content="旋律信">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="旋律信">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          前向传播(Forward Propagation)与反向传播(Back Propagation)个人理解与总结
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-03 20:01:55" itemprop="dateCreated datePublished" datetime="2019-09-03T20:01:55+08:00">2019-09-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-03-10 22:12:28" itemprop="dateModified" datetime="2021-03-10T22:12:28+08:00">2021-03-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>
            </span>

          
            <span id="/article/FPandBP.html" class="post-meta-item leancloud_visitors" data-flag-title="前向传播(Forward Propagation)与反向传播(Back Propagation)个人理解与总结" title="热度">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">热度：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/article/FPandBP.html#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/article/FPandBP.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>深度学习中的的前向传播（Forward Propagation）与反向传播（Back Propagation）的目的就是，寻找一组最优的参数，使得损失函数（Lost Function）或称为成本函数（Cost Function）获取到最小值。</p>
<a id="more"></a>
<p>下图为一个多层的神经网络，本文则对此图例进行前向传播和反向传播的解析。</p>
<p><img alt="NeuralNetworks" data-src="https://img.wangcong.net/FPandBP01.png?watermark/4/text/d2FuZ2NvbmcubmV0/font/6buR5L2T/fontsize/400/fill/Z3JheQ==/dissolve/30/rotate/30/uw/180/uh/180/resize/1"></p>
<h3 id="前向传播（Forward-propagation）"><a href="#前向传播（Forward-propagation）" class="headerlink" title="前向传播（Forward propagation）"></a>前向传播（Forward propagation）</h3><p>单个数据前向传播的计算过程：（此处损失函数以平方差损失函数为例）</p>
<script type="math/tex; mode=display">
z^{[L]} = w^L a^{[L-1]} + b^{[L]}</script><script type="math/tex; mode=display">
a^{[L]}=\sigma(z^{[L]})</script><script type="math/tex; mode=display">
L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^2</script><p>多数据前向传播的计算过程：</p>
<script type="math/tex; mode=display">
Z^{[L]} = W^L A^{[L-1]} + b^{[L]}</script><script type="math/tex; mode=display">
A^{[L]}=\sigma(A^{[L]})</script><script type="math/tex; mode=display">
L(\hat{y}^{(i)},y^{(i)}) = \frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2</script><script type="math/tex; mode=display">
J(W, b)=\frac{1}{m}    \sum\limits_{i=1}^m{L(\hat{y}^{(i)},y^{(i)})}</script><p>其中：<script type="math/tex">\sigma(*)</script>为激活函数，<script type="math/tex">L(\hat{y}^{(i)},y^{(i)})</script>为Loss Function ，<script type="math/tex">J(W, b)</script>为Cost Function，<script type="math/tex">[L]</script>为神经网络的第L层，用大写字母表示相应参数的向量化表示。下同。</p>
<h3 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播(Back Propagation)"></a>反向传播(Back Propagation)</h3><p>单个数据反向传播的计算过程：</p>
<script type="math/tex; mode=display">
\mathrm{d}z^{[L]} = \mathrm{d}a^{[L]} \cdot g'(z^{[L]})</script><script type="math/tex; mode=display">
\mathrm{d}w^{[L]} = \mathrm{d}z^{[L]} a^{[L-1]}</script><script type="math/tex; mode=display">
\mathrm{d}b^{[L]} = \mathrm{d}z^{[L]}</script><script type="math/tex; mode=display">
\mathrm{d}a^{[L-1]} = w^{[L-1]T}\mathrm{d}z^{L}</script><p>多数据反向传播的计算过程：</p>
<script type="math/tex; mode=display">
\mathrm{d}Z^{[L]} = \mathrm{d}A^{[L]} \cdot g'(Z^{[L]})</script><script type="math/tex; mode=display">
\mathrm{d}W^{[L]} = \frac{1}{m}\mathrm{d}Z^{[L]} A^{[L-1]}</script><script type="math/tex; mode=display">
\mathrm{d}b^{[L]} = \frac{1}{m}np.sum(\mathrm{d}Z^{[L]}, axis=1, keepdim=True)</script><script type="math/tex; mode=display">
\mathrm{d}A^{[L-1]} = W^{[L-1]T}\mathrm{d}Z^{L}</script><p>权值更新：</p>
<script type="math/tex; mode=display">
W^{[L]}  := W - \alpha \mathrm{d}W^{[L]}</script><script type="math/tex; mode=display">
b^{[L]}  := b - \alpha \mathrm{d}b^{[L]}</script><h3 id="反向传播-Back-Propagation-与链式法则"><a href="#反向传播-Back-Propagation-与链式法则" class="headerlink" title="反向传播(Back Propagation)与链式法则"></a>反向传播(Back Propagation)与链式法则</h3><p>反向传播(BP, Back Propagation)算法是多层神经网络的训练中举足轻重的算法，其基于复合函数的链式法则，但在实际运算中的意义比链式法则要大的多。</p>
<p>以求$e=(a+b)\times(b+1)$的偏导为例。<br>为方便，在图中，引入了中间变量$c$，$d$。 其复合关系图如下所示：<br><img alt="这里写图片描述" data-src="https://img.wangcong.net/FPandBP02.jpg?watermark/4/text/d2FuZ2NvbmcubmV0/font/6buR5L2T/fontsize/400/fill/Z3JheQ==/dissolve/30/rotate/30/uw/180/uh/180/resize/1"></p>
<p>接着，分别求$\frac{\partial e}{\partial a}$，$\frac{\partial e}{\partial b}$的值。<br><img alt="这里写图片描述" data-src="https://img.wangcong.net/FPandBP03.jpg?watermark/4/text/d2FuZ2NvbmcubmV0/font/6buR5L2T/fontsize/400/fill/Z3JheQ==/dissolve/30/rotate/30/uw/180/uh/180/resize/1"></p>
<p>利用<strong>链式法则</strong>可得： </p>
<script type="math/tex; mode=display">
\frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial a}=2\times1=2</script><script type="math/tex; mode=display">
\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\cdot \frac{\partial d}{\partial b}=2\times1+3\times1=5</script><p>不难发现，$\frac{\partial e}{\partial a}$的值等于从 <strong>e -&gt; a</strong> 的路径上的偏导值的乘积，而$\frac{\partial e}{\partial b}$的值等于从  <strong>e -&gt; b</strong>  路径 <strong>e -&gt; c -&gt; b</strong>  上的偏导值的乘积加上路径 <strong>e -&gt; d -&gt; b</strong>  上的偏导值的乘积。可以注意到，这样做是十分耗时，因为很多路径被重复访问了。比如上图中， <strong>e -&gt;c -&gt; a</strong>和<strong>e -&gt; c -&gt; b</strong>  就都走了路径<strong>e -&gt; c</strong>  。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。</p>
<p>同样是利用链式法则，<strong>反向传播</strong>(BP, Back Propagation)算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。 梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到损失函数收敛。</p>
<ol>
<li><p>从最上层的节点 <strong>e</strong>开始，<strong>初始值为1</strong>，以层为单位进行处理。</p>
</li>
<li><p>对于e的下一层的所有子节点，将1乘以<strong>e</strong>到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。</p>
<p>如，对于子节点<strong>c</strong>，<script type="math/tex">1\cdot\frac{\partial e}{\partial c}=1\times2=2</script></p>
<p>如，对于子节点<strong>d</strong>，<script type="math/tex">1\cdot\frac{\partial e}{\partial d}=1\times3=3</script></p>
</li>
<li><p>然后将第二层的节点各自作为起始顶点，<strong>初始值设为第2步分别得到的偏导值</strong>，以”层”为单位重复上述传播过程，即可求出顶点e对每一层某个节点的偏导值。</p>
<p>如，对于子节点<strong>a</strong>，<script type="math/tex">2\cdot\frac{\partial c}{\partial a}=2\times1=2</script></p>
<p>如，对于子节点<strong>b</strong>，<script type="math/tex">1\cdot\frac{\partial c}{\partial b}+3\times\frac{\partial d}{\partial b}=2\times1+3\times1=5</script></p>
</li>
</ol>
<p>由上述过程可以发现，BP算法很好的解决了重复计算的问题。</p>
<h3 id="前向传播及反向传播具体计算过程示例"><a href="#前向传播及反向传播具体计算过程示例" class="headerlink" title="前向传播及反向传播具体计算过程示例"></a>前向传播及反向传播具体计算过程示例</h3><p>下图为一个三层神经网络结构，分别输入层（第0层），隐藏层（第1层），输出层（第2层）：</p>
<p><img alt="CNNSimple" data-src="https://img.wangcong.net/FPandBP04.png?watermark/4/text/d2FuZ2NvbmcubmV0/font/6buR5L2T/fontsize/400/fill/Z3JheQ==/dissolve/30/rotate/30/uw/180/uh/180/resize/1"></p>
<p>整个输入、各层参数矩阵、输出，表示如下：</p>
<script type="math/tex; mode=display">
输入：X=
\left[
\begin{matrix}
x_1\\
x_2\\
\end{matrix}
\right]
=\left[
\begin{matrix}
0.8\\
0.3\\
\end{matrix}
\right]</script><script type="math/tex; mode=display">
隐藏层权值参数：
W_{1,0}=
\left[
\begin{matrix}
w_{31}&w_{32}\\
w_{41}&w_{42}\\
\end{matrix}
\right]
=\left[
\begin{matrix}
0.3 & 0.5\\
0.6 & 0.4\\
\end{matrix}
\right]</script><script type="math/tex; mode=display">
隐藏层的偏差项：
B_1=
\left[
\begin{matrix}
b_3\\
b_4\\
\end{matrix}
\right]
=\left[
\begin{matrix}
0.02\\
0.03\\
\end{matrix}
\right]</script><script type="math/tex; mode=display">
输出层权值参数：
W_{2,1}=
\left[
\begin{matrix}
w_{53}&w_{54}\\
\end{matrix}
\right]
=\left[
\begin{matrix}
0.2 & 0.7\\
\end{matrix}
\right]</script><script type="math/tex; mode=display">
输出层的偏差项：
B_2=
\left[
\begin{matrix}
b_5\\
\end{matrix}
\right]
=\left[
\begin{matrix}
0.03\\
\end{matrix}
\right]</script><script type="math/tex; mode=display">
目标值：
y_{out}=0.5</script><h4 id="对于前向传播"><a href="#对于前向传播" class="headerlink" title="对于前向传播"></a>对于前向传播</h4><p>第1层的输出为：</p>
<script type="math/tex; mode=display">
\begin {aligned}
Z_{layer1}
&=\left[
\begin{matrix}
z_3\\
z_4\\
\end{matrix}
\right]
=W_{1,0}\cdot X + B_1
=\left[
\begin{matrix}
w_{31}&w_{32}\\
w_{41}&w_{42}\\
\end{matrix}
\right]
\cdot
\left[
\begin{matrix}
x_1\\
x_2\\
\end{matrix}
\right]
+\left[
\begin{matrix}
b_3\\
b_4\\
\end{matrix}
\right]\\
&=
\left[
\begin{matrix}
w_{31}\times x_1 + w_{32}\times x_2 + b_3\\
w_{41}\times x_1 + w_{42}\times x_2 + b_4\\
\end{matrix}
\right]
=\left[
\begin{matrix}
0.3\times 0.8 + 0.5\times0.3 + 0.02\\
0.6\times 0.8 + 0.4\times0.3 + 0.03\\
\end{matrix}
\right]\\
&=\left[
\begin{matrix}
0.41\\
0.63\\
\end{matrix}
\right]
\end {aligned}</script><p>经激活函数处理后的输出为：</p>
<script type="math/tex; mode=display">
\begin {aligned}
Y_{layer1}
&=\left[
\begin{matrix}
y_3\\
y_4\\
\end{matrix}
\right]
=f(Z_{layer1})
=f(\left[
\begin{matrix}
z_3\\
z_4\\
\end{matrix}
\right])\\
&=f(\left[
\begin{matrix}
0.41\\
0.63\\
\end{matrix}
\right])\\
&=
\left[
\begin{matrix}
0.601\\
0.652\\
\end{matrix}
\right]
\end {aligned}</script><p>同理，对与第2层的输出、经过经激活函数后的输出分别为：</p>
<script type="math/tex; mode=display">
\begin {aligned}
Z_{layer2}
&=W_{2,1}\cdot Y_{layer1}+B_2
=\left[
\begin{matrix}
w_{53} & w_{54}\\
\end{matrix}
\right]
\cdot
\left[
\begin{matrix}
y_3\\
y_4\\
\end{matrix}
\right]+b_5\\
&=\left[
\begin{matrix}
w_{53}\times y_3 + w_{54}\times y_4 + b_5\\
\end{matrix}
\right]
=0.601\times0.2+0.652\times0.7 + 0.03
\\
&=0.6066
\end {aligned}</script><script type="math/tex; mode=display">
\begin {aligned}
Y_{layer2}
&=f(Z_{layer2})
=f(W_{2,1}\cdot Y_{layer1})
=f(\left[
\begin{matrix}
w_{53} & w_{54}\\
\end{matrix}
\right]
\cdot
\left[
\begin{matrix}
y_3\\
y_4\\
\end{matrix}
\right])\\
&=f(0.6066)\\
&=0.647
\end {aligned}</script><p>可得最终的损失函数的值为：（为下文方便表示，<script type="math/tex">F_{loss}=L(Y_{layer2},y_{out})</script>）</p>
<script type="math/tex; mode=display">
\begin {aligned}
F_{loss}&=\frac{1}{2}(Y_{layer2}-y_{out})^2\\
&=\frac{1}{2}(0.647-0.5)^2\\
&=0.0108
\end {aligned}</script><p>优化的目标则是通过训练，调节参数，来使得损失函数的值越小越好。反向传播（BP，Back Propagation）算法，便是利用梯度下降法来求得使损失函数达到最小值得一种方法。</p>
<h4 id="对于反向传播"><a href="#对于反向传播" class="headerlink" title="对于反向传播"></a>对于反向传播</h4><p>首先对于<strong>第2层</strong>而言：</p>
<script type="math/tex; mode=display">
\begin{cases}
F_{loss}=\frac{1}{2}(Y_{layer2}-y_{out})^2\\
Y_{layer2}=f(Z_{layer2})\\
Z_{layer2}=W_{2,1}\cdot Y_{layer1}=w_{53}\times y_3 + w_{54}\times y_4 + b_5\\
\end{cases}</script><p>求解$\frac{\partial F_{loss}}{\partial w_{53}}$，则根据反向传播(BP, Back Propagation)算法可得:</p>
<script type="math/tex; mode=display">
\begin {aligned}
\frac{\partial F_{loss}}{\partial w_{53}}
&=\frac{\partial F_{loss}}{\partial Y_{layer2}}
\cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}}
\cdot \frac{\partial Z_{layer2}}{\partial w_{53}}\\
&=(Y_{layer2}-y_{out})\times f(Z_{layer2})\times(1-f(Z_{layer2}))\times y_3\\
&=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.601\\
&=0.0202
\end {aligned}</script><p>其中Sigmod函数导数为：</p>
<script type="math/tex; mode=display">
\begin {align}
f(x)=&\frac{1}{1+e ^{-x}}\\
\frac{df}{dx}=&-(\frac{1}{1+e ^{-x}})^2\times(-e^{-x})\\
=&f(x)\times(1-f(x))\\
因此，\frac{\partial F_{loss}}{\partial Y_{layer2}}=&f(Z{_layer2})\times(1-f(Z{_layer2}))
\end {align}</script><p><strong>（注：不同激活函数求导后的结果是不同的，在BP过程中所谓的梯度消失，梯度爆炸与激活函数的不恰当有很大关系！）</strong></p>
<p>求解$\frac{\partial F_{loss}}{\partial w_{54}}$，根据则根据反向传播(BP, Back Propagation)算法可得：</p>
<script type="math/tex; mode=display">
\begin {aligned}
\frac{\partial F_{loss}}{\partial w_{54}}
&=\frac{\partial F_{loss}}{\partial Y_{layer2}}
\cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}}
\cdot \frac{\partial Z_{layer2}}{\partial w_{53}}\\
&=(Y_{layer2}-y_{out})\times f(Z{_layer2})\times(1-f(Z{_layer2}))\times y_4\\
&=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.652\\
&=0.0219
\end {aligned}</script><p>求解$\frac{\partial F_{loss}}{\partial b_5}$，根据则根据反向传播(BP, Back Propagation)算法可得：</p>
<script type="math/tex; mode=display">
\begin {aligned}
\frac{\partial F_{loss}}{\partial b_{5}}
&=\frac{\partial F_{loss}}{\partial Y_{layer2}}
\cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}}
\cdot \frac{\partial Z_{layer2}}{\partial b_5}\\
&=(Y_{layer2}-y_{out})\times f(Z{_layer2})\times(1-f(Z{_layer2}))\times 1\\
&=(0.647-0.5)\times(0.647)\times(1-0.647)\times1\\
&=0.0336
\end {aligned}</script><p>进一步的，我们计算<strong>第1层</strong>的参数：</p>
<script type="math/tex; mode=display">
\begin{cases}
F_{loss}=\frac{1}{2}(Y_{layer2}-y_{out})^2\\
Y_{layer2}=f(Z_{layer2})\\
Z_{layer2}=W_{2,1}\cdot Y_{layer1}=w_{53}\times y_3 + w_{54}\times y_4 + b_5\\
Y_{layer1}
=\left[
\begin{matrix}
y_3\\
y_4\\
\end{matrix}
\right]
=f(Z_{layer1})
=f(\left[
\begin{matrix}
z_3\\
z_4\\
\end{matrix}
\right])\\\\
Z_{layer1}
=\left[
\begin{matrix}
z_3\\
z_4\\
\end{matrix}
\right]
=W_{1,0}\cdot X + B_1
=\left[
\begin{matrix}
w_{31}\times x_1 + w_{32}\times x_2 + b_3\\
w_{41}\times x_1 + w_{42}\times x_2 + b_4\\
\end{matrix}
\right]
\end{cases}</script><p>求解$\frac{\partial F_{loss}}{\partial w_{31}}$，则根据反向传播(BP, Back Propagation)算法可得:</p>
<script type="math/tex; mode=display">
\begin {aligned}
\frac{\partial F_{loss}}{\partial w_{31}}
&=\frac{\partial F_{loss}}{\partial Y_{layer2}}
\cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}}
\cdot \frac{\partial Z_{layer2}}{\partial y_3}
\cdot \frac{\partial y_3}{\partial z_3}
\cdot \frac{\partial z_3}{\partial w_{31}}
\\
&=(Y_{layer2}-y_{out})\times f(Z_{layer2})\times(1-f(Z_{layer2}))\times w_{53}\times f(Z_{layer1})\times(1-f(Z_{layer1}))\times x_1 \\
&=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.2\times(0.601)\times(1-0.601)\times(0.8)\\
&=0.001288
\end {aligned}</script><p>同理可得到其它几个参数权值：$\frac{\partial F_{loss}}{\partial w_{32}}$，$\frac{\partial F_{loss}}{\partial w_{41}}$，$\frac{\partial F_{loss}}{\partial w_{42}}$。</p>
<p>求解$\frac{\partial F_{loss}}{\partial b_3}$，则根据反向传播(BP, Back Propagation)算法可得:</p>
<script type="math/tex; mode=display">
\begin {aligned}
\frac{\partial F_{loss}}{\partial b_3}
&=\frac{\partial F_{loss}}{\partial Y_{layer2}}
\cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}}
\cdot \frac{\partial Z_{layer2}}{\partial y_3}
\cdot \frac{\partial y_3}{\partial z_3}
\cdot \frac{\partial z_3}{\partial b_3}
\\
&=(Y_{layer2}-y_{out})\times f(Z_{layer2})\times(1-f(Z_{layer2}))\times w_{53}\times f(Z_{layer1})\times(1-f(Z_{layer1}))\times 1 \\
&=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.2\times(0.601)\times(1-0.601)\times1\\
&=0.00161
\end {aligned}</script><p><strong>这里重点说明一下，一个函数在某一点的导数描述了这个函数在这一点附近的变化率。$ {\partial F_{loss}}/{\partial  w_{31}}$描述了$ F_{loss}$在$ w_{31}$该点变化率(或切线斜率)，则有：</strong></p>
<script type="math/tex; mode=display">
\begin{cases}
   F_{loss}在w_{31}处为递增 &\mbox{if $\frac{\partial F_{loss}}{\partial w_{31}}>0$ }\\
   F_{loss}在w_{31}处为递减 &\mbox{if $\frac{\partial F_{loss}}{\partial w_{31}}<0$}
   \end{cases}</script><p><strong>通过搜索方向和步长来对参数进行更新，其中搜索方向是目标函数在当前位置的负梯度方向，因为这个方向是最快的下降方向。</strong></p>
<p>则第1层的各权值更新为：</p>
<script type="math/tex; mode=display">
\begin{cases}
w_{31}=w_{31}-\frac{\partial F_{loss}}{\partial w_{31}}=0.298\\
w_{32}=w_{32}-\frac{\partial F_{loss}}{\partial w_{32}}=0.494\\
w_{41}=w_{41}-\frac{\partial F_{loss}}{\partial w_{41}}=0.598\\
w_{42}=w_{42}-\frac{\partial F_{loss}}{\partial w_{42}}=0.394\\
\end{cases}</script><p>按照更新后的权重参数再进行一次正向传播得出来的损失函数的值为：$F_{loss}=0.0099$</p>
<p>而这个值比第一次迭代时的<code>0.0108</code>更小，如果继续迭代，则会不断修正各层的权值参数，使得损失函数越来越小，预测值不断逼近 <code>0.5</code>。</p>
<p>经过迭代了100次后，$F_{loss}=7.09786073e-07$（<strong>已经很小很小很小了，说明预测值与真实值非常接近</strong>），最后的权值为：</p>
<script type="math/tex; mode=display">
W_{1,0}=
\left[
\begin{matrix}
0.30508134 & 0.40828582\\
0.60508134 & 0.30828582\\
\end{matrix}
\right]</script><script type="math/tex; mode=display">
W_{2,1}
=\left[
\begin{matrix}
-0.2601328 & 0.20000184\\
\end{matrix}
\right]</script><p>到此，整个前向传播（Forward propagation）与反向传播（Back Propagation）的过程可能差不多就是这样了。</p>
<p>上面实现的python代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nonlin</span><span class="params">(x, deriv=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (deriv == <span class="keyword">True</span>):</span><br><span class="line">        <span class="keyword">return</span> x * (<span class="number">1</span> - x) <span class="comment">#如果deriv为true，求导数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入层</span></span><br><span class="line">X = np.array([[<span class="number">0.8</span>],[<span class="number">0.3</span>]])</span><br><span class="line"><span class="comment"># 输出值</span></span><br><span class="line">y = np.array([[<span class="number">0.5</span>]])</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">W0 = np.array([[<span class="number">0.3</span>,<span class="number">0.5</span>],[<span class="number">0.6</span>,<span class="number">0.4</span>]])</span><br><span class="line">W1 = np.array([[<span class="number">0.2</span>,<span class="number">0.7</span>]])</span><br><span class="line">B1 = np.array([[<span class="number">0.02</span>,<span class="number">0.03</span>]])</span><br><span class="line">B2 = np.array([[<span class="number">0.03</span>]])</span><br><span class="line"><span class="comment"># print('original: ', '\n', W0, '\n',W1)</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 输入层（第0层）</span></span><br><span class="line">    l0 = X</span><br><span class="line">    <span class="comment"># 隐藏层（第1层）</span></span><br><span class="line">    l1 = nonlin(np.dot(W0,l0) + B1.T)</span><br><span class="line">    print(<span class="string">"l1:"</span>,<span class="string">'\n'</span>, l1)</span><br><span class="line">    <span class="comment"># 输出层（第2层）</span></span><br><span class="line">    l2 = nonlin(np.dot(W1,l1) + B2.T)</span><br><span class="line">    print(<span class="string">"l2:"</span>,<span class="string">'\n'</span>, l2)</span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    l2_error = y - l2</span><br><span class="line">    Error = <span class="number">1</span>/<span class="number">2.0</span>*(y-l2)**<span class="number">2</span></span><br><span class="line">    print(<span class="string">"Error:"</span>,<span class="string">'\n'</span>, Error)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    l2_delta = l2_error * nonlin(l2, deriv=<span class="keyword">True</span>)</span><br><span class="line">    l1_error = l2_delta*W1</span><br><span class="line">    l1_delta = l1_error * nonlin(l1, deriv=<span class="keyword">True</span>)</span><br><span class="line">    <span class="comment"># 更新权值</span></span><br><span class="line">    W1 += l2_delta*l1.T; </span><br><span class="line">    W0 += l0.T.dot(l1_delta)</span><br><span class="line">    print(<span class="string">'W0:'</span>, <span class="string">'\n'</span>, W0, <span class="string">'\n'</span>,<span class="string">'W1:'</span>, <span class="string">'\n'</span>,W1)</span><br></pre></td></tr></table></figure>
<hr>
<p>本文参考了以下文献：</p>
<p>[1]. <a href="https://www.zhihu.com/question/27239198/answer/89853077" target="_blank" rel="noopener">如何直观地解释 backpropagation 算法</a></p>
<p>[2]. <a href="https://zhuanlan.zhihu.com/p/24801814" target="_blank" rel="noopener">通俗理解神经网络BP传播算法</a></p>

    </div>

    
    
    
      
        <div class="reward-container">
  <div><p style="color:#34495e; margin:0 0 5px 0;">🍭支持一根棒棒糖吧！</p></div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    <i class="fa fa-qrcode fa-2x" style="line-height:35px;"></i>
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="https://img.wangcong.net/wechat_reward.png" alt="旋律信 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="https://img.wangcong.net/alipay_reward.png" alt="旋律信 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>旋律信
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://wangcong.net/article/FPandBP.html" title="前向传播(Forward Propagation)与反向传播(Back Propagation)个人理解与总结">https://wangcong.net/article/FPandBP.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a>
              <a href="/tags/反向传播/" rel="tag"><i class="fa fa-tag"></i> 反向传播</a>
              <a href="/tags/Back-Propagation/" rel="tag"><i class="fa fa-tag"></i> Back Propagation</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/article/ConvolutionalLayer.html" rel="next" title="图解卷积神经网络中二维与三维图像卷积操作">
                  <i class="fa fa-chevron-left"></i> 图解卷积神经网络中二维与三维图像卷积操作
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/article/GraduateLife.html" rel="prev" title="研究生生涯记事">
                  研究生生涯记事 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播（Forward-propagation）"><span class="nav-number">1.</span> <span class="nav-text">前向传播（Forward propagation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播-Back-Propagation"><span class="nav-number">2.</span> <span class="nav-text">反向传播(Back Propagation)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播-Back-Propagation-与链式法则"><span class="nav-number">3.</span> <span class="nav-text">反向传播(Back Propagation)与链式法则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播及反向传播具体计算过程示例"><span class="nav-number">4.</span> <span class="nav-text">前向传播及反向传播具体计算过程示例</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#对于前向传播"><span class="nav-number">4.1.</span> <span class="nav-text">对于前向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对于反向传播"><span class="nav-number">4.2.</span> <span class="nav-text">对于反向传播</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <img class="site-author-image" itemprop="image" alt="旋律信"
    src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">旋律信</p>
  <div class="site-description" itemprop="description">旋律信</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">97</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">106</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/cong" title="GitHub &rarr; https://github.com/cong" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/xuanlvxin" title="Twitter &rarr; https://twitter.com/xuanlvxin" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/" title="知乎 &rarr; https://www.zhihu.com/" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>知乎</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/qimenlupai" title="微博 &rarr; https://weibo.com/qimenlupai" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>微博</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://nacl.ai" title="https://nacl.ai" rel="noopener" target="_blank">NaCl.AI</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.qimenlupai.com" title="http://www.qimenlupai.com" rel="noopener" target="_blank">齐门鲁派</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/wu-qi-18-39" title="https://www.zhihu.com/people/wu-qi-18-39" rel="noopener" target="_blank">吴二狗</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">旋律信</span>
</div>
  <div class="beian"><a href="http://www.beian.miit.gov.cn" rel="noopener" target="_blank">闽ICP备 15006828号-2 </a>
  </div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






  <script>
  function leancloudSelector(url) {
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = visitors.getAttribute('id').trim();
      var title = visitors.getAttribute('data-flag-title').trim();

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .then(() => {
                leancloudSelector(url).innerText = counter.time + 1;
              })
              .catch(error => {
                console.log('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.log('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return element.getAttribute('id').trim();
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            leancloudSelector(url).innerText = time;
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.log('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=QtAanVyY1lpC3B2SHpz0flrW-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'QtAanVyY1lpC3B2SHpz0flrW-gzGzoHsz',
            'X-LC-Key': 'DfQofItPxzAfDtSv0TeyYJ0A',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        const localhost = /http:\/\/(localhost|127.0.0.1|0.0.0.0)/;
        if (localhost.test(document.URL)) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });
  </script>


  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=66482463";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


<script>
NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: true,
    appId: 'QtAanVyY1lpC3B2SHpz0flrW-gzGzoHsz',
    appKey: 'DfQofItPxzAfDtSv0TeyYJ0A',
    placeholder: "ヾﾉ≧∀≦)o 欢迎交流!   请输入您的昵称和邮箱，以便您收到回复时能及时得到通知...",
    avatar: 'wavatar',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname,
    recordIP: true,
    serverURLs: ''
  });
}, window.Valine);
</script>

<script src="/js/comment-locate.js"></script>
<script src="//cdn.jsdelivr.net/npm/minigrid@3.1.1/dist/minigrid.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
