<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[云服务器部署Python Web环境(Flask+Nginx+Gunicorn+Supervisor)]]></title>
    <url>%2Farticle%2FFlaskNginxGunicorn.html</url>
    <content type="text"><![CDATA[Flask 是一个使用 Python 编写的轻量级Web 应用框架。Flask的目的是要建立一个非常稳定和可靠的Web应用的基础系统，我们可以使用Flack再加上各种插件，扩展和其他模块，能够构建功能强大的网站和应用。根据2019年的使用情况数据可看出，主流 Python Web 框架包括：django、Flask、Tornado、Web2Py 等，web.py由于其原作者的仙逝，近些年表现出颓疲的状态，令人惋惜不已。 本教程在云服务器上安装了WSGI (Web Server Gateway Interface )接口 及 Http Sever (如 Apache、Nginx等)。 Python Web环境 -&gt; Flask+ Nginx + Gunicorn + Supervisor Flask是Web框架、Nginx 提供对外的服务、Gunicorn为WSGI服务器、Supervisor来守护进程。 注：本教程在Python3.x版本运行，安装可参考：Python3.x的安装及相关设置 此前基于云服务器部署Python Web环境(Nginx+Spawn-fcgi+web.py)的教程可以按需参阅。 安装VirtualEnv Ubuntu下安装VirtualEnv： 1sudo pip install virtualenv 创建venv文件夹，并安装虚拟环境： 12345678# 创建文件夹，或者进入自己的项目文件夹mkdir project# 进入该文件夹cd project# 安装虚拟环境virtualenv venv# 会提示如下的信息&gt;&gt; created virtual environment CPython3.7.5.final.0-64 in 546ms ... 激活虚拟环境： 123# 激活虚拟环境source venv/bin/activate# 退出虚拟环境：deactivate 启用虚拟环境后，命令行状态进入虚拟环境的状态，如下： 1(venv) ~/project# 安装Flask通用安装方法： 在激活的虚拟环境中安装Flask 12# 在激活的虚拟环境中安装Flask(venv) ~/project$ pip install Flask 测试Flask，新建hello.py内容如下： 12345678910111213#!/usr/bin/env python# -*- coding: utf-8 -*-from flask import Flask app = Flask(__name__) @app.route('/hello')def hello(): return 'Hello World!' if __name__ == "__main__": app.run() # 启动应用 运行该代码： 12python hello.py# 提示：* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) 另开一个终端，输入curl 127.0.0.1:5000，其中端口号与上方一致，显示结果为： Hello, World! 修改Flask的默认端口： 1env FLASK_APP=hello.py flask run -p 5001 安装Nginx Ubuntu下可使用下述命令： 1sudo apt-get install nginx 安装好的文件位置： /usr/sbin/nginx：主程序 /etc/nginx：存放配置文件 /usr/share/nginx：存放静态文件 /var/log/nginx：存放日志 其实从上面的根目录文件夹可以知道，Linux系统的配置文件一般放在/etc，日志一般放在/var/log，运行的程序一般放在/usr/sbin或者/usr/bin。 如果要更清楚Nginx的具体配置项，可以打开/etc/nginx/nginx.conf 对于其他Linux系统可参照官网链接：https://www.nginx.com/resources/wiki/start/topics/tutorials/install/ 启动Nginx： 12# 启动Nginxsudo /etc/init.d/nginx start 其他相关命令： 1234# 关闭Nginxsudo /etc/init.d/nginx stop# 重启Nginxsudo /etc/init.d/nginx restart 访问xx.xx.xx.xx（其中xx.xx.xx.xx更换为云服务器的公网IP），显示结果为： Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org.Commercial support is available at nginx.com. Thank you for using nginx. 安装Gunicorn 安装Gunicorn 1pip install gunicorn 配置 nginx.conf 支持 Gunicorn 12# 编辑配置文件vim /etc/nginx/nginx.conf 在http内如下之处： 1234http&#123; server&#123;&#125;&#125; 添加以下代码： 12345678910111213141516171819server&#123; listen 80; server_name xx.xx.xx.xx; # 其中xx.xx.xx.xx更换为云服务器的公网IP index index.html index.php; location /hello &#123; proxy_pass http://127.0.0.1:9001; # 把请求通过gunicorn传送给本机的9001端口 proxy_redirect off; proxy_set_header Host $host:80; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; location /static/ &#123; # 配置静态文件的访问 if (-f $request_filename) &#123; # 如果请求文件名是一个文件 rewrite ^/static/(.*)$ /static/$1 break; # 直接跳转到对应的资源，中断fastcgi的传输 &#125; &#125;&#125; 特别注意：【此处尤为重要，一开始始终无法正常显示，我花费了一下午的时间才解决相关问题】 在云服务器【安全组】的入站规则和出站规则分别添加80、90、50端口； 重启Nginx：sudo /etc/init.d/nginx restart； nginx.conf中proxy_pass http://127.0.0.1:5000端口号可以更改，但是需要保持与启动Gunicorn进程中的相同，同事注意修改端口号后要重启Nginx。 启动应用 启动一个Gunicorn进程: 12# 启动进程gunicorn -D -w 5 -b 127.0.0.1:9001 hello:app -D: 后台运行-w: 代表启动5个进程(worker)，可以通过ps -ef | grep 9001 可以看到四个PID； -b: 打标绑定的IP和端口号,0.0.0.1表示不仅仅能在本台机器上访问，外网也可以访问，绑定的为9001端口 main:app**, wsgi代表文件名，app为对应到该文件中创建的Flask对象此外还有其他参数: -log-level LEVEL:表示日志级别，测试可以用DEBUG -timeout: 超时时间，单位是秒 —access-logfile access.log：访问日志 —error-logfile error.log：错误日志 注：可以随意填写地址和端口信息，但是一定需要和Nginx配置文件相匹配。 启动Nginx 12# 启动Nginxsudo /etc/init.d/nginx start 查看9001端口是否存在 1netstat -ano |grep 9001 若存在则显示： tcp 0 0 127.0.0.1:9001 0.0.0.0:* LISTEN off (0.00/0/0) 访问xx.xx.xx.xx/hello（其中xx.xx.xx.xx云服务器的公网IP），显示结果为： Hello, world! 关闭/重启应用 查看Gunicorn进程树： 1pstree -ap|grep gunicorn 显示如下： 12345678910111213|-gunicorn,24610 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:9001 hello:app| |-gunicorn,24613 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:9001 hello:app| |-gunicorn,24614 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:9001 hello:app| |-gunicorn,24615 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:9001 hello:app| |-gunicorn,24616 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:9001 hello:app| `-gunicorn,24622 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:9001 hello:app|-gunicorn,25266 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:5000 hello:app| |-gunicorn,25269 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:5000 hello:app| |-gunicorn,25270 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:5000 hello:app| |-gunicorn,25271 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:5000 hello:app| |-gunicorn,25272 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:5000 hello:app| `-gunicorn,25273 /root/cong/venv/bin/gunicorn -D -w 5 -b 127.0.0.1:5000 hello:app| | |-grep,26544 --color=auto gunicorn 退出一个Gunicorn进程: 12# 如退出上方的24610进程kill -9 24610 重启进程： 12# 如重启上方的25266进程kill -HUP 25266 安装 SupervisorSupervisor是一个使用Python开发的进程管理程序 安装supervisor： 1pip install supervisor 生成配置文件 123456# 创建 /etc/supervisor 文件夹mkdir /etc/supervisor# 生成 supervisord.conf 文件echo_supervisord_conf &gt; /etc/supervisor/supervisord.conf# 为supervisord.conf增加执行权限chmod +x /etc/supervisor/supervisord.conf 修改supervisord.conf最后的[include]部分配置： 12[include]files = /etc/supervisor/conf.d/*.conf 这样就可以支持子配置文件，而不用改动主配置文件。 在/etc/supervisor/conf.d/新增子进程配置文件 hello.conf： 1234567# 创建 /etc/supervisor/conf.d 文件夹cd /etc/supervisormkdir conf.d# 创建hello.conf 文件cd /etc/supervisor/conf.d# 为hello.conf增加执行权限chomod +x hello.conf 创建 hello.conf配置文件如下： 12345678[program:hello]command=/root/cong/venv/bin/gunicorn -w 5 -b 127.0.0.1:9001 hello:appdirectory=/root/cong //项目目录user=rootautorestart=true //设置自动重启startretires=3 //重启失败3次[supervisord] 在上面的配置文件中，[program:hello]设置了进程名，这与之后操作进程的状态名称有关，为hello; command为进程运行的命令，必须使用绝对路径，并且使用虚拟环境下的gunicorn命令； user指定了运行进程的用户，这里设置为root 注意hello.conf中不要有汉字 启动这个demo进程： 1supervisord -c /etc/supervisor/conf.d/hello.conf 停止进程 1234# 找到supervisord进程编号ps -ef | grep supervisord# kiil进程kiil -9 24918 因为在虚拟环境中安装了Supervisor，我的supervisorctl无法使用，在虚拟环境之外也另外安装了Supervisor，解决了部分问题。总之Supervisor对Python3.x支持效果不好。 导出pip安装包记录将虚拟环境中安装的包记录到requirements.txt中： 12345678910pip freeze &gt; requirements.txt# 如下所示：click==7.1.2Flask==1.1.2gunicorn==20.0.4itsdangerous==1.1.0Jinja2==2.11.2MarkupSafe==1.1.1supervisor==4.2.0Werkzeug==1.0.1 当该项目被迁移到其他机器时，直接运行（或新建一个虚拟环境，再运行该命令）： 12# 安装所需要的包pip install -r requirements.txt DebugBug 1: 出现 1The 'supervisor==3.3.1' distribution was not found and is required by the application 解决方法: 通过apt-get安装的supervisor，实际上是按照Python2.x版本中。但由于我把ubuntu的python默认环境改成3.7，在Python3使用supervisor其实是借用Python2的环境，所以通过apt-get安装的supervisor显然不可行。 直接用Python3.x版本通过pip安装，会提示： 1Supervisor requires Python 2.4 or later but does not work on any version of Python 3. You are using version 3.7.5 (default, Nov 7 2019, 10:50:52) 安装上文中通过pip进行安装，应该不会出现此问题。 Bug 2: 出现 12Error: .ini file does not include supervisord sectionFor help, use /root/cong/venv/bin/supervisord -h 意思是少了 [supervisrod] 配置项，严格按照上方的流程不会出现此问题，可以参考 [supervisord-section-settings增添其他配置选项。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Gunicorn</tag>
        <tag>Flask</tag>
        <tag>Supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下终端升级（iTem2 + oh-my-zsh + powerlevel9k）]]></title>
    <url>%2Farticle%2FiTem2Setting.html</url>
    <content type="text"><![CDATA[作为一个一直在路上追求艺术和高效的汉子，为了在使用命令行的过程中让自己更加赏心悦目，我又来折腾我的终端了。 目标的效果如下： 为什么选择ITem2 功能强大，譬如搜索功能强大，搜索出来的匹配字符高亮显示； 分屏功能强大，一定程度上可以代替tmux（自认为iTerm2的自带分屏功能比 tmux 香🤦‍♂️）； 可以配置的更加优雅、艺术。 安装iTem2 首先在官网 http://iterm2.com/ 下载 iTem2 。 将iTem2设置为默认终端：iTerm2 -&gt; Make iTerm2 Default Term 导入iTem2配色一般我所有的软件的配色都是设置为Solarized，iTem2已经内置了，但感觉差点。 为实现文章开头中的效果，则需要导入Material Design配色，下载后通过Preferences &gt; Profiles &gt; Colors &gt; Color Presets &gt; Import进行设定。 在iTerm2 Color Schemes上面，有很多配色可以选择，可按照自己的喜欢进行下载。 注：重新打开iTem2窗口则会生效。 安装oh-my-zsh安装命令： 1sh -c "$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)" 但是在我这报错，我就直接将install.sh下载下来，然后sudo sh install.sh进行了安装。 执行完以后如果没有出现什么报错，在Home文件夹下生成了 ~/.oh-my-zsh文件夹就代表成功了。 安装oh-my-zsh的插件 安装语法高亮插件 1git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting 安装自动补全插件 1git clone https://github.com/zsh-users/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions 安装字体因为我们要用的主题会用到很多的特殊icon，所以iTerm2 选用的字体必需要支援这种特殊icon font。 没有安装的话会现在如下这样，遇到icon会变框框问号： 安装完字体后的效果： 点击Meslo LG L DZ Regular Nerd Font Complete 下载字体，双击进行安装。 然后通过Preferences &gt; Profiles &gt; Text进行设定。 安装zsh powerlevel9k 主题1git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k 修改zsh配置为了让powerlevel9k主题生效，需要修改~/.zshrc 文件，修改主题为powerlevel9k： vim ~/.zshrc123456789101112131415161718192021222324252627282930313233343536373839404142434445export ZSH="$HOME/.oh-my-zsh"ZSH_THEME="powerlevel9k/powerlevel9k"# Nerd 字体POWERLEVEL9K_MODE="nerdfont-complete"POWERLEVEL9K_PROMPT_ON_NEWLINE=true# command line左边想显示的内容：系统icon、用户、写权限、路径、版本控制系统等POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(os_icon user dir_writable dir vcs)# command line右边想显示的内容：状态、命令执行时间、...、系统时间、已用空间、RAM占用POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status command_execution_time root_indicator background_jobs time disk_usage ram)#POWERLEVEL9K_MULTILINE_LAST_PROMPT_PREFIX="%(?:%&#123;$fg_bold[green]%&#125;➜ :%&#123;$fg_bold[red]%&#125;➜ )"#POWERLEVEL9K_MULTILINE_FIRST_PROMPT_PREFIX=""#POWERLEVEL9K_USER_ICON="\uF415" # POWERLEVEL9K_ROOT_ICON="\uF09C"#POWERLEVEL9K_SUDO_ICON=$'\uF09C' # POWERLEVEL9K_TIME_FORMAT="%D&#123;%H:%M&#125;"#POWERLEVEL9K_VCS_GIT_ICON='\uF408 '#POWERLEVEL9K_VCS_GIT_GITHUB_ICON='\uF408 'ZSH_DISABLE_COMPFIX=trueENABLE_CORRECTION="true"COMPLETION_WAITING_DOTS="true"# zsh插件plugins=( git iterm2 macports man osx python composer zsh-syntax-highlighting zsh-autosuggestions)source $ZSH/oh-my-zsh.shalias suroot='sudo -E -s'# source ~/.bash_profileif [ -f ~/.bash_profile ]; then . ~/.bash_profile;fi 最终就可以实现了这种显示效果： 另外，此处Show Off Your Config.有更多的参数配置，大家可以尽情欣赏。 iTerm2 常用快捷键12345678910⌘ + 数字 ： 各 tab 标签切换⌘ + f ： 查找 ，所查找的内容会被自动复制 ,输入查找的部分字符，找到匹配的值按tab，即可复制⌘ + d ： 横着分屏 ⌘ + shift + d ： 竖着分屏⌘ + r = clear ： 换到新一屏，而不是 类似clear ，会创建一个空屏ctrl + u ：清空当前行，无论光标在什么位置() + ⌘ + ; ： [() 输入的命令开头字符],会自动列出输入过的命令⌘ + shift + h ： 会列出剪切板历史⌘← / ⌘→ : 到一行命令最左边/最右边 ⌘ + enter : 全屏]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>iTem2</tag>
        <tag>oh-my-zsh</tag>
        <tag>powerlevel9k</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Pycharm来远程调用GPU服务器资源]]></title>
    <url>%2Farticle%2FPycharmWithSeverGPU.html</url>
    <content type="text"><![CDATA[一直没有使用Pycharm来写代码的习惯，以往都是习惯了用Sublime Text来写代码，然后通过Transmit (FTP工具)上传到服务器上，在使用SecureCRT运行脚本。 为方便通过断点来参看数据并调试代码，遂考虑使用Pycharm来远程调用云服务器GPU来调试本地代码，一是因为本地Mac本显卡实在无力运行此高计算量的程序，二是怕自己的本本爆掉。 Pycharm有以下几个便利之处：(也是在不断折腾中发现的，🤦‍♂️) 一款专注于Python开发的IDE 远程链接服务器并调用GPU等资源 可以SSH链接到服务器终端 十分方便对服务器上的程序绘图进行显示 新建项目 通过SFTP链接服务器 点击Configuration进入Deployment，并新建SFTP Root path，我填写的是：/root/wangcong 点击Mapping，填写期望同步的本地路径与服务器路径 Deployment path，我填写的是：/ 尤其要注意服务器映射的目录路径!：实际路径 = Root path + Deployment path 在上方菜单栏进入Tools —&gt; Browse Remote Host来打开相应的RemoteHost面板。 这个面板显示的就是服务器上的文件，显示的范围就是步骤2中Deployment path路径下的文件及文件夹。 可以直接在RemoteHost面板里双击某个文件并且直接进行编辑。 双击某个文件后你可以看到编辑区域的顶部有一个横条，并且横条的右边有三各按钮，分别是比较、撤销和上传操作。在这里面编辑文件之后，可以直接点击上传按钮，就会提交到服务器了。 更改Python解释器为云服务器上的Python解释器 进入Preference，点击Project Interpreter，选择Add 进入SSH Interpreter进行设置 注意，有时候会出现Next无法点击的情况。 点击Create copy of this deployment server in IDE settings后，即可选中Next按钮。 Python解释器路径，一般设置默认 设置服务器GPU环境变量一次设置，永久有效！ 在终端上执行cat .bashrc命令，查询服务器中设置的CUDA环境变量。 12345678#export PATH=/usr/local/cuda-10.0/bin:$PATH#export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64$LD_LIBRARY_PATH#export CUDA_HOME=/usr/local/cuda-10.0上面是.bashrc文件中的配置，下方是Pycharm中的环境变量配置LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64CUDA_HOME=/usr/local/cuda-10.0 我仅仅设置了这两条，设置PATH会报错。 点击Edit Configurations 点击 Templates，设置Python脚本模板的Environment Variables 若已有运行过的文件，在完成上面两步后，记得清空Python下所有已经运行的文件，重新运行程序 测试在Pycharm IDE的右下角的Project Interpreter选为远程SSH的Python解释器。 新建脚本文件 testGPU.py123456789101112131415161718192021#!/usr/bin/python# -*- coding: UTF-8 -*-# @Time : 2019-12-16 17:49# @Author : WangCongimport tensorflow as tfimport osos.environ["CUDA_VISIBLE_DEVICES"] = '0'# Creates a graph.a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')c = tf.matmul(a, b)# Creates a session with log_device_placement set to True.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))# Runs the op.print(sess.run(c)) 运行脚本测试 通过Pycharm绘图启用 SSH X11 转发以便使得Pycharm能够进行绘图（Plot），打开配置文件/etc/ssh/sshd_config shell1# X11Forwarding yes 找到上面的代码，并取消注释。 SSH远程链接通过Pycharm可以建立服务器SSH远程链接，在上方菜单栏点击Tools —&gt; Start SSH seesion即可远程连接到服务器。 若远程Terminal出现乱码：进入Perferrnces点击Tools —&gt; SSH Terminal，在Default encoding选项中选择utf-8即可解决。 需要注意的问题 报错ImportError: libcublas.so.: cannot open shared object file: No such file or directory 在脚本文件的Environment Variables中，配置CUDA参数 上文中的新建项目路径与Mapping中的本地路径等路径要一一对应，服务器中的路径也要一一对应。 在Pycharm IDE的右下角的Project Interpreter应选为远程的Python解释器。 参考文献： https://stackoverflow.com/questions/37933890/tensorflow-gpu-setup-error-with-cuda-on-pycharm]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Pycharm</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyTorch 快速教程]]></title>
    <url>%2Farticle%2FPyTorchTutorial.html</url>
    <content type="text"><![CDATA[PyTorch是一个基于Torch的Python开源机器学习库，由Facebook的人工智能研究小组开发，可替代Numpy实现GPU加速的张量计算。 对比PyTorch和Tensorflow 没有好的框架，只有合适的框架， 这篇知乎文章有个简单的对比，所以这里就不详细再说了。 并且技术是发展的，知乎上的对比也不是绝对的，比如Tensorflow在1.5版的时候就引入了Eager Execution机制实现了动态图，PyTorch的可视化,windows支持，沿维翻转张量等问题都已经不是问题了。 一句话，PyTorch是一个相当简洁优雅且高效快速的框架！ 快速入门：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import torchfrom torch.nn import Linear, Module, MSELossfrom torch.optim import SGDimport numpy as npimport pandas as pdimport matplotlibimport matplotlib.pyplot as pltx = np.random.rand(256)noise = np.random.randn(256) / 4y = x * 5 + 7 + noise# 模型：Linearmodel = Linear(1, 1)# 损失函数：MSELossloss_fn = MSELoss()# 学习率learning_rate = 1e-2# 优化器: SGDoptimizer = SGD(model.parameters(), lr = learning_rate)# 训练的次数epochs = 3000# 训练前，数据预处理x_train = x.reshape(-1, 1).astype('float32')y_train = y.reshape(-1, 1).astype('float32')# 训练模型for i in range(epochs): # 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型 inputs = torch.from_numpy(x_train) labels = torch.from_numpy(y_train) #使用模型进行预测 outputs = model(inputs) # 计算损失 loss = loss_fn(outputs, labels) #梯度置0，否则会累加 optimizer.zero_grad() # 反向传播 loss.backward() # 使用优化器默认方法优化 optimizer.step() if (i%100==0): #每 100次打印一下损失函数，看看效果 print('epoch &#123;&#125;, loss &#123;:1.4f&#125;'.format(i,loss.data.item()))# 对数据进行预测predicted = model.forward(torch.from_numpy(x_train)).data.numpy() 项目示例经过一段时间的项目开发，关于深度学习的目录构建推荐如下： 12345678910.├── datasets│ └── xxxx //数据集├── models│ ├── model-weights.h5 //保存的模型│ └── pretrain_model│ └── keras.h5 //预训练模型├── modle.py //神经网络结构├── train.py //训练的脚本└── test.py //测试的脚本 关于model.py脚本内容示例如下 (不保证运行，下同)： model.py12345678910111213141516171819202122232425262728293031323334353637# usr/bin/env python# -*- coding:utf-8 -*-import torch.nn as nnclass LeNet5(nn.Module): def __init__(self): super(LeNet5, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) # 这里论文上写的是conv,官方教程用了线性层 self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresif __name__ == '__main__': net = LeNet5() print(net) 关于train.py脚本内容示例如下： train.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# usr/bin/env python#-*- coding:utf-8 -*-import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transforms# import the neural networkfrom . import modelclass Train(model, device, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) # Forward pass output = model(data) loss = F.nll_loss(output, target) optimizer.zero_grad() # Backward and optimize loss.backward() optimizer.step() if(batch_idx+1)%30 == 0: print('Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))if __name__ == '__main__': # batch size BATCH_SIZE = 512 # 总共训练批次 EPOCHS = 20 # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多 DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu") model = model.LeNet5().to(DEVICE) optimizer = optim.Adam(model.parameters()) # 加载训练数据 train_loader = torch.utils.data.DataLoader( datasets.MNIST('data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=BATCH_SIZE, shuffle=True) # Training for epoch in range(1, EPOCHS + 1): Train(model, DEVICE, train_loader, optimizer, epoch) # Saving model torch.save(model.state_dict(), 'model.ckpt') test.py脚本内容示例如下： test.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546# usr/bin/env python#-*- coding:utf-8 -*-import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transforms# Import the neural networkfrom . import modelclass Test(model, device, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加 pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标 correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n'.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) if __name__ == '__main__': # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多 DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu") model = model.LeNet5().to(DEVICE) # 加载测试函数 test_loader = torch.utils.data.DataLoader( datasets.MNIST('data', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=BATCH_SIZE, shuffle=True) # Load model model.load_state_dict(torch.load('params.ckpt')) # Testing for epoch in range(1, EPOCHS + 1): Test(model, DEVICE, test_loader)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于TensorFlow、Keras及PyTorch的训练过程可视化方法]]></title>
    <url>%2Farticle%2FDLFrameworkVisualization.html</url>
    <content type="text"><![CDATA[通过可视化训练过程，我们可以对神经网络训练更为方便的理解、调试与优化。本文将分别介绍三种主流的神经网络框架 (TensorFlow、PyTorch及Keras) 中的可视化方法。 TensorFlow训练过程可视化Tensorboard是tensorflow内置的一个可视化工具，它通过将tensorflow程序输出的日志文件的信息可视化使得tensorflow程序的理解、调试和优化更加简单高效。 Tensorboard的可视化依赖于tensorflow程序运行输出的日志文件，因而tensorboard和tensorflow程序在不同的进程中运行。 TensorBoard给我们提供了极其方便而强大的可视化环境。它可以帮助我们理解整个神经网络的学习过程、数据的分布、性能瓶颈等等。 官方介绍：https://tensorflow.google.cn/guide/summaries_and_tensorboard 查看自己训练过程中的loss变化以及参数的变化过程，以及自己图运算的流程。 查看损失 12# 查看损失tf.summary.scalar('scalar_loss', loss) 查看参数的变化 12tf.summary.histogram('weights',w)tf.summary.histogram('bias',b) 保存 123456merged_summary = tf.summary.merge_all()# 得到输出到文件的对象writer = tf.summary.FileWriter('./result', sess.graph)for... summary=sess.run(merged_summary) writer.add_summary(summary, step) 命令输入 1tensorboard --logdir ./result/ 打开google浏览器,输入：http://localhost:6006 查看计算图 关于可视化的很好的总结：https://www.jianshu.com/p/bea7fc33cbf4 代码示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# -- encoding:utf-8 --import numpy as npimport tensorflow as tfimport sys# 1. 构造一个数据np.random.seed(28)N = 100x = np.linspace(0, 6, N) + np.random.normal(loc=0.0, scale=2, size=N)y = 14 * x - 7 + np.random.normal(loc=0.0, scale=5.0, size=N)# 将x和y设置成为矩阵# print(x)x.shape = -1, 1y.shape = -1, 1# print(x)# print(type(x))# 2. 模型构建# 定义一个变量w和变量b# random_uniform：（random意思：随机产生数据， uniform：均匀分布的意思） ==&gt; 意思：产生一个服从均匀分布的随机数列# shape: 产生多少数据/产生的数据格式是什么； minval：均匀分布中的可能出现的最小值，maxval: 均匀分布中可能出现的最大值w = tf.Variable(initial_value=tf.random_uniform(shape=[1], minval=-1.0, maxval=1.0), name='w')b = tf.Variable(initial_value=tf.zeros([1]), name='b')# 构建一个预测值y_hat = w * x + b# 构建一个损失函数# 以MSE作为损失函数（预测值和实际值之间的平方和）loss = tf.reduce_mean(tf.square(y_hat - y), name='loss')# 以随机梯度下降的方式优化损失函数optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)# 在优化的过程中，是让那个函数最小化train = optimizer.minimize(loss, name='train')# 全局变量更新init_op = tf.global_variables_initializer()# 运行def print_info(r_w, r_b, r_loss): print("w=&#123;&#125;,b=&#123;&#125;,loss=&#123;&#125;".format(r_w, r_b, r_loss))tf.summary.scalar('scalar_loss', loss)tf.summary.histogram('weights',w)tf.summary.histogram('bias',b)with tf.Session() as sess: # merge all summary merged_summary = tf.summary.merge_all() # 得到输出到文件的对象 writer = tf.summary.FileWriter('./result', sess.graph) # 初始化 sess.run(init_op) # 输出初始化的w、b、loss r_w, r_b, r_loss = sess.run([w, b, loss]) print_info(r_w, r_b, r_loss) # 进行训练(n次) for step in range(100): # 模型训练 sess.run(train) # 输出训练后的w、b、loss r_w, r_b, r_loss = sess.run([w, b, loss]) summary=sess.run(merged_summary) writer.add_summary(summary, step) print_info(r_w, r_b, r_loss) PyTorch训练过程可视化Visdom是Facebook在2017年发布的一款针对PyTorch的可视化工具。Visdom由于其功能简单，一般会被定义为服务器端的matplot，也就是说我们可以直接使用python的控制台模式进行开发并在服务器上执行，将一些可视化的数据传送到Visdom服务上，通过Visdom服务进行可视化。 官方GitHub：https://github.com/facebookresearch/visdom PyTorch亦可使用Tensorboard进行可以可视化，GitHub上已有大神进行了实现：https://github.com/lanpa/tensorboardX Kreas训练过程可视化在训练文件train.py中最后加入一下代码。 123456789101112131415# Where H = model.fit_generator(...)N = NUM_EPOCHS # Number of epochsplt.style.use("ggplot")plt.figure()plt.plot(np.arange(0, N), H.history["loss"], label="train_loss")plt.plot(np.arange(0, N), H.history["val_loss"], label="val_loss")plt.plot(np.arange(0, N), H.history["acc"], label="train_acc")plt.plot(np.arange(0, N), H.history["val_acc"], label="val_acc")plt.title("Training Loss and Accuracy on Dataset")plt.xlabel("Epoch #")plt.ylabel("Loss/Accuracy")plt.legend(loc="lower left")plt.savefig("plot.png")]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[研究生生涯记事]]></title>
    <url>%2Farticle%2FGraduateLife.html</url>
    <content type="text"><![CDATA[毕业已两月有余，中秋佳节将至，借本篇来记述研究生期间印象比较深刻的几个场景，以此来缅怀自己「酸甜苦辣」的研究生生活！ 前记关于研究生生活的开端，最早可以追溯到16年4月份在本科做毕设期间，赵老师那时便早早安排我入驻进了实验室。 于16年4月份第一次参加组会的场景，我仍然记忆犹新。犹记袁毅师兄在讲台上进行汇报，台下的我却是满脸懵逼，一句专业词汇也听不懂，但也不能说什么，强忍着在那傻傻的听。会后便与谭星师兄、聂志巧师兄及方嘉佳师兄加了微信，一同去食堂吃了午餐。 令我记忆深刻的便是第二次组会，当天陪一朋友去集美学村派出所补办身份证，又因为赵老师没通知我开组会，我便没有去参加。（其实我当时以为第一次让我参加组会主要是让我观摩情况，根本没想到竟还有第二次、第三次、、、） 10点左右，赵老师便给我打电话问我为何没有参加组会。 赵老师：王聪，你为什么没来参加组会？我解释道：老师，我没有收到通知。赵老师：那你现在过来开会。我补充道：我陪朋友去补办身份证了，暂时赶不过去，抱歉啊赵老师，下次我绝对准时参加。 谁知赵老师11点又给我拨来电话。 赵老师：你补办完身份证了么？我心惊胆战：还没有，正在申请临时身份证。赵老师：行，那你弄完过来。我小心说道：好的老师，弄完我立马过去。 回到学校已是将近中午12点，我便向赵老师发了短信，表达了刚补办身份证，赶不过去参加组会，并再次表明了自己下次按时参加组会的态度。 本以为这件事就这么过去了，中午1点左右刚躺在宿舍床上打算午休，赵老师又一通电话打来，让我马上到他办公室。疑惑中，我一个鲤鱼打挺从床上下来，麻利穿上衣服便快步走向赵老师办公室。 来到A619后，赵老师让我在旁边的板凳坐下，开始对我谈心。赵老师语重心长的说，你现在不能把自己当本科生来看待了，也不能再像本科生那种方式学习，更不要想着让老师整天督促着你去学习，你要习惯你研究生的新身份……（我一直在那点头，以致于后面老师说的啥也记不清了，捂脸🤦‍♂️）。 Anyway，经过这件事，我在一定程度上摆正了学习的心态。 后来5月份，林鸿鑫师兄给了我一篇韩国棒子的文章，权当入门练手用，通过对此文献的阅读、公式推导及写仿真程序，也使得自己对师兄们所做的科研内容有了一丝了解，也算是站在了物理层安全研究的入口。（不过让我后悔的一件事，便是研二的时候将Lyx公式推导、Matlab程序以及Visio模型图统统都给了欧阳大亮、杨裕琳及徐伟他们，感觉反而让他们没有了自己动手的动力，后悔不已！不过现在他们都不做传统方向了，便没有了内心的负罪感，哈哈哈哈！🤣） 返校第一场景16年8月10日上午9点多，下了BRT后，我特意从正门步入校园，来到了这个既熟悉有又陌生的土地。说熟悉，毕竟我在这已求学四年，说陌生，此时校园已是物是人非，同一届的同学们早已离开校园，各自奔向了自己的人生道路。 拉着我蓝色略带调皮的小行李箱，重踏这校园里的每一步，本科时的场景一幅幅浮现在眼前，仿佛还有曾经的小伙伴们从我身边走过、说说笑笑。我深深叹了一口气，心想还要待在这里三年，心里面便不是个滋味。(不过，回过头来看，很庆幸当时能够在校园再享受三年的校园时光。) 走进当时暑假的临时宿舍-刺#3 A403，一进门我变看到吴奇和李元建两位同学，当时李元建在看中国好声音，吴奇在左前床位的桌子那噼里啪啦的打游戏，其上铺的床栏杆上还挂着一件肥大的、浅蓝色的「面肉蛋菜面」T恤，我一看见这场面，心想这就是以后的同窗了，这个死肥宅 😂。整理好床铺后，便和他们攀谈了几句(按后来他们回忆说，他们第一次见我时，感觉我话不多，就在门外一直和志鹏在聊)。 到了中午我本科同学曾志鹏也返回了宿舍，我便拉他出去谈心，向他倾述物是人非，两人一起远眺这这校园，我猜想，此时我俩的感受应该是一样的。收拾完已是中午，我怕影响他们三位午休，特意搬了个板凳在门外泡了一桶泡面，吃着泡面，看着校园，「酸甜苦辣」研究生生涯便正式从此开启。 不过另提一句，在刺#3住的那将近20天，是我整个研究生阶段睡得最踏实、最轻松的阶段，在宿舍养养绿植，逗一逗小仓鼠，和舍友们卧谈等等等等，那时候真的是对研究生生活充满了向往和追求的。 16年8月20号，课题组计划去野山谷团建，犹记出游前一天跟着师兄们和老赵一起去后街购买第二天的口粮，恰巧在灯光篮球场附近迎面碰到吴奇、李元健他们，回来宿舍吴奇还给了我一包小鱼干。第二天老赵和方师兄分别驱车前往野山谷，原本我是计划做方师兄的车可以在途中吹逼几句，麻利地将书包放在了方师兄车的后备箱，老赵却硬要我负责导航，途中还因为一道路在修缮而百度地图没及时更新，老赵嫌我导航不行，「人在车中坐，锅从天上来」（这也是我从百度地图转向高德地图的原因之一，哈哈哈🤣）。 在野山谷大家玩的还是蛮疯狂的，一路从山脚向山顶走走停停，到了一处溪水，老赵和谭师兄跃跃欲试，开心的在那游了起来，后两人一个安利旁边观望的我们下水，岸上师兄也忽悠我下水，实在没办下，下水游了两下😅。上来后大家就开始吃起了午餐，方师兄还专门买了两大袋包子，真的香！ 12点左右，行至半山腰一亭子处，天公不作美，下起了一阵大雨。约莫半小时后，阵雨渐停，大家便继续向山顶前行。走到景区尽头后，我们又忽悠后门的老大爷给我们开门，索性去后山看一看，途中绕了好几个弯，还途径一水电站，边走边探索未知的路线。后又经过一上个世纪七八十年代的石渠，然后继续向上前进，到达一居民区，发现没有公交车到达景区正门，为安全起见，便原路返回，乘坐小白电瓶车直驱山脚野山谷正门，遂返程。可能也是时间有点久远了，场景回忆起来都有点像做梦一样，不知道师兄、同学们有无这种感受。 研一上自16年8月起，就开始慢慢进入了做科研的状态中。最疯狂的时候一周开两次组会，一周至少看英文文献5-6篇，巨大的压力和快速的节奏令我疯狂，不过也使我收获颇多。最重要的是，赵老师的监督及实验室良好的氛围，使得我在短时间内养成了静下心来踏踏实实做研究的习惯，在此也深深感谢赵老师及师兄们。 这里再对16年9月的莫兰蒂台风期间的事情记一下。台风当天夜里途经厦门时，后半夜被门窗震动的声音吵醒，我张眼向窗外瞄了一眼，外面狂风暴雨，倒是没想那么多，只是希望门窗不要那么响，好能够好好休息。此时，我发现舍友刘楚佳也醒了，我倆遥遥对视一眼，看了看应该没什么大事，预估和往年台风一样，下阵雨就过去了，就心安地继续睡了。可谁知，第二天早上一看，四周的紫荆树大都腰斩，少数没有腰斩的也被摧残的不成样子。羽网综合馆的房顶也被台风掀了，去走廊一看，田径场那边的围网全部被吹倒，不远处的号称国家深林公园的兑山公园也是叶落凋零。从早上到中午，手机一直处于无信号的状态。下午勉强有了信号后，看老赵在QQ群里说，大家都别再宿舍傻等着了，赶紧来实验室吧🤣！台风刚过，心想老赵这就来实验室了？！后面课题组便陆陆续续去了实验室，大家都在吐槽昨晚的经历。记得袁师兄说，他整晚都基本没睡，说感觉当时整栋宿舍楼都在晃动。吴奇说他第一次遇见台风，当晚吓得不轻，蜷缩在一个小角落里，生怕台风吹破玻璃。我还特意去看了看当年大一栽的那棵树，也被台风吹断了，心痛不已。不过，到毕业时，整个校园的树木恢复的还算可以。 从做完棒子文章后，自己接手的第一个项目就是对林鸿鑫师兄发表的一篇专利进行公式推导，犹记的那时一个双向中继。里面的一个四重积分，我积了大半个月后，剩下最后一重积分死活积分不出来，参考了「Table of Integrals, Series, and Products」，也使用了Mathematics，仍然无解，在和林师兄通了电话后，他笑着说，本来这个模型我就觉得太复杂，闭合表达式求不出来，你就全权当练手了。后来和老赵说明了这个情况，老赵便让我开始做吴亚峰师兄有限反馈的方向，亚峰师兄的那篇「基于有限反馈的非可信中继系统的物理层安全性能分析」我前后详读了不下5遍，文章最后附的参考文献我也全部都下载下来，看了绝大部分，受益匪浅。也就这样，慢慢有了方向，便沉下心去研究这个有限反馈的方向。 研一上有两个场景我觉得值得写一下以便回味。第一个便是在10月7号，课题组在实验室煮了第一次火锅（小声说）。当天下午袁师兄带领我们大家伙去新华都采购食材，火腿肠买了好几大包，排骨、牛肉、丸子、藕、海带等等等等(为什么我流着口水在打这段🤣)，dei dei dei 想起来老干妈还买了好几瓶，真正的是满载而归。谭师兄从岛内带来的虾和螃蟹，使得整个火锅宴更丰盛了起来。火锅底料是谭师兄专门从重庆捎带过来的「桥头牌」底料，先向锅内倒上一点油，然后将排骨放入锅中，煎至两面金黄，然后将火锅底料用热油溶解，而后向锅内加水，盖上锅盖至底汤沸腾，所有人都围在桌子两侧，迫不及待的享受这顿每餐。如果要问我这个流程为何记得这么详细，我除了负责洗菜，就是在旁观师兄们的神操作了😂。犹记袁师兄还没毕业时，经常带着我们去打羽毛球，另外吃喝比较Crazy的时期，哈哈哈！ 另外一件事，便是研一期末考试完之后，师兄们都陆续回家，A621实验室就剩下我和 死肥宅 吴奇两人，眼看这实验室前面的投影设备，便心想着能不能演示出来，两人一拍即合，便将VGA接口与吴奇的主机连接起来，但一开始实验室左侧的大音箱始终没有声音，心感无法有效形成立体声的效果，便一不做二不休，将所有的线捋顺逐个排查，不一会就找到了原因解决左侧音响不响的问题。两人还特意把左侧的门窗用纸板挡住，关上灯，营造电影院的氛围。可不曾想，那竟然是三年唯一一次在实验室用投影仪放电影。 研一下（17年4月初和吴奇、舍友刘楚嘉抽时间去了香港一趟，先挖个坑，这个后面找个时间详记一下。) 可不曾想，研一下学期17年4月份的时候，老赵把我从实验室喊出来，说咱们下去溜达溜达。对于这种情况，我心想绝对没有表面这么简单，但咱啥也不敢问，啥也不敢说，便乖乖跟着下了实验大楼。 出来实验大楼，老赵说咱们在校园走走，心中试想了一百种挨批的可能。 老赵：我看你对实现类的项目应该感兴趣吧？！我：是，是挺感兴趣。（其实当时内心已经奔腾了，项目实现好啊，就业应该比较好对口！回头来看，其实没有那么轻松，实则从一个坑跳到了另外一个坑🤣）老赵：我之前和东大的几位老师也聊过，和郭老师也聊了聊，想着让你做一做无人机这个方向。我：无人机啊，可以做么？（卧槽，无人机，这么高端的东西，这可咋整！）老赵：可以的，那边就有一架，我想着是无人机与通信结合，搞一搞多无人机组网通信这种。我：可以的，我可以尝试做一做！ 后来基本就是问我能不能做，感不感兴趣之类的了。（记不太清了）也算是让我开辟一个新方向吧！至此，便开始了搞无人机的征程！ 在6月底，老赵带我去深圳参加了「第二届深圳国际无人机展览会」，会展前一天约见了某无人机创业公司的老总，去其公司参观了下，给我们介绍了下他们的科教类无人机，晚上和他们一次吃了顿饭，聊得什么我是记不住了，不过当晚的菜是很美味。饭后和老赵在深圳滨海大道的深圳湾公园骑自行车的场景回忆起来都有点梦幻，行程来回大约20公里，犹记老赵还用他的华为手机识别路边的花卉，给我介绍华为手机这一强大的功能，以至于后来19年年前我也置换了一台华为系的手机。展会中各式各样的无人机令我大开眼界，中午参观完毕边找了一处吃完饭还是返厦，在候车厅老赵看到了周黑鸭，还特地过去看了看，后特买一盒，我和老赵在动车上吃的津津有味。 研一下的那个暑假我是回家了的，老赵劝我留校，也方便和他沟通沟通，但我「思乡心切」，还是回了家。7月看了一整个月的嵌入式开发，月底我向老赵远程汇报了我的进展。老赵直接电话拨了过来，给我说，暑假他又回了东大一趟，和那边的老师交流了交流，说那边都在搞深度学习，让后让我把无人机与深度学习结合起来。 其实，一直倒是对这个深度学习和数据挖掘比较感兴趣，老赵这次指定这个方向，也符合我之前对自己的规划。自8月起，便开始了深度学习的学习之旅。 ### 研二 研二上便是苦逼的啃深度学习相关理论、看无人机相关文献等事宜，不断去沉淀相关理论知识及相关实践，也进一步地去思考如何让深度学习与无人机结合起来。 不过也趁着共享单车的鼎盛时代，我和小伙伴们疯狂的在厦骑行。 >🚲🚲🚲 >那些年疯狂骑行的岁月... >路线一：T4候机楼站—白石炮台 >路线二：学校—中山路 >路线三：T4候机楼站—中山路—斗西八合里 >路线四：学校—北站—后溪—学校 >路线五：学校—同安方特 >路线六：T4候机楼站—狐尾山 >路线七：集美万达—学校 >路线八：学校—集美图书馆 [视频点此](https://weibo.com/tv/v/I4mkbiInd?fid=1034:4410390705325136) 仍记得17年中秋节时电通小班博饼时的情形。先前我喊了粘春湄、吴淼及小红几位同学一起去新华都采购博饼的奖品，大都是生活用品及一些零食等，因经费有限，博得「状元」仅可得红包🧧80元，不过还是很令同学们兴奋不已。因为采购奖品用了一下午，当晚的课题组聚餐还迟到了半小时，很是惭愧。 第二天便在紫二餐厅包间进行了班级聚餐，因为全班共15人，每次在紫二聚餐都是一个包间两桌拼在一起，想一想还有点有趣。饭后进行了激动人心的博饼环节，我就记得自己博到了杯子及抽纸一包🤣，不过大家都参与的十分开心，也算是在中秋佳节虽然没能和家人在一起，但是正是因为班里同学们，这里也变有了家的感觉。也愿我们电通小班能够一直团建下去，并希望所有同学越来越好！ 研二下便是一方面写小论文，另一方面准备求职的技能和寻找各种实习机会。 此外就是于5月前往成都参加会议，之前对这件事情做了记录：成都参会之记 。 明明研二是最苦逼的一年，为何却记不起来有什么事情了，难道是选择性失忆么？！ 研三上也是一个偶然的机会，跑到济南找了一实习，关于此处面试经历此前也做了一个记录济南实习面试之记，不过在这里额外对实习期间的一些所见所感做一个记录。 也正因为在厦门上了七年，以至于在济南基本没有同学，孑身一人。每每买过晚餐后，在路边路灯下等红绿灯时，看着地上自己的孤影，心中不免一丝凄凉涌上心头，全凭在心中吟句李白的《将进酒》里面的一句诗，「古来圣贤皆寂寞，惟有饮者留其名」，若无其事地走过一个个路口。慢慢地，我也逐渐习惯了没有朋友、同学在身边的生活节奏，自己也学会一个人去找事做，去丰富自己的生活。当时给自己订下的安排是周六下午去大明湖走上一圈，随便找个地方坐一下发呆，或看看各类老大爷的绝技，一是权当锻炼身体，顺便也让自己去外界交流起来。此外，就是周末下午去省图看《三联生活周刊》，当全神贯注时，就有一种在学校图书馆读书的感觉，也算是去一千七百多公里外的学校的思念吧。 犹记有这么几个时间节点另外感受非同一般。第一个就是18年9月份，这个全民开学的日子里，自己却仍然在外实习，想一想这应该也是上学20多年来第一次开学没有回学校，心情有点怪怪的，有种负罪感，总感觉自己做错了些什么。后来我思考这个事情的时候，从心理学上分析是因为20几年的生活习惯的突然改变，或者一个一直认为总会发生的事情却没有发生时的失落感。 第二个时间节点，便是18年10月初回学校准备参加毕业论文中期检查答辩事宜，当从BRT「大学城站」下车后，看着西街耀眼而华丽的霓虹灯，看着校园端庄且肃穆的灯光，自己竟然激动了起来。其实也就一个多月没回学校，但心情却是像极了他们所谓地异地恋人相聚的激动与愉悦感。后再从济南回厦门的几次中，便没有了那种感觉，应该是总觉得过一段时间我还会回来，反而就比较悠然自得了。 此外，很庆幸在这段实习的间隙中完成了毕设的撰写，也为后面的论文的修改留足了时间。 记得18年11月份返厦途中于潮汕转机时，还专门在潮汕一带玩了一天，按吴奇的话说，这是一片美食的净土。一直想把在潮汕探索美食的过程记录一番，半年前草稿虽已打好，却一直拖延到现在还没完成，此处再挖个坑，后面找时间补充下。 于12月底和老赵及课题组一起参加方师兄婚礼，大家都甚是快乐！不过在去莆田的路上吹逼一路的我，回来的时候却吐得懵逼，捂脸🤦‍♂️，Anyway 也算是一段开心的快乐时光。 研三下19年2月25号开学后不久，便进行了毕业论文预答辩的工作，而后疯狂修改论文，从行间距到标点符号，从公式编辑到图表调整，生怕出现纰漏。Deadline时小心翼翼的将终稿提交了上去。 心想毕业之后就要离开厦门，提交文论后便计划重刷厦门的各个角落，和课题组的同学、师弟师妹们一起疯狂刷厦门美食，在白鹭洲公园追鸽子，在鼓浪屿各种试吃，在中山路畅游，在园博苑远眺，在植物园赏花，等等等等，也算是与多年熟悉的地方一次面对面的告别。唯一比较遗憾的从上学期就答应领着王培臣、张孟洁、周洁师弟师妹们爬一次天马山，原本以为距离毕业时间还很充裕，等时间合适就带他们爬一次。 可谁知当时实习单位突发通知，论文答辩后便早早召我回去干活。原计划自己去泰国一趟，行程及路线都早已规划好，还特意向同班蔡小红同学取了经，不过最终泰国之行还是泡了汤。 离校场景19年6月底专门从济南飞厦准备参加6月30号的毕业典礼，很多人都不理解为何非要大老远的还过来参加者典礼，我觉得这是一种仪式感，同时也为我这三年的经历画上一个句号。 30号当天上午，兴奋的参加了毕业典礼，多年的付出终于得到了收获，这一刻便是属于我们的荣耀 (王者荣耀 ?)。 毕业典礼后从学院领取到证书后差不多中午1点多，实验室准备再聚一次餐，不过这次聚餐吃饭却是十分匆忙，我一直怕赶不上下午四点多的飞机，饭吃了一半2点多我就回宿舍去了行李。 领着行李走向西门，在西街与欢颜笑语、嘴角泛着油光😂的师弟师妹们挥手告别（这群没良心的😅），后便上了去往机场的滴滴。 吴奇说送我至机场，在出租车上看着主楼离我越来越远，泪水忍不住的在眼眶里打圈，和吴主席说起话来也有些哽咽，两人共同缅怀放荡不羁的曾经。我望着窗外，这些年的一场场记忆中的画面都浮现在了眼前，这个走过无数次的集美大道、厦门大桥，等等等等，满载着我的青春及回忆呐。 总结从本篇记述中也可以看得出，越往后，阶段的篇幅越来越少，这个其实和内心的感受是一致的，深感时间过得越来越快，总感觉没做什么，时光却飞速的流逝了。一方面希望师弟师妹们能好好珍惜在校美好的事光，另一方面也提醒自己也好好的利用好时光，趁着年轻去看看着大千世界，去体验着人生百态。 课题组的师兄师姐们真的是一个大宝藏，这些年师兄师姐们带我们科研、带我们玩耍、带我们吃喝、带我们成长！这些年，我们课题组开辟了一个个根据地，从西街湘菜馆到开元141，从小东北到BOB，从天马山到鼓浪屿等等等等，这些地方都满载着我们共同的记忆，共同见证了我们的欢颜笑语。三年中也见到了已毕业的帅气凛人的笑笑师兄、晓龙师兄、亚峰师兄，唯一遗憾的就是没亲眼看到已毕业两位师姐的芳容。一直都很庆幸进入了PHY团队，这是我这三年最值得骄傲的事情！感谢老赵给予我的指导，开拓了我的 学术眼界，教会我严谨治学。感谢我的家人们！(为何强行写成了论文致谢，捂脸…🤣)以上！​ 王 聪​ 19年 秋]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>研究生</tag>
        <tag>回忆录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前向传播(Forward Propagation)与反向传播(Back Propagation)个人理解与总结]]></title>
    <url>%2Farticle%2FFPandBP.html</url>
    <content type="text"><![CDATA[深度学习中的的前向传播（Forward Propagation）与反向传播（Back Propagation）的目的就是，寻找一组最优的参数，使得损失函数（Lost Function）或称为成本函数（Cost Function）获取到最小值。 下图为一个多层的神经网络，本文则对此图例进行前向传播和反向传播的解析。 前向传播（Forward propagation）单个数据前向传播的计算过程：（此处损失函数以平方差损失函数为例） z^{[L]} = w^L a^{[L-1]} + b^{[L]} a^{[L]}=\sigma(z^{[L]}) L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^2多数据前向传播的计算过程： Z^{[L]} = W^L A^{[L-1]} + b^{[L]} A^{[L]}=\sigma(A^{[L]}) L(\hat{y}^{(i)},y^{(i)}) = \frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2 J(W, b)=\frac{1}{m} \sum\limits_{i=1}^m{L(\hat{y}^{(i)},y^{(i)})}其中：\sigma(*)为激活函数，L(\hat{y}^{(i)},y^{(i)})为Loss Function ，J(W, b)为Cost Function，[L]为神经网络的第L层，用大写字母表示相应参数的向量化表示。下同。 反向传播(Back Propagation)单个数据反向传播的计算过程： \mathrm{d}z^{[L]} = \mathrm{d}a^{[L]} \cdot g'(z^{[L]}) \mathrm{d}w^{[L]} = \mathrm{d}z^{[L]} a^{[L-1]} \mathrm{d}b^{[L]} = \mathrm{d}z^{[L]} \mathrm{d}a^{[L-1]} = w^{[L-1]T}\mathrm{d}z^{L}多数据反向传播的计算过程： \mathrm{d}Z^{[L]} = \mathrm{d}A^{[L]} \cdot g'(Z^{[L]}) \mathrm{d}W^{[L]} = \frac{1}{m}\mathrm{d}Z^{[L]} A^{[L-1]} \mathrm{d}b^{[L]} = \frac{1}{m}np.sum(\mathrm{d}Z^{[L]}, axis=1, keepdim=True) \mathrm{d}A^{[L-1]} = W^{[L-1]T}\mathrm{d}Z^{L}权值更新： W^{[L]} := W - \alpha \mathrm{d}W^{[L]} b^{[L]} := b - \alpha \mathrm{d}b^{[L]}反向传播(Back Propagation)与链式法则反向传播(BP, Back Propagation)算法是多层神经网络的训练中举足轻重的算法，其基于复合函数的链式法则，但在实际运算中的意义比链式法则要大的多。 以求$e=(a+b)\times(b+1)$的偏导为例。为方便，在图中，引入了中间变量$c$，$d$。 其复合关系图如下所示： 接着，分别求$\frac{\partial e}{\partial a}$，$\frac{\partial e}{\partial b}$的值。 利用链式法则可得： \frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial a}=2\times1=2 \frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\cdot \frac{\partial d}{\partial b}=2\times1+3\times1=5不难发现，$\frac{\partial e}{\partial a}$的值等于从 e -&gt; a 的路径上的偏导值的乘积，而$\frac{\partial e}{\partial b}$的值等于从 e -&gt; b 路径 e -&gt; c -&gt; b 上的偏导值的乘积加上路径 e -&gt; d -&gt; b 上的偏导值的乘积。可以注意到，这样做是十分耗时，因为很多路径被重复访问了。比如上图中， e -&gt;c -&gt; a和e -&gt; c -&gt; b 就都走了路径e -&gt; c 。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。 同样是利用链式法则，反向传播(BP, Back Propagation)算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。 梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到损失函数收敛。 从最上层的节点 e开始，初始值为1，以层为单位进行处理。 对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。 如，对于子节点c，1\cdot\frac{\partial e}{\partial c}=1\times2=2 如，对于子节点d，1\cdot\frac{\partial e}{\partial d}=1\times3=3 然后将第二层的节点各自作为起始顶点，初始值设为第2步分别得到的偏导值，以”层”为单位重复上述传播过程，即可求出顶点e对每一层某个节点的偏导值。 如，对于子节点a，2\cdot\frac{\partial c}{\partial a}=2\times1=2 如，对于子节点b，1\cdot\frac{\partial c}{\partial b}+3\times\frac{\partial d}{\partial b}=2\times1+3\times1=5 由上述过程可以发现，BP算法很好的解决了重复计算的问题。 前向传播及反向传播具体计算过程示例下图为一个三层神经网络结构，分别输入层（第0层），隐藏层（第1层），输出层（第2层）： 整个输入、各层参数矩阵、输出，表示如下： 输入：X= \left[ \begin{matrix} x_1\\ x_2\\ \end{matrix} \right] =\left[ \begin{matrix} 0.8\\ 0.3\\ \end{matrix} \right] 隐藏层权值参数： W_{1,0}= \left[ \begin{matrix} w_{31}&w_{32}\\ w_{41}&w_{42}\\ \end{matrix} \right] =\left[ \begin{matrix} 0.3 & 0.5\\ 0.6 & 0.4\\ \end{matrix} \right] 隐藏层的偏差项： B_1= \left[ \begin{matrix} b_3\\ b_4\\ \end{matrix} \right] =\left[ \begin{matrix} 0.02\\ 0.03\\ \end{matrix} \right] 输出层权值参数： W_{2,1}= \left[ \begin{matrix} w_{53}&w_{54}\\ \end{matrix} \right] =\left[ \begin{matrix} 0.2 & 0.7\\ \end{matrix} \right] 输出层的偏差项： B_2= \left[ \begin{matrix} b_5\\ \end{matrix} \right] =\left[ \begin{matrix} 0.03\\ \end{matrix} \right] 目标值： y_{out}=0.5对于前向传播第1层的输出为： \begin {aligned} Z_{layer1} &=\left[ \begin{matrix} z_3\\ z_4\\ \end{matrix} \right] =W_{1,0}\cdot X + B_1 =\left[ \begin{matrix} w_{31}&w_{32}\\ w_{41}&w_{42}\\ \end{matrix} \right] \cdot \left[ \begin{matrix} x_1\\ x_2\\ \end{matrix} \right] +\left[ \begin{matrix} b_3\\ b_4\\ \end{matrix} \right]\\ &= \left[ \begin{matrix} w_{31}\times x_1 + w_{32}\times x_2 + b_3\\ w_{41}\times x_1 + w_{42}\times x_2 + b_4\\ \end{matrix} \right] =\left[ \begin{matrix} 0.3\times 0.8 + 0.5\times0.3 + 0.02\\ 0.6\times 0.8 + 0.4\times0.3 + 0.03\\ \end{matrix} \right]\\ &=\left[ \begin{matrix} 0.41\\ 0.63\\ \end{matrix} \right] \end {aligned}经激活函数处理后的输出为： \begin {aligned} Y_{layer1} &=\left[ \begin{matrix} y_3\\ y_4\\ \end{matrix} \right] =f(Z_{layer1}) =f(\left[ \begin{matrix} z_3\\ z_4\\ \end{matrix} \right])\\ &=f(\left[ \begin{matrix} 0.41\\ 0.63\\ \end{matrix} \right])\\ &= \left[ \begin{matrix} 0.601\\ 0.652\\ \end{matrix} \right] \end {aligned}同理，对与第2层的输出、经过经激活函数后的输出分别为： \begin {aligned} Z_{layer2} &=W_{2,1}\cdot Y_{layer1}+B_2 =\left[ \begin{matrix} w_{53} & w_{54}\\ \end{matrix} \right] \cdot \left[ \begin{matrix} y_3\\ y_4\\ \end{matrix} \right]+b_5\\ &=\left[ \begin{matrix} w_{53}\times y_3 + w_{54}\times y_4 + b_5\\ \end{matrix} \right] =0.601\times0.2+0.652\times0.7 + 0.03 \\ &=0.6066 \end {aligned} \begin {aligned} Y_{layer2} &=f(Z_{layer2}) =f(W_{2,1}\cdot Y_{layer1}) =f(\left[ \begin{matrix} w_{53} & w_{54}\\ \end{matrix} \right] \cdot \left[ \begin{matrix} y_3\\ y_4\\ \end{matrix} \right])\\ &=f(0.6066)\\ &=0.647 \end {aligned}可得最终的损失函数的值为：（为下文方便表示，F_{loss}=L(Y_{layer2},y_{out})） \begin {aligned} F_{loss}&=\frac{1}{2}(Y_{layer2}-y_{out})^2\\ &=\frac{1}{2}(0.647-0.5)^2\\ &=0.0108 \end {aligned}优化的目标则是通过训练，调节参数，来使得损失函数的值越小越好。反向传播（BP，Back Propagation）算法，便是利用梯度下降法来求得使损失函数达到最小值得一种方法。 对于反向传播首先对于第2层而言： \begin{cases} F_{loss}=\frac{1}{2}(Y_{layer2}-y_{out})^2\\ Y_{layer2}=f(Z_{layer2})\\ Z_{layer2}=W_{2,1}\cdot Y_{layer1}=w_{53}\times y_3 + w_{54}\times y_4 + b_5\\ \end{cases}求解$\frac{\partial F_{loss}}{\partial w_{53}}$，则根据反向传播(BP, Back Propagation)算法可得: \begin {aligned} \frac{\partial F_{loss}}{\partial w_{53}} &=\frac{\partial F_{loss}}{\partial Y_{layer2}} \cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}} \cdot \frac{\partial Z_{layer2}}{\partial w_{53}}\\ &=(Y_{layer2}-y_{out})\times f(Z_{layer2})\times(1-f(Z_{layer2}))\times y_3\\ &=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.601\\ &=0.0202 \end {aligned}其中Sigmod函数导数为： \begin {align} f(x)=&\frac{1}{1+e ^{-x}}\\ \frac{df}{dx}=&-(\frac{1}{1+e ^{-x}})^2\times(-e^{-x})\\ =&f(x)\times(1-f(x))\\ 因此，\frac{\partial F_{loss}}{\partial Y_{layer2}}=&f(Z{_layer2})\times(1-f(Z{_layer2})) \end {align}（注：不同激活函数求导后的结果是不同的，在BP过程中所谓的梯度消失，梯度爆炸与激活函数的不恰当有很大关系！） 求解$\frac{\partial F_{loss}}{\partial w_{54}}$，根据则根据反向传播(BP, Back Propagation)算法可得： \begin {aligned} \frac{\partial F_{loss}}{\partial w_{54}} &=\frac{\partial F_{loss}}{\partial Y_{layer2}} \cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}} \cdot \frac{\partial Z_{layer2}}{\partial w_{53}}\\ &=(Y_{layer2}-y_{out})\times f(Z{_layer2})\times(1-f(Z{_layer2}))\times y_4\\ &=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.652\\ &=0.0219 \end {aligned}求解$\frac{\partial F_{loss}}{\partial b_5}$，根据则根据反向传播(BP, Back Propagation)算法可得： \begin {aligned} \frac{\partial F_{loss}}{\partial w_{54}} &=\frac{\partial F_{loss}}{\partial Y_{layer2}} \cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}} \cdot \frac{\partial Z_{layer2}}{\partial b_5}\\ &=(Y_{layer2}-y_{out})\times f(Z{_layer2})\times(1-f(Z{_layer2}))\times 1\\ &=(0.647-0.5)\times(0.647)\times(1-0.647)\times1\\ &=0.0336 \end {aligned}进一步的，我们计算第1层的参数： \begin{cases} F_{loss}=\frac{1}{2}(Y_{layer2}-y_{out})^2\\ Y_{layer2}=f(Z_{layer2})\\ Z_{layer2}=W_{2,1}\cdot Y_{layer1}=w_{53}\times y_3 + w_{54}\times y_4 + b_5\\ Y_{layer1} =\left[ \begin{matrix} y_3\\ y_4\\ \end{matrix} \right] =f(Z_{layer1}) =f(\left[ \begin{matrix} z_3\\ z_4\\ \end{matrix} \right])\\\\ Z_{layer1} =\left[ \begin{matrix} z_3\\ z_4\\ \end{matrix} \right] =W_{1,0}\cdot X + B_1 =\left[ \begin{matrix} w_{31}\times x_1 + w_{32}\times x_2 + b_3\\ w_{41}\times x_1 + w_{42}\times x_2 + b_4\\ \end{matrix} \right] \end{cases}求解$\frac{\partial F_{loss}}{\partial w_{31}}$，则根据反向传播(BP, Back Propagation)算法可得: \begin {aligned} \frac{\partial F_{loss}}{\partial w_{31}} &=\frac{\partial F_{loss}}{\partial Y_{layer2}} \cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}} \cdot \frac{\partial Z_{layer2}}{\partial y_3} \cdot \frac{\partial y_3}{\partial z_3} \cdot \frac{\partial z_3}{\partial w_{31}} \\ &=(Y_{layer2}-y_{out})\times f(Z_{layer2})\times(1-f(Z_{layer2}))\times w_{53}\times f(Z_{layer1})\times(1-f(Z_{layer1}))\times x_1 \\ &=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.2\times(0.601)\times(1-0.601)\times(0.8)\\ &=0.001288 \end {aligned}同理可得到其它几个参数权值：$\frac{\partial F_{loss}}{\partial w_{32}}$，$\frac{\partial F_{loss}}{\partial w_{41}}$，$\frac{\partial F_{loss}}{\partial w_{42}}$。 求解$\frac{\partial F_{loss}}{\partial b_3}$，则根据反向传播(BP, Back Propagation)算法可得: \begin {aligned} \frac{\partial F_{loss}}{\partial b_3} &=\frac{\partial F_{loss}}{\partial Y_{layer2}} \cdot \frac{\partial Y_{layer2}}{\partial Z_{layer2}} \cdot \frac{\partial Z_{layer2}}{\partial y_3} \cdot \frac{\partial y_3}{\partial z_3} \cdot \frac{\partial z_3}{\partial b_3} \\ &=(Y_{layer2}-y_{out})\times f(Z_{layer2})\times(1-f(Z_{layer2}))\times w_{53}\times f(Z_{layer1})\times(1-f(Z_{layer1}))\times 1 \\ &=(0.647-0.5)\times(0.647)\times(1-0.647)\times0.2\times(0.601)\times(1-0.601)\times1\\ &=0.00161 \end {aligned}这里重点说明一下，一个函数在某一点的导数描述了这个函数在这一点附近的变化率。$ {\partial F_{loss}}/{\partial w_{31}}$描述了$ F_{loss}$在$ w_{31}$该点变化率(或切线斜率)，则有： \begin{cases} F_{loss}在w_{31}处为递增 &\mbox{if $\frac{\partial F_{loss}}{\partial w_{31}}>0$ }\\ F_{loss}在w_{31}处为递减 &\mbox{if $\frac{\partial F_{loss}}{\partial w_{31}}]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>反向传播</tag>
        <tag>Back Propagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图解卷积神经网络中二维与三维图像卷积操作]]></title>
    <url>%2Farticle%2FConvolutionalLayer.html</url>
    <content type="text"><![CDATA[经查阅大量资料及手动公式推导，对卷积神经网络中二维与三维图像卷积操作进行图解及公式表示。 注：在数学中，两个矩阵进行卷积操作，卷积核是要翻卷的，如下面动图所示。 在深度学习中，卷积核不需要翻卷。 二维图像卷积操作对于大小为$ h \times w$图像$I$和 大小为$(k_1 \times k_2)$ 卷积核 $K$，定义其Cross-correlation： {(I \otimes K)_{i，j}} = \sum\limits_{m = 0}^{k_1-1}\sum\limits_{n = 0}^{k_2-1}{I(i + m,j + n)}K(m,n)其中 $0 \leqslant i \leqslant h - {k_1}$，$ 0\leqslant j \leqslant h - {k_2}$注意：这里的使用的符号和ii的范围，不考虑Padding及Stride = 1 的情况。 用上图举例： 图中图像大小为：$h \times w = 5 \times 5$，卷积核尺寸为：$k_1 \times k_2 = 3 \times 3$则 $i, j$ 的定义域为$0 \leqslant i \leqslant h - {k_1}$ —&gt; $0 \leqslant i \leqslant 2$，$0 \leqslant j \leqslant h - {k_2}$ —&gt; $0 \leqslant i \leqslant 2$ 则图像中$(0,0)$处卷积后的结果为： \begin {aligned} {(I \otimes K)_{0,0}} &= \sum\limits_{m = 0}^{k_1-1}{\sum\limits_{n = 0}^{k_2-1}{I(i + m,j + n)} }K(m,n)\\ &= \sum\limits_{m = 0}^{2}\sum\limits_{n = 0}^{2}{I(0 + m,0 + n)}K(m,n)\\ &=1\times1+1\times1+1\times1+1\times1\\ &=4 \end {aligned}三维图像卷积操作对于大小为 $h \times w \times d$ 图像 $I$ 和大小为 $(k_1 \times k_2 \times d)$ 卷积核 $K$，定义其Cross-correlation： {(I \otimes K)_{i,j}} =\sum\limits_{l = 1}^{d}\sum\limits_{m = 0}^{k_1-1}\sum\limits_{n = 0}^{k_2-1}{I_l(i + m,j + n)}K_l(m,n)+b其中 $0 \leqslant i \leqslant h - {k_1} + 2p$$0 \leqslant j \leqslant h - {k_2} + 2p$ 用上图举例： 图中图像大小为：$h \times w \times d = 5 \times 5 \times 3$，Padding后的图像大小为：$h \times w \times d = 7 \times 7 \times 3$卷积核尺寸为：$k_1 \times k_2 \times d= 3 \times 3\times 3$，偏差 $b=1$则 $i, j$ 的定义域为$1 \leqslant i \leqslant h - {k_1} + 2p$ —&gt; $1 \leqslant i \leqslant 4$，$1 \leqslant j \leqslant h - {k_2} + 2p$ —&gt; $1 \leqslant i \leqslant 4$ 则图像中$(0,0)$处卷积后的结果为： \begin {aligned} {(I \otimes K)_{0,0}} &=\sum\limits_{l = 1}^{d}\sum\limits_{m = 0}^{k_1-1}\sum\limits_{n = 0}^{k_2-1}{I_l(i + m,j + n)}K_l(m,n)+b\\ &= \sum\limits_{l = 1}^{3}\sum\limits_{m = 0}^{2}\sum\limits_{n = 0}^{2}{I_l(0 + m,0 + n)}K_l(m,n)+1\\ &=2\times1+2\times(-1)+2\times(-1)+2\times(-1)+1\\ &=3 \end {aligned}]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
        <tag>卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[把全连接层转化为卷积层]]></title>
    <url>%2Farticle%2FFCNtoCNN.html</url>
    <content type="text"><![CDATA[全连接层可以视作一种特殊的卷积过程，本文将主要介绍如何把全连接层连接的过程转化为卷积的过程。 全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。 首先以ALexNet为例，其网络结构如下所示： 特征图和全连接层相连。 AlexNet经过五次卷积并池化后得到$6\times6\times256$ 的特征图，下一层为$1\times4096$ 的全连接层，这个过程可以看做将$ 6\times6\times256$ 的特征图与4096个$6\times6\times256$的卷积核进行卷积操作，最终得到$ 1\times1\times4096$的特征图，等价与为$1\times4096$ 的全连接层。 全连接层和全连接层相连。 接着上面的全连接层，其下一层仍然为$1\times4096$ 的全连接层，通过第1步得到了$1\times1\times4096$的特征图，本次全连接过程可以看做存在4096个$1\times1\times4096$个卷积核，依次和$1\times1\times4096$的特征图进行卷积操作。 下面使用形象化的例子来说明全连接层如何转化为全卷积层。 将一个$6\times6\times256$ 的特征图通过全连接层输出一个$1\times4$的特征向量如下图所示： 通过转换矩阵 $W_{4\times4}$便可以实现上述功能，具体的计算过程如下所示： 将$ 2\times2\times1$ 的特征图与4个$2\times3\times1$的卷积核进行卷积操作，便能够得到$ 1\times1\times4$的特征向量。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>FCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云服务器深度学习服务器环境搭建]]></title>
    <url>%2Farticle%2FDeepLearningServerBuilding.html</url>
    <content type="text"><![CDATA[前几天在腾讯云服务器上搭建深度学习的环境，查阅了腾讯云的官方文档及相关博客，一是发现介绍的不太全，二是大都是本地工作站上的部署教程。经过多种尝试和血泪的踩坑，暂总结出一份文档，方便将来重新部署时的查阅及参考。 各模块介绍什么是CUDA？ CUDA (Compute Unified Device Architecture) is a computing platform launched by graphics card manufacturer NVIDIA. CUDA ™ is a universal parallel computing architecture introduced by NVIDIA, which enables GPUs to solve complex computing problems. It contains the CUDA instruction set architecture (ISA) and the parallel computing engine inside the GPU. Developers can now use C to write programs for the CUDA ™ architecture CUDA (ComputeUnified Device Architecture)，是显卡厂商NVIDIA推出的运算平台。CUDA是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。 什么是cuDNN？ The NVIDIA CUDA® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers.Deep learning researchers and framework developers worldwide rely on cuDNN for high-performance GPU acceleration. It allows them to focus on training neural networks and developing software applications rather than spending time on low-level GPU performance tuning. cuDNN accelerates widely used deep learning frameworks, including Caffe,Caffe2, Chainer, Keras,MATLAB, MxNet, TensorFlow, and PyTorch. For access to NVIDIA optimized deep learning framework containers, that has cuDNN integrated into the frameworks, visit NVIDIA GPU CLOUD to learn more and get started. NVIDIA cuDNN (The NVIDIA CUDA® Deep Neural Network library)是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。NVIDIA cuDNN可以集成到更高级别的机器学习框架中，Tensorflow、Caffe等。简单的插入式设计可以让开发人员专注于设计和实现神经网络模型，而不是简单调整性能，同时还可以在GPU上实现高性能现代并行计算。 CUDA与CUDNN的关系CUDA看作是一个工作台，上面配有很多工具，如锤子、螺丝刀等。cuDNN是基于CUDA的深度学习GPU加速库，有了它才能在GPU上完成深度学习的计算。它就相当于工作的工具，比如它就是个扳手。但是CUDA这个工作台买来的时候，并没有送扳手。想要在CUDA上运行深度神经网络，就要安装cuDNN。 在安装cuDNN时，只需要把cuDNN文件复制到CUDA的对应文件夹里就可以，即是所谓插入式设计，把cuDNN数据库添加CUDA里，cuDNN是CUDA的扩展计算库，不会对CUDA造成其他影响。 配置GPU相关应用安装Nvidia驱动安装Nvidia驱动总体来讲有两种主流方式，一是通过源来进行安装，另一种是在官网下载.run安然文件进行安装。 注：通过源安装后进行重启如果无法加载，或者通过.run安装时编译报错，可尝试通过升级Linux内核来解决。 12apt updateapt install dkms build-essential linux-headers-generic 1）通过源安装驱动首先查看符合自己机器的Nvidia驱动，在官网 http://www.nvidia.com/Download/index.aspx 进行查询，如我的显卡是Tesla V100，则对应的配置： 点击 Search，可以看到查询结果如下所示： Version: 418.87 Release Date: 2019.8.14 Operating System: Linux 64-bit CUDA Toolkit: 10.1 Language: English (US) File Size: 103.47 MB 卸载之前的Nvidia驱动 1sudo apt purge nvidia* 添加一个 PPA 源，命令如下： 1sudo add-apt-repository ppa:graphics-drivers/ppa 更新源： 1sudo apt-get update 安装Nvidia显卡驱动： 1sudo apt-get install nvidia-418 注意这里的418就是刚才根据我的机器型号查询出来的版本，以你实际查询出来的版本为准。 2）通过官网下载安装文件进行驱动安装首先，禁用 nouveau驱动。 打开文件: 1sudo vim /etc/modprobe.d/blacklist.conf 在末尾添加: 1blacklist nouveau 更新设置: 1sudo update-initramfs -u 重启系统： 1reboot 使用 lsmod 命令查看是否禁用成功 1lsmod | grep nouveau 若没有输出内容，则是禁用成功. 从官网下载驱动到本地，接着： 卸载旧驱动 1sudo apt purge nvidia* 进入到NVIDIA驱动所在目录,安装驱动 12# 安装驱动sudo sh NVIDIA-Linux-x86_64-410.66.run 3）检查驱动安装情况查看Nvidia驱动版本: 1sudo dpkg --list | grep nvidia- 大致会显示： 123ii nvidia-418 418.56-0ubuntu0~gpu16.04.1 amd64 NVIDIA binary driver - version 418.56ii nvidia-opencl-icd-418 418.56-0ubuntu0~gpu16.04.1 amd64 NVIDIA OpenCL ICDii nvidia-settings 418.56-0ubuntu0~gpu16.04.1 amd64 Tool for configuring the NVIDIA graphics driver 安装完毕后，查看显卡状态信息： 123nvidia-smi# 若上面的命令不可以，则尝试重启查看结果sudo reboot 安装CUDA 10.0 进入 CUDA Toolkit官方链接，根据自己的机器配置，选择相应安装包 根据上图中的提示，输入命令安装即可： 12# 不同CUDA版本命令不同，请注意！sudo sh cuda_10.1.243_418.87.00_linux.run 安装过程需要输入一些确认选项，过程如下： 123456789101112131415Do you accept the previously read EULA? # accept 接受accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 384.81? # 选否(y)es/(n)o/(q)uit: nInstall the CUDA 10.1 Toolkit? #yes(y)es/(n)o/(q)uit: yEnter Toolkit Location # 默认 [ default is /usr/local/cuda-10.1 ]: Do you want to install a symbolic link at /usr/local/cuda? # yes(y)es/(n)o/(q)uit: yInstall the CUDA 10.1 Samples? # 可装可不装(可以先选yes,然后用不到了再删除)(y)es/(n)o/(q)uit: y Enter CUDA Samples Location [ default is /home/ubuntu ]:Installing the CUDA Toolkit in /usr/local/cuda-10.1 ... 安装成功后,添加环境变量 1vim ~/.bashrc 在打开的文件中末尾添加: 1234# CUDAexport PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;export CUDA_HOME=/usr/local/cuda 保存并关闭文件,终端输入 12# 使环境生效source ~/.bashrc 使用 nvcc -V命令查看是否配置成功，成功后会显示如下信息： 安装cuDNNcuDNN 的全称是 The NVIDIA CUDA® Deep Neural Network library，是专门用来对深度学习加速的库，能够对TensorFlow、PyTorch及Theano等深度学习框架进行加速优化。 下载地址为：https://developer.nvidia.com/rdp/cudnn-download，需要注册之后才能下载。 cuDNN有两种方式进行安装，本文暂介绍压缩包的形式进行安装。 首先选择Download cuDNN v7.6.2 (July 22, 2019), for CUDA 10.0，然后选择cuDNN Library for Linux，如图所示： 解压进行安装： 12345tar -zxvf cudnn-9.0-linux-x64-v7.1.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/include/sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -dsudo chmod a+r /usr/local/cuda/include/cudnn.hsudo chmod a+r /usr/local/cuda/lib64/libcudnn* 查看安装的结果 1cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2 执行完如上命令之后，cuDNN 就安装好了，这时我们可以发现在 /usr/local/cuda/include 目录下就多了 cudnn.h 头文件。 安装Python 3.6.x参看：Python3.6的安装及相关设置 安装TensorFlow 安装TensorFlow： 1pip install tensorflow-gpu==1.14 验证TensorFlow是否安装成功 12345# 输出Hello, TensorFlow! 则代表安装成功。&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello)) 安装过程中出现的Error在安装过程中出现的问题大多都是 Linux内核版本与CUDA版本不匹配 Tensorflow_gpu与CUDA及cuDNN版本不匹配的问题 可参考此篇：Linux内核、Nvidia驱动、CUDA 版本与TensorFlow GPU之间的对应关系]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
        <tag>CUDA</tag>
        <tag>TensorFlow</tag>
        <tag>Nvidia Driver</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内核、Nvidia Driver、CUDA与TensorFlow间的版本对应关系]]></title>
    <url>%2Farticle%2FLinuxNvidiaCUDATensorFlowVersion.html</url>
    <content type="text"><![CDATA[在深度学习环境搭建的过程中出现的问题大多都是相关驱动及应用版本不匹配的问题，如： Linux内核版本与CUDA版本不匹配 TensorFlow GPU与CUDA及cuDNN版本不匹配等等 本文经查阅相应官网文档，将版本对应关系罗列如下： CUDA版本与Linux发行版本内核及GCC间的对应关系 Distribution Kernel* GCC GLIBC ICC PGI XLC CLANG RHEL 8.0 4.18 8.2.1 2.28 RHEL 7.6 3.10 4.8.5 2.17 19.0 18.x, 19.x NO 8.0.0 RHEL 6.10 2.6.32 4.4.7 2.12 CentOS 7.6 3.10 4.8.5 2.17 CentOS 6.10 2.6.32 4.4.7 2.12 Fedora 29 4.16 8.0.1 2.27 OpenSUSE Leap 15.0 4.15.0 7.3.1 2.26 SLES 15.0 4.12.14 7.2.1 2.26 SLES 12.4 4.12.14 4.8.5 2.22 Ubuntu 18.10 4.18.0 8.2.0 2.28 Ubuntu 18.04.3 (**) 5.0.0 7.4.0 2.27 Ubuntu 16.04.6 (**) 4.4 5.4.0 2.23 Ubuntu 14.04.6 (**) 3.13 4.8.4 2.19 — — — — 来源：Table 1. Native Linux Distribution Support in CUDA 10.1 Update 2 Linux 系统驱动安装失败表现为 nvidia-smi 无法工作，一般有下面几个常见原因： 系统缺乏编译 kernel module 所需要的包，如 gcc，kernel-devel-xxx 等，导致无法编译，最终安装失败。 系统里面存在多个版本的 kernel，由于 DKMS 的不正确配置，导致驱动编译为非当前版本 kernel 的 kernel moudule，导致 kernel module 安装失败。 安装驱动后，升级了 kernel 版本导致原来的安装失效。 Nvidia驱动版本与CUDA版本的对应关系 CUDA Toolkit Linux x86_64 Driver Version Windows x86_64 Driver Version CUDA 10.1 (10.1.105 general release, and updates) &gt;= 418.39 &gt;= 418.96 CUDA 10.0.130 &gt;= 410.48 &gt;= 411.31 CUDA 9.2 (9.2.148 Update 1) &gt;= 396.37 &gt;= 398.26 CUDA 9.2 (9.2.88) &gt;= 396.26 &gt;= 397.44 CUDA 9.1 (9.1.85) &gt;= 390.46 &gt;= 391.29 CUDA 9.0 (9.0.76) &gt;= 384.81 &gt;= 385.54 CUDA 8.0 (8.0.61 GA2) &gt;= 375.26 &gt;= 376.51 CUDA 8.0 (8.0.44) &gt;= 367.48 &gt;= 369.30 CUDA 7.5 (7.5.16) &gt;= 352.31 &gt;= 353.66 CUDA 7.0 (7.0.28) &gt;= 346.46 &gt;= 347.62 来源：Table 1. CUDA Toolkit and Compatible Driver Versions 关于Nvidia驱动版本和自己机器硬件的版本关系，可通过云服务器深度学习服务器环境搭建来查询。 Tensorflow GPU版本与CUDA及cuDNN版本的对应关系 版本 Python 版本 编译器 编译工具 cuDNN CUDA tensorflow_gpu-1.13.1 2.7、3.3-3.6 GCC 4.8 Bazel 0.19.2 7.4 10.0 tensorflow_gpu-1.12.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9 tensorflow_gpu-1.11.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9 tensorflow_gpu-1.10.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9 tensorflow_gpu-1.9.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.11.0 7 9 tensorflow_gpu-1.8.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 7 9 tensorflow_gpu-1.7.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9 tensorflow_gpu-1.6.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9 tensorflow_gpu-1.5.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.8.0 7 9 tensorflow_gpu-1.4.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.5.4 6 8 tensorflow_gpu-1.3.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 6 8 tensorflow_gpu-1.2.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 5.1 8 tensorflow_gpu-1.1.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8 tensorflow_gpu-1.0.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8 来源：TensorFlow安装 目前而言，tensorflow_gpu-1.13.1及以上安装CUDA 10版本，tensorflow_gpu-1.12.0到tensorflow_gpu-1.4.0安装CUDA 9版本，tensorflow_gpu-1.4.0版本以下安装CUDA 8版本。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
        <tag>CUDA</tag>
        <tag>TensorFlow</tag>
        <tag>Nvidia Driver</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装MySQL并实现远程链接]]></title>
    <url>%2Farticle%2FCentOSMySQL.html</url>
    <content type="text"><![CDATA[Navicat是一套数据库管理工具，专为简化数据库的管理及降低系统管理成本而设。Navicat 是以直觉化的图形用户界面而建的，可以安全和简单地创建、组织、访问并共用信息。 本文将介绍如何在云服务器中的CentOS 7.x版本下安装MySQL及基于Navicat实现数据库的远程链接。 安装MySQL进入目录： 1cd /usr/local/src/ 安装MySQL： 123456# 下载安装包wget http://repo.mysql.com/mysql57-community-release-el7-8.noarch.rpm# 安装rpm包rpm -ivh mysql57-community-release-el7-8.noarch.rpm# 安装 mysql-serveryum -y install mysql-server 重启MySQL： 12service mysqld restart# 或 /etc/init.d/mysqld start 获取mysql创建的随机密码 123grep "password" /var/log/mysqld.log# 会显示类似如下： # [Note] A temporary password is generated for root@localhost: &lt;G142uD),u59 通过上面命令得到的随机密码登录MySQL： 1mysql -u root -p 重置密码： 1234567# 设置密码，注意密码的格式 一般为大写+小写+数字+特殊符号！mysql&gt; alter user 'root'@'localhost' identified by 'Password123!';# 使用' flush privileges '刷新权限mysql&gt; flush privileges;# 退出MySQLmysql&gt; exit;# 注，SQL命令最后有分号";" 开机自启（自选）： 12345678# 设置开机自启systemctl enable mysqld.service# 启动MySQLsystemctl start mysqld# 停止MySQLsystemctl stop mysqld# 查看MySQL状态systemctl status mysqld 实现远程链接12345678910# 登录MySQL：mysql -u root -p# 选择MySQL：mysql&gt; use mysql;# 若允许所有用户远程访问 修改用户名和密码为你自己的mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'USERNAME'@'%' IDENTIFIED BY 'PASSWORD' WITH GRANT OPTION;# 若允许单个ip 修改用户名和密码为你自己的mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'USERNAME'@'1.2.3.4' IDENTIFIED BY 'PASSWORD' WITH GRANT OPTION;# 最后，是上述修改立即生效mysql&gt; FLUSH PRIVILEGES; 注，一定要前往服务商，修改服务器安全组规则，新增3306端口。 于Navicat客户端中建立远程链接： 以下为Ubuntu中设置远程链接的教程，同时Ubuntu也可通过上述方法来设置远程链接。 12345678910111213141516171819202122232425# 登陆mysql$ mysql -u root -pmysql&gt; use mysql;mysql&gt; update user set host = '%' where user = 'root';mysql&gt; select host, user from user;+-----------+------------------+| host | user |+-----------+------------------+| % | root || localhost | debian-sys-maint || localhost | mysql.session || localhost | mysql.sys |+-----------+------------------+4 rows in set (0.00 sec)# ok 退出MySQL 重启服务mysql&gt; quit;$ service mysqld restart# 发现客户端远程还是连接不上 继续修改mysql.cnf配置文件# 需要root权限，配置文件是只读的$ sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf# 往下翻，注释掉这一行,保存退出# bind-address = 127.0.0.1# 重启服务$ service mysqld restart# 以上，就可以了]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>MySQL</tag>
        <tag>Centos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[济南实习面试之记]]></title>
    <url>%2Farticle%2FJinanInterview.html</url>
    <content type="text"><![CDATA[老早就想写一篇杂文来记录18年7月29-30日来济南参加「实习面试」的过程，种种原因拖到今日。也因昨日从家返回济南，相似的场景使我犹记起当初来济的情形，望通过此文来缅怀那时的经历及感受，特此记录。 7月29日 济宁，记不清什么天气 于29日傍晚从济宁出发，坐上了火车去拉萨济南，去看那神奇的布达拉千佛山。济宁火车站这个破站我还是要吐槽一下，估计除了里面的座椅，其他估计二三十年没变过，而且停靠的基本都是绿皮车。 济南，天黑黑 约晚上九点半抵达济南，新鲜感十足。眼望着来来往往的人群及周围建筑上明亮的灯光，感觉当时双眼是放光的，一是对这座城市的陌生感的兴奋，另外就是对自己未来的憧憬。虽然小时候来过一次，但和没来没什么区别。 从济南站走了一小会，便找到早早于携程上预订的宾馆，犹记老板还吐槽说我是入住最晚的住客。为了让自己明天保持好的状态，便匆匆洗漱完就入睡了。 7月30日 济南，早上清爽 一大清早就起床洗漱及收拾行李，在地图上搜了下附近的文印店便去打印简历。路过保亭买了最喜欢喝的红枣酸奶当做早餐，相对于厦门的最低8.9元/瓶，济南这边的6.4元/瓶，我还是挺震惊的。 于高德地图上规划了去往山东省人民检察院的路线，便在经四纬三站等K101路公交车。清晨，骑着电动车的年轻人，出门散早步的大爷大妈，飞驰的汽车慢慢多了起来。路边葱葱郁郁的桦树，枝头叽叽咋咋歌唱的鸟儿，干净的柏油路，这一切都使的我心情愉悦。 坐着公交车K101从经四纬三站到甸柳庄站，步行一小会便到了省检察院大门口。 和领导拨通了电话，便进了检察院3大楼会议室进行了面试。面试还是主要以问我简历上的项目为主，回答的还算顺利。半个小时后结束了面试，便与领导握手告辞。 从检察院出来后才十点半，心想着去大明湖看一看，便坐K50路公交车从二环东路和平路站到天地坛街站。当时芙蓉街还在装修，便穿街走巷抵达百花洲，水里的锦鲤可是真大，一看见人就立马游过来坐等投喂。此时正值暑假，来这游玩的游客络绎不绝，一个个小朋友争抢的给锦鲤喂食，不亦乐乎。 趴在百花洲的石栏杆上挺久，一直在看小孩子投喂锦鲤，看的我也不亦乐乎🤦‍♂️，哈哈哈！ 而后穿过马路，便就到了大明湖。一开始还以为要门票，进去之后发现只有湖心小岛收来回船票。从南门进去了，便从右侧围着大明湖边走边看，也是因为假期，里面的人儿甚是热闹。途中，走累了便在一大石头上坐下休息，静静的观望着着大明湖，尽情的让微风吹拂自己的脸颊，沉醉其中，不能自拔。 步行至大明湖东门时，手机仅剩不到10%的电量，怕手机没电无法刷二维码乘坐公交车，遂紧忙规划了路线前往火车站，后来发现自己竟坐反了方向🤦‍♂️。不过济南的公交有这么一个特点，就算你坐反了，一样能到达目的站点，因为每条线都基本上不停歇绕个圈开始返程，只是时间长一些。 到达火车站后，便去旁边的肯德基里面进行充电，顺便买了点吃的当做晚饭。令我惊喜的是，不一会就接到了领导的电话，告知我被录用，并说明了相关情况。经过自己的思考及和同学的聊天，我便把该事答应了下来。而后在火车站前面的广场上走了走，便进了候车室进行等候。 济宁，天黑黑 到达济宁站月晚上八点多，整个人无比轻松。 此次行程终结。 附图]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>济南</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac中安装OpenCV]]></title>
    <url>%2Farticle%2FMacInstallOpenCV.html</url>
    <content type="text"><![CDATA[继前面在Mac上安装了Pytorch后（在Mac中搭建PyTorch网络框架），为方便在本地环境上对部分图像处理的算法进行测试，打算在Mac上安装OpenCV。 看了不少教程，发现有些教程中的命令早已老旧无法运行，有些教程中安装过程太过繁杂，怕步骤太多，安装中出了错会影响之前安装的环境，所以综合之后整理了本篇教程，并分给出了不同情况下的处理办法。 情况零：若电脑已经安装了Python，可通过以下命令安装OpenCV： 12# 电脑已安装Pythonpip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-contrib-python 测试： 123456Python 3.7.4 (default, Oct 12 2019, 18:55:28)[Clang 11.0.0 (clang-1100.0.33.8)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import cv2&gt;&gt;&gt; cv2.__version__'4.3.0' 如果没有报错，说明安装成功了。 情况一：如果在没有安装Python3的情况下安装OpenCV，会自动安装最新的Python版本，并将该版本设置为系统默认Python。 直接运行下面的命令安装OpenCV： 12# 没有安装Homebrew可以查看：《在Mac中搭建PyTorch网络框架》brew install opencv 测试： 12345Python 3.7.4 (default, Oct 12 2019, 18:55:28) [Clang 11.0.0 (clang-1100.0.33.8)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import cv2&gt;&gt;&gt; 如果没有报错，说明安装成功了，恭喜！ 情况二：当Mac中之前已经安装Python 3.6，则在上面的Python3.7的安装过程中会报错。报错的命令忘记截图了，我根据系统的报错，总结了以下解决办法。 注：Mac 默认安装Python2.7，此步骤对Python2.7版本没有影响。 首先删除老版本Python 3.6 12345678910111213# 1.查看Python版本python -V# 显示：Python 3.6# 2.删除Python 3.6 框架ls /Library/Frameworks/Python.framework/Versions/# 显示：3.6sudo rm -rf /Library/Frameworks/Python.framework/Versions/3.6# 3.在应用目录中手动将程序移到废纸篓# 或者用以下命令删除cd /Applicationssudo rm -rf Python\ 3.6/ # Python 3.7存在空格 在&#39;/usr/local/bin&#39;下，首先罗列出所有需要删除的软连接： 123# To list all files that would be deleted:brew link --overwrite --dry-run python# 然后对冲突的文件一一删除（我是一一删除后，然后执行的下面的命令，按理直接运行下面的程序会覆盖原来的，这点需要考证） 覆盖之前Python的软连接： 12# To force the link and overwrite all conflicting files:brew link --overwrite python 修改系统环境变量： 1vim ~/.bash_profile 在.bash_profile最后添加如下代码（根据自己的路径来修改） 123# Setting PATH for Python 3.7alias python="/usr/local/Cellar/python/3.7.4_1/bin/python3"alias python2="/usr/bin/python2.7" 使得环境变量生效： 1source ~/.bash_profile 情况三：如果安装上面教程安装成功了，恭喜你不用再往下面看了，如果没有成功，我们需要手动将OpenCV与系统中Python建立关联。 注意到我们上面使用Homebrew安装了程序，Homebrew安装的程序位置都在该目录下。 1/usr/local/Cellar/ 找到的OpenCV的cv2.***.so文件，我的.so文件位置为： 1/usr/local/Cellar/opencv/4.1.2/lib/python3.7/site-packages/cv2/python-3.7/cv2.cpython-37m-darwin.so 下面就是和python3环境关联，进入Python3 的site-packages文件夹下面和原来上面的so建立一个软链接。 12cd /usr/local/lib/python3.7/site-packages ln -s /usr/local/Cellar/opencv/4.1.2/lib/python3.7/site-packages/cv2/python-3.7/cv2.cpython-37m-darwin.so cv2.so 测试效果： 123&gt;&gt;&gt; import cv2 &gt;&gt;&gt; cv2.__version__ '4.1.2' 若出现此情况，则说明安装成功。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>OpenCV</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Mac中搭建PyTorch网络框架]]></title>
    <url>%2Farticle%2FMacWithPyTorch.html</url>
    <content type="text"><![CDATA[近期一直在研究PyTorch这个网络，发现真的是很好用啊！ 首先，网络比较轻量；此外，网络框架的灵活性也比较高。相比于TensorFlow真的是很好入手，而且逻辑结构十分清晰！为Facebook点个赞！ 在macOS Majave 10.14.4版本下进行PyTorch的安装，过程中遇到一些问题，在此对安装的过程及解决问题的方法进行一个记录。 Pip安装PyTorch根据PyTorch官网上的说明 指令为：pip3 install torch torchvision 安装完毕后对PyTorch进行测试，报错如下： 12345678&gt;&gt;&gt; import torchTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/__init__.py", line 79, in &lt;module&gt; from torch._C import *ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/lib/libshm.dylib Reason: image not found 提示没有libomp.dylib这个库。 1brew install libomp 然后进一步测试： 12345Python 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 05:52:31) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import torch&gt;&gt;&gt; 问题解决！ 安装HomeBrew (可选)若Mac中没有安装brew工具，请参考此处的教程。 Homebrew是基于Ruby的（Mac电脑默认是安装的1.8.7版本的ruby），直接将下面的代码粘贴到Terminal中执行： 1/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 过程中需要多次回车进行确认。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>Mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[云服务器部署Python Web环境(Nginx+Spawn-fcgi+web.py)]]></title>
    <url>%2Farticle%2FWebpyNginxFastCGI.html</url>
    <content type="text"><![CDATA[Web.py是一款轻量级的python web开发框架，其简单、高效、学习门槛低，特别适合作为Python Web开发的入门框架，并可以为云服务器中相关应用提供便捷接口。 本教程在云服务器上安装了WSGI (Web Server Gateway Interface )接口 及 Http Sever (如 Apache、Nginx等)。 Python Web环境 -&gt; Web.py + Nginx + Spawn-fcgi 注：本教程在Python3.6版本运行，安装可参考：Python3.x的安装及相关设置 更新了：云服务器部署Python Web环境(Flask+Nginx+Gunicorn+Supervisor)，后面的工作将主要会基于Flask进行开展。 安装Web.pyUbuntu下可使用下述命令： 1sudo pip install web.py 通用安装方法： 下载 地址 http://webpy.org/static/web.py-0.37.tar.gz 随便解压到一个路径（安装后就不需要了） 安装命令 1python setup.py install 本地测试web.py，新建hello.py内容如下： 123456789101112131415161718192021#!/usr/bin/env python# -*- coding: utf-8 -*-# 导入web.pyimport web# 添加URL：“/”(即首页)通过index类进行处理urls = ( '/', 'index')# 创建一个上方声明的URL列表对应的application，application会在这个文件的全局命名空间中查找对应类app = web.application(urls, globals())# 创建usls中映射的类class index: def GET(self): return "Hello, world!" if __name__ == "__main__": app.run() # 启动应用 运行该代码： 1python hello.py 打开浏览器，访问xx.xx.xx.xx:8080（其中xx.xx.xx.xx更换为云服务器的公网IP），显示结果为： Hello, World! 注：默认端口为8080 可以通过python hello.py 8000来设定启动端口。 安装Nginx Ubuntu下可使用下述命令： 1sudo apt-get install nginx 安装好的文件位置： /usr/sbin/nginx：主程序 /etc/nginx：存放配置文件 /usr/share/nginx：存放静态文件 /var/log/nginx：存放日志 其实从上面的根目录文件夹可以知道，Linux系统的配置文件一般放在/etc，日志一般放在/var/log，运行的程序一般放在/usr/sbin或者/usr/bin。 如果要更清楚Nginx的具体配置项，可以打开/etc/nginx/nginx.conf 对于其他Linux系统可参照官网链接：https://www.nginx.com/resources/wiki/start/topics/tutorials/install/ 启动Nginx： 12# 启动Nginxsudo /etc/init.d/nginx start 其他相关命令： 1234# 关闭Nginxsudo /etc/init.d/nginx stop# 重启Nginxsudo /etc/init.d/nginx restart 访问xx.xx.xx.xx（其中xx.xx.xx.xx更换为云服务器的公网IP），显示结果为： Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org.Commercial support is available at nginx.com. Thank you for using nginx. 安装Spawn-fcgi 及 Flup 安装Spawn-fcgi 1sudo apt-get install spawn-fcgi 安装Flup 1pip install flup 配置 nginx.conf 支持 fastcgi 12# 编辑配置文件vim /etc/nginx/nginx.conf 在http内如下之处： 1234http&#123; server&#123;&#125;&#125; 添加以下代码： 1234567891011121314151617181920212223242526server&#123; listen 80; server_name xx.xx.xx.xx; # 其中xx.xx.xx.xx更换为云服务器的公网IP index index.html index.php; location /hello &#123; root /opt/webpy-test; # 应用webpy-test所在的目录 include fastcgi_params; # 包含默认的fastcgi参数 fastcgi_param SCRIPT_FILENAME $fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_script_name; fastcgi_pass 127.0.0.1:9001; # 把请求通过fastcgi传送给本机的9001端口 &#125; location /static/ &#123; # 配置静态文件的访问 if (-f $request_filename) &#123; # 如果请求文件名是一个文件 rewrite ^/static/(.*)$ /static/$1 break; # 直接跳转到对应的资源，中断fastcgi的传输 &#125; &#125; #location /helloword &#123; # root /opt/webpy-test2; # 应用webpy-test2所在的目录 # include fastcgi_params; # 包含默认的fastcgi参数 # fastcgi_param SCRIPT_FILENAME $fastcgi_script_name; # fastcgi_param PATH_INFO $fastcgi_script_name; # fastcgi_pass 127.0.0.1:9002; # 把请求通过fastcgi传送给本机的9002端口 #&#125;&#125; 特别注意：【此处尤为重要，一开始始终无法正常显示，我花费了一下午的时间才解决相关问题】 在云服务器控制界面添加80端口； 将80端口添加至防火墙：sudo ufw allow 80； 在云服务器控制界面添加9001端口，并将其添加至防火墙； 其他同上…… 修改源代码如果想要使用Nginx，则必须添加以下代码： web.wsgi.runwsgi = lambda func, addr=None: web.wsgi.runfcgi(func, addr) 新建程序index.py为： 123456789101112131415#!/usr/bin/env python# -*- coding: utf-8 -*-import weburls = ("/.*", "hello")app = web.application(urls, globals())class hello: def GET(self): return 'Hello, world!'if __name__ == "__main__": web.wsgi.runwsgi = lambda func, addr=None: web.wsgi.runfcgi(func, addr) # Neew added app.run() 设置代码可执行权限： 1chmod +x index.py 启动应用 启动一个Spawn-fcgi进程: 123# 启动进程spawn-fcgi -d /path/to/www -f /path/to/www/index.py -a 127.0.0.1 -p 9001# 我的是：spawn-fcgi -d /opt/webpy-test -f /opt/webpy-test/index.py -a 127.0.0.1 -p 9001 123# 关闭进程kill `pgrep -f "python /path/to/www/index.py"`# 我的是：kill `pgrep -f "python /opt/webpy-test/index.py"` 其中/path/to/www为程序所在路径。 -f 指定调用FastCGI的web文件，web程序的入口文件，即code.py文件-d 指定web程序的主目录，即code.py所在的目录-a 绑定到地址 addr-p 绑定到端口 port-F 指定产生的 FastCGI 的进程数-P 指定产生的进程的 PID 文件路径-u 和 -g FastCGI 使用什么身份运行 注：可以随意填写地址和端口信息，但是一定需要和Nginx配置文件相匹配。 启动Nginx 12# 启动Nginxsudo /etc/init.d/nginx start 查看9001端口是否存在 1netstat -ano |grep 9001 若存在则显示： tcp 0 0 127.0.0.1:9001 0.0.0.0:* LISTEN off (0.00/0/0) 访问xx.xx.xx.xx/hello（其中xx.xx.xx.xx云服务器的公网IP），显示结果为： Hello, world! Debug 运行python hello.py yourip:8080 报错socket.error: No socket could be created 该情况往往是因为端口被占用，是因为我之前配置Java Web环境占用了8080端口。 解决方法： 1）关闭相应程序，释放8080端口 2）在云服务器开放新的端口，供web.py使用 运行spawn-fcgi -d /opt/webpy-test -f /opt/webpy-test/index.py -a 127.0.0.1 -p 9001 提示spawn-fcgi: child exited with: 126 因为index.py脚本缺少执行权限 解决办法： 通过chmod +x index.py添加执行权限。 运行spawn-fcgi -d /opt/webpy-test -f /opt/webpy-test/index.py -a 127.0.0.1 -p 9001 提示spawn-fcgi: child exited with: 2 解决办法： 在index.py的开头中添加#!/usr/bin/env python 运行spawn-fcgi -d /opt/webpy-test -f /opt/webpy-test/index.py -a 127.0.0.1 -p 9001 提示spawn-fcgi: child exited with: 1 应该是index.py脚本有错误 解决办法： 先对该脚本进行debug。 启动Nginx时 123# 报错[....] Restarting nginx (via systemctl): nginx.serviceJob for nginx.service failed because the control process exited with error code. See "systemctl status nginx.service" and "journalctl -xe" for details. failed! 通过执行systemctl status nginx.service提示： 12Apr 01 19:12:38 iZuf636nqxmkad2s6umlm3Z nginx[15377]: nginx: [emerg] "server" directive is not allowed here in /etc/nginx/nginx.confApr 01 19:12:38 iZuf636nqxmkad2s6umlm3Z nginx[15377]: nginx: configuration file /etc/nginx/nginx.conf test failed 解决方法： 一般是修改nginx.conf时出了问题，请严格参照 上文中的3.3 。 运行时报错ImportError: No module named _dummy_thread 基本确定是python2.7中没有_thread模块的原因，推荐安装Python3.6。 pip3 安装web.py报错 通过下述命令进行安装： 1pip3 install web.py==0.40.dev0 参考文献 Webpy + Nginx with FastCGI搭建Web.py nginx[+spawn-fcgi]+flup+webpy服务搭建]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Spawn-fcgi</tag>
        <tag>web.py</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3.x的安装及相关设置]]></title>
    <url>%2Farticle%2FUbuntuUpdatePython.html</url>
    <content type="text"><![CDATA[当搭建Python Web环境及开发深度学习相关应用时，Python2.7已经显得十分无力，并与很多最新的组件不相兼容，并且官方已发布公告表示2020年后便不再维护Python2系列版本，所以对Python3的需求迫在眉睫。 对于Ubuntu安装 Python3.X1234567891011# 新增Python3.6 ppa源sudo apt-get install software-properties-commonsudo add-apt-repository ppa:jonathonf/python-3.6# 新增Python3.7 ppa源，其他版本类同# sudo apt-get install software-properties-common# sudo add-apt-repository ppa:jonathonf/python-3.7sudo apt-get update # 安装Python3.6sudo apt-get install python3.6# 安装Python3.7# sudo apt-get install python3.7 123456789# 设置Python3.6为系统默认cd /usr/binrm python ln -s python3.6m python # 若上面安装Python3.7，则执行下发的命令# ln -s python3.7m python# 检查Python版本python -V# 显示为：Python 3.6.7 (default, Oct 25 2018, 09:16:13) 安装、升级pip1234567# 安装pipapt-get install python3-pip python3-dev build-essential# 升级pippython -m pip install --upgrade pip# 检查pip版本pip -V# 显示为：pip 19.0.3 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6) MacOSMacOS中大可也可以按照上文中介绍的来实现。 如果使用iTerm2 + oh-my-zsh 1234567vim ~/.zshrc# 添加这一行alias python="/usr/local/bin/python3"# 保存后，执行source ~/.bash_profile]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04配置Java Web环境]]></title>
    <url>%2Farticle%2FJavaTomcatMysql.html</url>
    <content type="text"><![CDATA[前些日子帮师妹的服务器配置Java Web环境，自己无形中也学到了写新知识，擅自总结记录下来，方便以后自己及师弟师妹查阅参考。 Java Web环境 = JavaJDK + Tomcat + MySQL 下载 JavaJDK 及 Tomcat进入/opt目录： 1cd /opt 下载 JavaJDK 直接用wget命令下载到云服务器上，解压的时候会报错如下（因为不是二进制下载）： 123gzip: stdin: not in gzip formattar: Child returned status 1tar: Error is not recoverable: exiting now 建议下载到本地，再传到云服务器上。 进入oracle官方下载地址，找到所需的 JavaJDK 版本，选中Accept License Agreement单选框，点击进行下载。 附加说明：在oracle官网下载的时候，需要关注一下你的机器类型，64位机型的选X64，32位机型的选X86。 下载 Tomcat 用wget命令下载到云服务器上: 1wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.17/bin/apache-tomcat-9.0.17.tar.gz 也可以进入apache官网，左边Download栏下让你选择你需要的Tomcat版本，找到tar.gz包，右键复制链接地址，使用wget下载最新版本。 进行解压 12345# 解压 JavaJDK# 需要将tar -zxvf 后的 ‘apache-tomcat-9.0.7.tar.gz’ 修改为你下载的压缩包的名称tar -xzvf apache-tomcat-9.0.7.tar.gz# 为方便后续操作 将解压后的 ‘apache-tomcat-9.0.17’修改为‘tomcat’mv apache-tomcat-9.0.17 tomcat 12345# 解压 Tomcat# 需要将tar -zxvf 后的 ‘jdk-8u201-linux-x64.tar.gz’ 修改为你下载的压缩包的名称tar -xzvf jdk-8u201-linux-x64.tar.gz# 为方便后续操作 将解压后的 ‘jdk-8u201-linux-x64’修改为‘jdk’mv jdk-8u201-linux-x64 jdk 修改JavaJDK配置文件 配置 JavaJDK 用vi打开配置文件 1sudo vim /etc/profile 在/etc/profile 末尾部添加以下配置： 1234567#set java EnvironmentJAVA_HOME=/opt/jdkPATH=$JAVA_HOME/bin:$PATHLASSPATH=:.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JAVA_HOMEexport PATHexport CLASSPATH 生效配置： 1source /etc/profile 测试配置是否生效： 1echo $JAVA_HOME 测试Java是否安装成功: 1java -version 可以看到Java的版本，说明 Java 安装成功了。 修改Tomcat配置文件进入Tomcat的bin目录 1cd /opt/tomcat/bin 修改start.sh文件 1sudo vim startup.sh 跳到最后，在 exec &quot;$PRGDIR&quot;/&quot;$EXECUTABLE&quot; start &quot;$@&quot; 这一行的上面添加： 12345678#set java environmentexport JAVA_HOME=/opt/jdkexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:%&#123;JAVA_HOME&#125;/lib:%&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH #tomcatexport TOMCAT_HOME=/opt/tomcat 修改shutdown.sh文件 1sudo vim shutdown.sh 跳到最后，在 exec &quot;$PRGDIR&quot;/&quot;$EXECUTABLE&quot; stop &quot;$@&quot; 这一行的上面添加： 12345678#set java environmentexport JAVA_HOME=/opt/jdkexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:%&#123;JAVA_HOME&#125;/lib:%&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH #tomcatexport TOMCAT_HOME=/opt/tomcat 启动Tomcat 12# 进入到 /opt/tomcat/bin 目录sudo ./startup.sh 显示如下： 将8080端口添加至防火墙 1sudo ufw allow 8080 测试是否成功 查看端口号/进程： 1sudo netstat -naptl 查看进程： 1ps -ef |grep 8080/tcp 在浏览器访问8080端口： 关闭Tomcat 12# 进入到 /opt/tomcat/bin 目录sudo ./shutdown.sh 安装 MySQL 安装MySQL服务器及客户端 123sudo apt-get install mysql-serversudo apt-get install mysql-clientsudo apt-get install libmysqlclient-dev 注意：安装过程中需要设置密码 检查是否安装成功 1sudo netstat -tap | grep mysql 如果看到有MySQL的socket处于listen状态则表示安装成功。 创建远程用户 1mysql -u root -p -u 表示选择登陆的用户名， -p 表示登陆的用户密码，上面命令输入之后会提示输入密码，此时输入密码就可以登录到mysql。 查看当前的数据库： 1show databases; 使用数据库： 1use mysql; 查看表： 1show tables;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Java</tag>
        <tag>Tomcat</tag>
        <tag>JavaJDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python--批量更改Markdown文件中的七牛云图床链接]]></title>
    <url>%2Farticle%2FImageHostingChange.html</url>
    <content type="text"><![CDATA[背景介绍这两天阿里云将我的域名（wangcong.info）的备案取消接入并向工信部提交了注销申请。于是乎，我先后收到了几个短信和邮件通知，贴出一条如下： 今早，本站点中图片便已经无法正常显示，我赶紧用早已备案的另外一域名（babibobi.com）在七牛云上予以了替换，暂时解决图片不能访问的问题。 因为Markdown文件中有很多图片链接，一一更换的话，工作量略大，通过查资料，写了一个Python脚本，进而可以实现一键批量更改Markdown文件中的七牛云图床链接的功能。具体如下： 更改后的md文档： 实现代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# usr/bin/env python# -*- coding:utf-8 -*-import re, os, time, argparseimport sys, iofrom itertools import chain# Markdown中图片语法 ![](url) 或者 &lt;img src='' /&gt;global img_pattenimg_patten = r'!\[.*?\]\((.*?)\)|&lt;img.*?src=[\'\"](.*?)[\'\"].*?&gt;'def replace_md_url(md_file): """ 批量更改Markdown文件中的七牛云图床链接 :param md_file: Markdown文件 :return: """ if os.path.splitext(md_file)[1] != '.md': print('&#123;&#125; 不是Markdown文件，不做处理。'.format(md_file)) return num_replace = 0 # 本次操作时间戳 dir_ts = time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime()) with io.open(md_file, 'r', encoding='utf-8') as f: #使用utf-8 编码打开 post = f.read() matches = re.compile(img_patten).findall(post) if matches and len(matches)&gt;0 : # 多个group整合成一个列表 for match in list(chain(*matches)) : if match and len(match)&gt;0 : print("match pic : ", match) new_url = match.replace('wangcong.info','babibobi.com') # 更新MarkDown文件中的URL if new_url : post = post.replace(match, new_url) num_replace = num_replace + 1 # 如果有内容的话，就直接覆盖写入当前的MarkDown文件 if post and num_replace &gt; 0: io.open(md_file, 'w', encoding='utf-8').write(post) print('&#123;0&#125; 中有&#123;1&#125;个URL被替换/&#123;2&#125;'.format(os.path.basename(md_file), num_replace, dir_ts)) elif num_replace == 0: print('&#123;&#125; 中没有需要替换的URL'.format(os.path.basename(md_file)))if __name__ == '__main__': fileset = os.listdir(sys.argv[1]) for filename in fileset: print filename absfile = os.path.join(sys.argv[1], filename) replace_md_url(absfile)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>七牛云</tag>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras快速教程]]></title>
    <url>%2Farticle%2FKerasTutorial.html</url>
    <content type="text"><![CDATA[最近所接触的项目基本都是使用Keras及TensorFlow共同实现的，准确的说是使用Keras作为入口，使用TensorFlow作为后端来开展的神经网络相关模型的训练、测试及应用工作。 Keras是一个高层神经网络API，Keras由纯Python编写而成并基Tensorflow、Theano以及CNTK后端。Keras 为支持快速实验而生，能够把你的idea迅速转换为结果，如果你有如下需求，请选择Keras： 简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性） 支持CNN和RNN，或二者的结合 无缝CPU和GPU切换 简单地说，好嗨呦，感觉人生已经到达了高潮，就是快！ 快速入门：123456789101112131415161718192021from keras.models import Sequentialfrom keras.layers import Dense# 构建模型model = Sequential()model.add(Dense(units=64, activation='relu', input_dim=100))model.add(Dense(units=10, activation='softmax'))# 配置优化器model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])# 模型训练model.fit(data_train, labels_train, epochs=5, batch_size=32)# 模型性能评估loss_and_metrics = model.evaluate(data_test, labels_test, batch_size=128)# 对新的数据进行预测classes = model.predict(data_test, batch_size=128) 项目示例经过一段时间的项目开发，关于深度学习的目录构建推荐如下： 12345678910.├── datasets│ └── xxxx //数据集├── models│ ├── model-weights.h5 //保存的模型│ └── pretrain_model│ └── keras.h5 //预训练模型├── modle.py //神经网络结构├── train.py //训练的脚本└── test.py //测试的脚本 关于model.py脚本内容示例如下 (不保证运行，下同)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# usr/bin/env python# -*- coding:utf-8 -*-from keras.models import Modelfrom keras.layers.core import Dense, Dropout, Activation, Reshape, Permutefrom keras.layers.convolutional import Conv2D, Conv2DTranspose, ZeroPadding2Dfrom keras.layers.pooling import AveragePooling2D, GlobalAveragePooling2Dfrom keras.layers import Input, Flattenfrom keras.layers.merge import concatenatefrom keras.layers.normalization import BatchNormalizationfrom keras.regularizers import l2from keras.layers.wrappers import TimeDistributeddef conv_block(input, growth_rate, dropout_rate=None, weight_decay=1e-4): x = BatchNormalization(axis=-1, epsilon=1.1e-5)(input) x = Activation('relu')(x) x = Conv2D(growth_rate, (3,3), kernel_initializer='he_normal', padding='same')(x) if(dropout_rate): x = Dropout(dropout_rate)(x) return xdef dense_block(x, nb_layers, nb_filter, growth_rate, droput_rate=0.2, weight_decay=1e-4): for i in range(nb_layers): cb = conv_block(x, growth_rate, droput_rate, weight_decay) x = concatenate([x, cb], axis=-1) nb_filter += growth_rate return x, nb_filterdef transition_block(input, nb_filter, dropout_rate=None, pooltype=1, weight_decay=1e-4): x = BatchNormalization(axis=-1, epsilon=1.1e-5)(input) x = Activation('relu')(x) x = Conv2D(nb_filter, (1, 1), kernel_initializer='he_normal', padding='same', use_bias=False, kernel_regularizer=l2(weight_decay))(x) if(dropout_rate): x = Dropout(dropout_rate)(x) if(pooltype == 2): x = AveragePooling2D((2, 2), strides=(2, 2))(x) elif(pooltype == 1): x = ZeroPadding2D(padding = (0, 1))(x) x = AveragePooling2D((2, 2), strides=(2, 1))(x) elif(pooltype == 3): x = AveragePooling2D((2, 2), strides=(2, 1))(x) return x, nb_filterdef dense_cnn(input, nclass): _dropout_rate = 0.2 _weight_decay = 1e-4 _nb_filter = 64 # conv 64 5*5 s=2 x = Conv2D(_nb_filter, (5, 5), strides=(2, 2), kernel_initializer='he_normal', padding='same', use_bias=False, kernel_regularizer=l2(_weight_decay))(input) # 64 + 8 * 8 = 128 x, _nb_filter = dense_block(x, 8, _nb_filter, 8, None, _weight_decay) # 128 x, _nb_filter = transition_block(x, 128, _dropout_rate, 2, _weight_decay) # 128 + 8 * 8 = 192 x, _nb_filter = dense_block(x, 8, _nb_filter, 8, None, _weight_decay) # 192 -&gt; 128 x, _nb_filter = transition_block(x, 128, _dropout_rate, 2, _weight_decay) # 128 + 8 * 8 = 192 x, _nb_filter = dense_block(x, 8, _nb_filter, 8, None, _weight_decay) x = BatchNormalization(axis=-1, epsilon=1.1e-5)(x) x = Activation('relu')(x) x = Permute((2, 1, 3), name='permute')(x) x = TimeDistributed(Flatten(), name='flatten')(x) y_pred = Dense(nclass, name='out', activation='softmax')(x) # basemodel = Model(inputs=input, outputs=y_pred) # basemodel.summary() return y_preddef dense_blstm(input): passinput = Input(shape=(32, 280, 1), name='the_input')dense_cnn(input, 5000) 关于train.py脚本内容示例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173# usr/bin/env python#-*- coding:utf-8 -*-import osimport jsonimport threadingimport numpy as npfrom PIL import Imageimport tensorflow as tffrom keras import lossesfrom keras import backend as Kfrom keras.utils import plot_modelfrom keras.preprocessing import imagefrom keras.preprocessing.sequence import pad_sequencesfrom keras.layers import Input, Dense, Flattenfrom keras.layers.core import Reshape, Masking, Lambda, Permutefrom keras.layers.recurrent import GRU, LSTMfrom keras.layers.wrappers import Bidirectional, TimeDistributedfrom keras.layers.normalization import BatchNormalizationfrom keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2Dfrom keras.optimizers import SGD, Adamfrom keras.models import Modelfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoardfrom imp import reloadimport modelimport ioimport sysreload(sys)#sys.setdefaultencoding('utf-8')img_h = 32img_w = 280batch_size = 128maxlabellength = 10def get_session(gpu_fraction=1.0): num_threads = os.environ.get('OMP_NUM_THREADS') gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction) if num_threads: return tf.Session(config=tf.ConfigProto( gpu_options=gpu_options, intra_op_parallelism_threads=num_threads)) else: return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))def readfile(filename): res = [] with open(filename, 'r') as f: lines = f.readlines() for i in lines: res.append(i.strip()) dic = &#123;&#125; for i in res: p = i.split(' ') dic[p[0]] = p[1:] return dicclass random_uniform_num(): """ 均匀随机，确保每轮每个只出现一次 """ def __init__(self, total): self.total = total self.range = [i for i in range(total)] np.random.shuffle(self.range) self.index = 0 def get(self, batchsize): r_n=[] if(self.index + batchsize &gt; self.total): r_n_1 = self.range[self.index:self.total] np.random.shuffle(self.range) self.index = (self.index + batchsize) - self.total r_n_2 = self.range[0:self.index] r_n.extend(r_n_1) r_n.extend(r_n_2) else: r_n = self.range[self.index : self.index + batchsize] self.index = self.index + batchsize return r_ndef gen(data_file, image_path, batchsize=128, maxlabellength=10, imagesize=(32, 280)): image_label = readfile(data_file) _imagefile = [i for i, j in image_label.items()] x = np.zeros((batchsize, imagesize[0], imagesize[1], 1), dtype=np.float) labels = np.ones([batchsize, maxlabellength]) * 10000 input_length = np.zeros([batchsize, 1]) label_length = np.zeros([batchsize, 1]) r_n = random_uniform_num(len(_imagefile)) _imagefile = np.array(_imagefile) while 1: shufimagefile = _imagefile[r_n.get(batchsize)] for i, j in enumerate(shufimagefile): img1 = Image.open(os.path.join(image_path, j)).convert('L') img = np.array(img1, 'f') / 255.0 - 0.5 x[i] = np.expand_dims(img, axis=2) # print('imag:shape', img.shape) str = image_label[j] label_length[i] = len(str) if(len(str) &lt;= 0): print("len &lt; 0", j) input_length[i] = imagesize[1] // 8 labels[i, :len(str)] = [int(k) - 1 for k in str] inputs = &#123;'the_input': x, 'the_labels': labels, 'input_length': input_length, 'label_length': label_length, &#125; outputs = &#123;'ctc': np.zeros([batchsize])&#125; yield (inputs, outputs)def ctc_lambda_func(args): y_pred, labels, input_length, label_length = args return K.ctc_batch_cost(labels, y_pred, input_length, label_length)def get_model(img_h, nclass): input = Input(shape=(img_h, None, 1), name='the_input') y_pred = model.dense_cnn(input, nclass) basemodel = Model(inputs=input, outputs=y_pred) basemodel.summary() labels = Input(name='the_labels', shape=[None], dtype='float32') input_length = Input(name='input_length', shape=[1], dtype='int64') label_length = Input(name='label_length', shape=[1], dtype='int64') loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length]) model = Model(inputs=[input, labels, input_length, label_length], outputs=loss_out) model.compile(loss=&#123;'ctc': lambda y_true, y_pred: y_pred&#125;, optimizer='adam', metrics=['accuracy']) return basemodel, modelif __name__ == '__main__': char_set = io.open('char_std_5990.txt', 'r', encoding='utf-8').readlines() char_set = ''.join([ch.strip('\n') for ch in char_set][1:]) nclass = len(char_set) K.set_session(get_session()) reload(model) basemodel, model = get_model(img_h, nclass) modelPath = './models/pretrain_model/keras.h5' if os.path.exists(modelPath): print("Loading model weights...") basemodel.load_weights(modelPath) print('done!') train_loader = gen('data_train.txt', './images', batchsize=batch_size, maxlabellength=maxlabellength, imagesize=(img_h, img_w)) test_loader = gen('data_test.txt', './images', batchsize=batch_size, maxlabellength=maxlabellength, imagesize=(img_h, img_w)) checkpoint = ModelCheckpoint(filepath='./models/model-weights-&#123;epoch:02d&#125;-&#123;val_loss:.2f&#125;.h5', monitor='val_loss', save_best_only=False, save_weights_only=True) lr_schedule = lambda epoch: 0.0005 * 0.4**epoch learning_rate = np.array([lr_schedule(i) for i in range(10)]) changelr = LearningRateScheduler(lambda epoch: float(learning_rate[epoch])) earlystop = EarlyStopping(monitor='val_loss', patience=2, verbose=1) tensorboard = TensorBoard(log_dir='./models/logs', write_graph=True) print('-----------Start training-----------') model.fit_generator(train_loader, steps_per_epoch = 3607567 // batch_size, epochs = 10, initial_epoch = 0, validation_data = test_loader, validation_steps = 36440 // batch_size, callbacks = [checkpoint, earlystop, changelr, tensorboard]) test.py脚本内容示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# usr/bin/env python#-*- coding:utf-8 -*-import osimport numpy as npfrom imp import reloadfrom PIL import Image, ImageOpsfrom keras.layers import Inputfrom keras.models import Model# import keras.backend as Kfrom . import modelreload(model)characters = keys.alphabet[:]characters = characters[1:]nclass = len(characters)input = Input(shape=(32, None, 1), name='the_input')y_pred= model.dense_cnn(input, nclass)basemodel = Model(inputs=input, outputs=y_pred)modelPath = os.path.join(os.getcwd(), '.models/weights-model.h5')if os.path.exists(modelPath): basemodel.load_weights(modelPath)def decode(pred): char_list = [] pred_text = pred.argmax(axis=2)[0] for i in range(len(pred_text)): if pred_text[i] != nclass - 1 and ((not (i &gt; 0 and pred_text[i] == pred_text[i - 1])) or (i &gt; 1 and pred_text[i] == pred_text[i - 2])): char_list.append(characters[pred_text[i]]) return u''.join(char_list)def predict(img): width, height = img.size[0], img.size[1] scale = height * 1.0 / 32 width = int(width / scale) img = img.resize([width, 32], Image.ANTIALIAS) img = np.array(img).astype(np.float32) / 255.0 - 0.5 X = img.reshape([1, 32, width, 1]) y_pred = basemodel.predict(X) y_pred = y_pred[:, :, :] # out = K.get_value(K.ctc_decode(y_pred, input_length=np.ones(y_pred.shape[0]) * y_pred.shape[1])[0][0])[:, :] # out = u''.join([characters[x] for x in out[0]]) out = decode(y_pred) return out]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Linux云服务器的本地可视化环境配置]]></title>
    <url>%2Farticle%2FLinuxCloudServerGUI.html</url>
    <content type="text"><![CDATA[因为做的项目更侧重于图像相关领域，在实际应用中有需要将目标识别或检测的结果进行显示的需求，后查阅相关资料，总结关于Linux云服务器的本地可视化环境配置如下： Linux云服务器设置一般情况下Linux镜像是不带可视化界面的，因此需要安装可视化（GUI）环境： 123apt-get install x-window-system-coreapt-get install gnome-coreapt-get install gdm Mac设置编辑mac 下 ~/.ssh/config，添加以下字段即可。 12345678910### default for all ##Host * ForwardAgent no ForwardX11 no ForwardX11Trusted yes User nixcraft Port 22 Protocol 2 ServerAliveInterval 60 ServerAliveCountMax 30 主要通过xQuartz这个软件（可视化环境，提供x11环境）， 调用方法是打开terminal输入ssh -X root@ip Windows设置主要通过xshell + x-manager组合 首先，安装Xmind和Xming-fonts这2个软件，后者是字体插件 其次，使用远程登录软件，打开“X11转发功能”（X11 Forward）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SecureCRT 个人配色方案]]></title>
    <url>%2Farticle%2FSecureCRTSetting.html</url>
    <content type="text"><![CDATA[在Mac上一直没有找到合适的SSH远程登录工具，后多次去知乎等相关平台查看类似问题，并经过实际尝试，暂时选定SecureCRT作为自己的临时SSH远程登录工具。 PS：也使用过其他的如自带的Terminal、FinalShell及Zoc等工具，总体而言FinalShell比较符合国人审美及操作，但是感觉不太安全，毕竟第三方个人开发，且云服务器莫名跑上了挖矿相关代码，不知道是否和该工具之间有关联。 为安全起见，还是使用了老牌的SecureCRT。 配置Linux终端颜色从Options中选择 Global options，然后点击Edit Defualt Setting 在Terminal下拉列表下选择Linux，勾选ANSI Color 配置ANSI ColorANSI Color 设置如下： 配置显示字体 配置光标样式设置闪烁 光标及相应颜色：]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>SecureCRT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python--字符串、列表、元组、字典之间的相互转换]]></title>
    <url>%2Farticle%2FPythonDataTypeConvert.html</url>
    <content type="text"><![CDATA[为方便以后开发查阅，特记录此相关代码。所要实现的功能： 字符串、列表、元组、字典之间的相互转换 字符串 字符串转列表 1234print list(eval("'happy', 'every', 'day'"))# 字符串转为列表，返回：['happy', 'every', 'day']print list(eval("1, 2, 3"))# 字符串转为列表，返回：[1, 2, 3] 字符串转元组 1234print tuple(eval("'happy', 'every', 'day'"))# 字符串转为列表，返回：('happy', 'every', 'day')print tuple(eval("1, 2,3"))# 字符串转为列表，返回：(1, 2, 3) 字符串转字典 12print eval("&#123;'happy':1, 'every':2, 'day':'nice'&#125;")#字符串转为字典，返回：&#123;'every': 2, 'day': 'nice', 'happy': 1&#125; 列表 列表转字符串 123list = ['happy', 'every', 'day', 1, 2, 3]print type(str(list)), str(list)# 列表转字符串，返回：&lt;type 'str'&gt; ['happy', 'every', 'day', 1, 2, 3] 列表转元组 123list = ['happy', 'every', 'day', 1, 2, 3]print tuple(list)# 列表转字符串，返回：('happy', 'every', 'day', 1, 2, 3) 列表转字典 将两个列表合成字典 1234list1 = ['happy', 'every', 'day']list2 = [1, 2, 3]print(dict(zip(list1,list2)))# 返回：&#123;'every': 2, 'day': 3, 'happy': 1&#125; 将嵌套列表转为字典 12345678list= [['key1','value1'],['key2','value2'],['key3','value3']]print(dict(list))# 返回：&#123;'key1': 'value1', 'key2': 'value2', 'key3': 'value3'&#125;list= [['key1','value1'],['key2','value2'],['key3','value3']]dict = &#123;&#125;for i in list: dict[i[0]] = i[1]# 返回：&#123;'key3': 'value3', 'key2': 'value2', 'key1': 'value1'&#125; 使用For循环 12345dict = &#123;&#125;list = ['happy', 'every', 'day', 1, 2, 3]for i in range(len(list)/2): dict[list[i]] = list[i + 3]# 返回：&#123;'every': 2, 'day': 3, 'happy': 1&#125; 元组 元组转字符串 123tuplex = ('happy', 'every', 'day', 1, 2, 3)print type(tuplex.__str__()), tuplex.__str__()# 元组转字符串，返回：&lt;type 'str'&gt; ('happy', 'every', 'day', 1, 2, 3) 元组转列表 123tuplex = ('happy', 'every', 'day', 1, 2, 3)print list(tuplex)# 元组转字符串，返回：['happy', 'every', 'day', 1, 2, 3] 元组转字典 123tuplex = ((1, 'happy'),(2, 'every'), (3, 'day'))print(dict((y, x) for x, y in tuplex))# 元组转字典，返回：&#123;'every': 2, 'day': 3, 'happy': 1&#125; 字典 字典转字符串 123dict = &#123;'every': 2, 'happy': 1, 'day': 'nice'&#125;print type(str(dict)), str(dict)# 字典转字符串，返回：&lt;type 'str'&gt; &#123;'every': 2, 'day': 'nice', 'happy': 1&#125; 字典转列表 12345dict = &#123;'every': 2, 'happy': 1, 'day': 'nice'&#125;print list(dict)# 字典转字符串，返回：['every', 'day', 'happy']print list(dict.values())# 字典转字符串，返回：[2, 'nice', 1] 字典转元组 12345dict = &#123;'every': 2, 'happy': 1, 'day': 'nice'&#125;print tuple(dict)# 字典转字符串，返回：('every', 'day', 'happy')print tuple(dict.values())# 字典转字符串，返回：(2, 'nice', 1)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[准确率(Precision)与召回率(Recall)与综合评价指标(F-Score)的解释]]></title>
    <url>%2Farticle%2FPrecisionRecallFscore.html</url>
    <content type="text"><![CDATA[机器学习领域三个最基本指标是召回率(Recall Rate)、准确率(Precision Rate)及综合评价指标(F-Score)。(召回率也称为查全率，准确率也称为查准率) 准确率(Precision) \begin{align} 准确率 =& \frac{预测到的真正例}{所有的例子(真实情况的正例+反例)} \tag{1}\\ \end{align} \begin{align} Precision =& \frac{TP}{TP+FP} \tag{1}\\ \end{align}即：在所有预测为正例中有多少是真实的正例。 召回率(Recall) \begin{align} 召回率 =& \frac{预测到的真正例}{所有的正例 (真实情况的正例)} \tag{1}\\ \end{align} \begin{align} Recall =& \frac{TP}{TP+FN} \tag{2}\\ \end{align}即：在所有真实的正例中有多少被正确预测到。 单一的准确率或者召回率在一些情况下不一定会有意义，譬如当结果都预测为正例，$FN=0$，$Recall=1$，无意义。 综合评价指标(F-Score)准确率或者召回率指标有时候会出现的矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Score（又称为F-Measure）。F-Score是Precision和Recall加权调和平均： \begin{align} F\verb|-|Score =& \frac{(\alpha^2+1)\times Precision\times Recall}{\alpha^2(Precision+Recall)} \tag{3}\\ \end{align}当参数$\alpha=1$时，就是最常见的$F_1$，也即 \begin{align} F_1 =& \frac{2\times Precision\times Recall}{Precision+Recall} \tag{4}\\ \end{align}P-R(Precision-Recall)曲线P-R曲线以召回率(Recall Rate)为横坐标，以准确率(Precision Rate)为纵坐标。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Precision</tag>
        <tag>Recall</tag>
        <tag>F-Score</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法框架图]]></title>
    <url>%2Farticle%2FDataStructureAlgorithm.html</url>
    <content type="text"><![CDATA[为方便后续查阅及学习，经参考相关书籍及资料，对数据结构与算法的框架图整理如下。 点此打开文件 （持续完善中…）]]></content>
      <categories>
        <category>Data Structure</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python--借助OpenCV在图片上显示文字及保存]]></title>
    <url>%2Farticle%2FPythonOpenCVPutText.html</url>
    <content type="text"><![CDATA[为方便以后开发查阅，特记录此相关代码。所要实现的功能： 在图片上显示文字及保存 所调用的函数12345678910111213cv2.putText(img, text, (40, 50), cv2.FONT_HERSHEY_PLAIN, 2.0, (0, 0, 255), 2)# putText 定义如下：void cv::putText( cv::Mat&amp; img, // 待绘制的图像 const string&amp; text, // 待绘制的文字 cv::Point origin, // 文本框的左下角 int fontFace, // 字体 (如cv::FONT_HERSHEY_PLAIN) double fontScale, // 尺寸因子，值越大文字越大 cv::Scalar color, // 线条的颜色（RGB） int thickness = 1, // 线条宽度 int lineType = 8, // 线型（4邻域或8邻域，默认8邻域） bool bottomLeftOrigin = false // true=’origin at lower left’ ); fontFace 字体类型 描述 CV_FONT_HERSHEY_SIMPLEX 正常尺寸sanserif字体 CV_FONT_HERSHEY_PLAIN 小尺寸sanserif字体 CV_FONT_HERSHEY_DUPLEX 正常尺寸sanserif, 比 CV_FONT_HERSHEY_SIMPLEX更复杂 CV_FONT_HERSHEY_COMPLEX 正常尺寸serif, 比 CV_FONT_HERSHEY_DUPLEX更复杂 CV_FONT_HERSHEY_TRIPLEX 正常尺寸serif, 比CV_FONT_HERSHEY_COMPLEX更复杂 CV_FONT_HERSHEY_COMPLEX_SMALL 小尺寸的 CV_FONT_HERSHEY_COMPLEX CV_FONT_HERSHEY_SCRIPT_SIMPLEX 手写风格 CV_FONT_HERSHEY_SCRIPT_COMPLEX 比CV_FONT_HERSHEY_SCRIPT_SIMPLEX更复杂的风格 程序示例：123456789101112131415161718import cv2img = cv2.imread('1.png')while True: W = 3.752 H = 4.382 L = 8.342 W = round(W, 2) # round函数将W保留两位小数 H = round(H, 2) L = round(L, 2) text = "W:"+str(W)+" " + "H:"+str(H) + " " + "L:"+str(L) cv2.putText(img, text, (40, 50), cv2.FONT_HERSHEY_PLAIN, 2.0, (0, 0, 255), 2) cv2.imshow("ori_image", img) key = cv2.waitKey(delay=1) if key == 27: cv2.destroyAllWindows() cv2.imwrite("new_img.png", img) break]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow Object detection API教程之三：测试自己的模型]]></title>
    <url>%2Farticle%2FTensorFlowObjectDetectionAPITutorial3.html</url>
    <content type="text"><![CDATA[TensorFlow Object detection API 教程系列： TensorFlow Object detection API 教程之一：Object detection API安装 TensorFlow Object detection API 教程之二：训练自己的模型 TensorFlow Object detection API 教程之三：测试自己的模型 在这一节，我们将要测试我们自己的模型，看一看训练的模型能否达到我们预期的效果。 将ckpt模型文件保存为pb模型文件首先我们需要导出计算图(Inference Graph)，在models/research/object_detection/目录中，官方提供的export_inference_graph.py脚本可以帮助我们轻松地去完成该操作。 找到一个想要导出pb文件的checkpoint，在models/research/object_detection/路径下执行命令 ： 12345python3 export_inference_graph.py \ --input_type image_tensor \ --pipeline_config_path training/ssd_mobilenet_v1_pets.config \ --trained_checkpoint_prefix training/model.ckpt-10856 \ --output_directory mac_n_cheese_inference_graph input_type：保持模型，不用修改。 pipeline_config_path：神经网络的参数设置文件路径，格式如上。 trained_checkpoint_prefix：训练后最大步长的ckpt文件的目录，格式如上。 output_directory：输入文件目录 如执行以上命令时报错为：no module named &#39;nets&#39;，进入models/research/路径下执行: 12# From tensorflow/models/research/export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim 读取pb模型文件 读取路径： 123456ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))# Path to frozen detection graph. This is the actual model that is used for the object detection.PATH_TO_CKPT = ROOT_PATH + '/include/hand_inference_graph/frozen_inference_graph.pb'# List of the strings that is used to add correct label for each box.PATH_TO_LABELS = ROOT_PATH + '/include/hand_inference_graph/hand_label_map.pbtxt'NUM_CLASSES = 1 加载模型： 12345678# Loading the modeldetection_graph = tf.Graph()with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') 加载标签： 1234# Loading label maplabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)category_index = label_map_util.create_category_index(categories) 读入图片： 123456789# For the sake of simplicity we will use only 2 images:# image1.jpg# image2.jpg# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.PATH_TO_TEST_IMAGES_DIR = 'test_images'TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image&#123;&#125;.jpg'.format(i)) for i in range(1, 3) ]# Size, in inches, of the output images.IMAGE_SIZE = (12, 8) 检测示例 完整代码点击查看官方代码 参考：[1]https://pythonprogramming.net/testing-custom-object-detector-tensorflow-object-detection-api-tutorial/?completed=/training-custom-objects-tensorflow-object-detection-api-tutorial/[2]https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python--判断字符串是否包含子字符串]]></title>
    <url>%2Farticle%2FPythonFindStrings.html</url>
    <content type="text"><![CDATA[为方便以后开发查阅，特记录此相关代码。所要实现的功能： 判断字符串是否包含子字符串 方法一：In12345string = 'helloworld'if 'world' in string: print('Exist')else: print('Not exist') 方法二：Find12345string = 'helloworld'if string.find(’world‘) &gt; -1: print('Exist')else: print('Not exist') 方法三：Index1234if string.index(’world‘) &gt; -1: #因为-1的意思代表没有找到字符，所以判断&gt;-1就代表能找到 print('Exist')else: print('Not exist') 但是，如果没找到，程序会抛出异常]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python--保存图片到文件夹]]></title>
    <url>%2Farticle%2FPythonSaveImage.html</url>
    <content type="text"><![CDATA[为方便以后开发查阅，特记录此相关代码。所要实现的功能： 将图片保存于指定文件夹中，可指定图片后缀 代码如下：12345678910111213def save_img(img, file_name,file_path='img'): # Save image to the file_path '\img' if not os.path.exists(file_path): print (file_path,'not exist') os.makedirs(file_path) # Image suffix # file_suffix = 'jpg' # Join image name # 项目中给定的图片名为“hiking_125.jpg_0”，需命名切片 filename = '&#123;&#125;&#123;&#125;&#123;&#125;'.format(file_path, os.sep, file_name[:-2]) # filename = file_name[:-2] # print(filename) plt.imsave(filename, img) 附plt常用保存图片的相应代码：plt.savefig(‘file_name’) 保存带有坐标的图像plt.imsave(‘file_name’, img) 保存原始图像]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python+Selenium实现网页自动化评论]]></title>
    <url>%2Farticle%2FPythonSelenium.html</url>
    <content type="text"><![CDATA[最开始是参与美的冰箱旗舰店的微淘活动，评论参与抽奖，如上图所示。因手动评论太过于枯燥，便想着写一个自动评论的脚本。 搭建Selenium环境先决条件，已安装好Python3 安装Selenium 1pip3 install selenium 安装Chromedriver Chromedriver下载地址：http://chromedriver.storage.googleapis.com/index.html 根据自己Chrome浏览器的版本下载对应的Chromedriver， 我的版本如下： 1sudo cp -r chromedriver /usr/local/bin/ 测试安装效果 test.py1234567from selenium import webdriverbrowser = webdriver.Chrome()browser.get("http://www.baidu.com")print(browser.page_source)browser.close() 运行： 1python test.py 在已打开的浏览器中进行Selenium控制 找到本地安装的浏览器启动路径，例如Chrome: 1234# For Mac/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome# For WinC:\Program Files (x86)\Google\Chrome\Application\chrome.exe 通过命令行启动ChromeDbug模式 1234# For Mac/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome -remote-debugging-port=9222# For WinC:\Program Files (x86)\Google\Chrome\Application&gt;chrome.exe --remote-debugging-port=9222 连接调试开关打开的Chrome 123options = webdriver.ChromeOptions()options.debugger_address = '127.0.0.1:9222'browser = webdriver.Chrome(options=options) 根据页面标签写脚本DeBugs处理弹窗确认评论成功后，页面会提示“回复成功”，并等待确认。 关于Selenium对弹窗的处理，参考了此篇文章：selenium对弹窗（alert）的处理 不过，在处理弹窗确认的这一步，出现报错Message: no such alert 123456789from selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutExceptionwait = WebDriverWait(driver, 10)wait.until(EC.alert_is_present())alert = driver.switch_to.alertalert.accept() 解决方案参考：https://stackoverflow.com/questions/33466853/switch-to-alert-text-not-working 评论后没有提交经过测试，发现是评论一下一步提交操作之间的时间过短，适当增加延时。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人主页升级之双线部署 + 全站Https]]></title>
    <url>%2Farticle%2FBlogUpdate.html</url>
    <content type="text"><![CDATA[关于Https单线部署在Github Pages时，用的是cloudflare提供的SSL证书。 双线部署（GitHub Pages + Coding Pages）时，Cloudflare就不能用了，但Coding Pages提供了通过Let’s Encrypt申请SSL证书进而可以开启全站Https的方法。 Coding.net配置篇注册Coding 升级银牌会员 创建项目 配置SSH 配置Pages 添加CNAME记录为pages.coding.me，注意将Github的线路类型设置为国外。 申请SSL证书 注意：如果是Github+Coding双线部署，申请SSL证书前需要先将解析到github.io的CNAME记录暂停！！！不然Let’s Encrypt主机在验证域名所有权时会定位到Github Pages的主机上导致SSL证书申请失败 强制开启Https _config文件配置篇12345678# Deployment # 非常重要的部署设置## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: # 可选择同时部署到 GitHub 和 coding 或者只部署到 Github github: git@github.com:你的GitHub名/你的GitHub名.github.io.git coding: git@git.coding.net:你的Coding名/你的Coding名.git branch: master]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS启动多launch文件的小技巧]]></title>
    <url>%2Farticle%2FROSLaunchTips.html</url>
    <content type="text"><![CDATA[在ROS工程中经常需要启动好几个launch文件，比较不方便，有下面两种方法可以更高效些： 重写一个大型的launch文件，将所有的节点的启动配置信息都包含进去。 通过bash写一个xxx.sh文件，将命令行一起写入一个脚本。 launch文件中则如下进行设置：pkg对应文件的包名。 type是CMakeList.txt中对应该文件add_executable(pcan_test src/pcan_test)中可执行文件的名称，在python中则是文件名，因为Python的可执行文件就是文件本身（解释性语言，同Matlab），所以若用C++编程不要误写为文件名全称。 name表示节点启动后的名称，该名称会覆盖ros::init中初始化的名称。 output后参数表示从屏幕输出打印信息，否则打印信息会存储到某个临时文件里。 1234567&lt;launch&gt; &lt;node pkg="uav_dl" name="position_control" type="position_control.py" output="screen" /&gt; &lt;node pkg="uav_dl" name="action_control" type="action_control.py" output="screen" /&gt; &lt;node pkg="uav_dl" name="goto_position_server" type="goto_position_server.py" output="screen" /&gt; &lt;node pkg="uav_dl" name="detect_object_server" type="detect_object_server.py" output="screen" /&gt; &lt;node pkg="uav_dl" name="tensorflow_detection" type="tensorflow_detection.py" output="screen" /&gt;&lt;/launch&gt; 注：只需要在src下建立launch文件夹，然后在其中创建launch文件即可，不需要做其他工作。 参数里name是ros::param::get()中第一个字符串去掉“~”后的名称，launch会在运行时进行查找匹配，type是变量类型，value是具体值。以下launch文件（包含私有变量和公有变量）。 12345678910111213141516171819202122232425262728293031323334353637&lt;launch&gt; &lt;arg name="fcu_url" default="serial:///dev/ttyACM0:921600" /&gt; &lt;arg name="gcs_url" default="udp://:14556@192.168.150.2:14550" /&gt; &lt;arg name="tgt_system" default="1" /&gt; &lt;arg name="tgt_component" default="50" /&gt; &lt;node name="mavros" pkg="mavros" type="mavros_node" output="screen"&gt; &lt;param name="fcu_url" value="$(arg fcu_url)" /&gt; &lt;param name="gcs_url" value="$(arg gcs_url)" /&gt; &lt;param name="target_system_id" value="$(arg tgt_system)" /&gt; &lt;param name="target_component_id" value="$(arg tgt_component)" /&gt; &lt;rosparam command="load" file="$(find mavros)/launch/px4_blacklist.yaml" /&gt; &lt;!-- enable heartbeat send and reduce timeout --&gt; &lt;param name="conn_heartbeat" value="5.0" /&gt; &lt;param name="conn_timeout" value="5.0" /&gt; &lt;!-- automatically start mavlink on USB --&gt; &lt;param name="startup_px4_usb_quirk" value="true" /&gt; &lt;/node&gt; &lt;node name="camera" pkg="usb_cam" type="usb_cam_node"&gt; &lt;param name="video_device" value="/dev/video0" /&gt; &lt;param name="image_width" value="800" /&gt; &lt;param name="image_height" value="600" /&gt; &lt;param name="pixel_format" value="mjpeg" /&gt; &lt;param name="framerate" value="30" /&gt; &lt;param name="camera_frame_id" value="webcam" /&gt; &lt;/node&gt; &lt;node name="viewer" pkg="image_view" type="image_view"&gt; &lt;remap from="image" to="/camera/image_raw" /&gt; &lt;/node&gt; &lt;/launch&gt; 在ubuntu下进行节点启动顺序控制的简单策略就是通过shell实现新建文件后命名为xxx.sh 123456789#!/bin/bashroslaunch bhand_controller bhand_controller.launch &amp;sleep 5echo "bhand controller starting success!" roslaunch beginner_tutorials bhand_force_control.launch &amp;sleep 0.1waitexit 0 代码解释：第一行表示用bash执行，sleep表示演示，echo用来输出一定内容，注意不要忘记句子后的”&amp;“符号。注：若ROS的关键词不能在终端识别，需先source下ROS环境。 节点启动顺序控制策略就是如果某个节点必须先执行，可以单独为其写一个launch文件，然后通过shell控制先行启动。 编写保存后，在终端要给xxx.sh执行权限，sudo chmod a+x xxx.sh，之后可通过./xxx.sh进行启动，xxx代表任意字符。有关Ubuntu shell的其他操作，可以自行查询相关资料。 Debugs1234# Bug:ERROR: cannot launch node of type [ros_tensorflow/imgTalker.py]: can't locate node [imgTalker.py] in package [ros_tensorflow]# Debug: 修改为可执行sudo chmod +x imgTalker.py]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Kreas实现经典CNN]]></title>
    <url>%2Farticle%2FKerasWithCNNNetworks.html</url>
    <content type="text"><![CDATA[Keras是搭建深度神经网络很好用的工具，集成度高，做深度学习的原型非常方便，可选择使用Theano或Tensorflow作为后端，非常适合学习和研究深度学习。 可以用 Keras 进行实验和测试，然后迁移到 Caffe 1 。 LeNet12345678910111213141516171819202122232425262728293031323334353637#coding=utf-8from keras.models import Sequentialfrom keras.layers import Dense,Flattenfrom keras.layers.convolutional import Conv2D,MaxPooling2Dfrom keras.utils.np_utils import to_categoricalimport cPickleimport gzipimport numpy as npseed = 7np.random.seed(seed) data = gzip.open(r'/media/wmy/document/BigData/kaggle/Digit Recognizer/mnist.pkl.gz')train_set,valid_set,test_set = cPickle.load(data)#train_x is [0,1]train_x = train_set[0].reshape((-1,28,28,1))train_y = to_categorical(train_set[1]) valid_x = valid_set[0].reshape((-1,28,28,1))valid_y = to_categorical(valid_set[1]) test_x = test_set[0].reshape((-1,28,28,1))test_y = to_categorical(test_set[1]) model = Sequential()model.add(Conv2D(32,(5,5),strides=(1,1),input_shape=(28,28,1),padding='valid',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(64,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Flatten())model.add(Dense(100,activation='relu'))model.add(Dense(10,activation='softmax'))model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])model.summary() model.fit(train_x,train_y,validation_data=(valid_x,valid_y),batch_size=20,epochs=20,verbose=2)#[0.031825309940411217, 0.98979999780654904]print model.evaluate(test_x,test_y,batch_size=20,verbose=2) AlexNet1234567891011121314151617181920212223242526#coding=utf-8from keras.models import Sequentialfrom keras.layers import Dense,Flatten,Dropoutfrom keras.layers.convolutional import Conv2D,MaxPooling2Dfrom keras.utils.np_utils import to_categoricalimport numpy as npseed = 7np.random.seed(seed) model = Sequential()model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),padding='valid',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))model.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))model.add(Flatten())model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(1000,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])model.summary() ZF Net1234567891011121314151617181920212223242526#coding=utf-8from keras.models import Sequentialfrom keras.layers import Dense,Flatten,Dropoutfrom keras.layers.convolutional import Conv2D,MaxPooling2Dfrom keras.utils.np_utils import to_categoricalimport numpy as npseed = 7np.random.seed(seed) model = Sequential()model.add(Conv2D(96,(7,7),strides=(2,2),input_shape=(224,224,3),padding='valid',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))model.add(Conv2D(256,(5,5),strides=(2,2),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))model.add(Flatten())model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(1000,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])model.summary() GoogLeNet123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#coding=utf-8from keras.models import Modelfrom keras.layers import Input,Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,concatenatefrom keras.layers.convolutional import Conv2D,MaxPooling2D,AveragePooling2Dimport numpy as npseed = 7np.random.seed(seed) def Conv2d_BN(x, nb_filter,kernel_size, padding='same',strides=(1,1),name=None): if name is not None: bn_name = name + '_bn' conv_name = name + '_conv' else: bn_name = None conv_name = None x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x) x = BatchNormalization(axis=3,name=bn_name)(x) return x def Inception(x,nb_filter): branch1x1 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branch3x3 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branch3x3 = Conv2d_BN(branch3x3,nb_filter,(3,3), padding='same',strides=(1,1),name=None) branch5x5 = Conv2d_BN(x,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branch5x5 = Conv2d_BN(branch5x5,nb_filter,(1,1), padding='same',strides=(1,1),name=None) branchpool = MaxPooling2D(pool_size=(3,3),strides=(1,1),padding='same')(x) branchpool = Conv2d_BN(branchpool,nb_filter,(1,1),padding='same',strides=(1,1),name=None) x = concatenate([branch1x1,branch3x3,branch5x5,branchpool],axis=3) return x inpt = Input(shape=(224,224,3))#padding = 'same'，填充为(步长-1）/2,还可以用ZeroPadding2D((3,3))x = Conv2d_BN(inpt,64,(7,7),strides=(2,2),padding='same')x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)x = Conv2d_BN(x,192,(3,3),strides=(1,1),padding='same')x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)x = Inception(x,64)#256x = Inception(x,120)#480x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)x = Inception(x,128)#512x = Inception(x,128)x = Inception(x,128)x = Inception(x,132)#528x = Inception(x,208)#832x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)x = Inception(x,208)x = Inception(x,256)#1024x = AveragePooling2D(pool_size=(7,7),strides=(7,7),padding='same')(x)x = Dropout(0.4)(x)x = Dense(1000,activation='relu')(x)x = Dense(1000,activation='softmax')(x)model = Model(inpt,x,name='inception')model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])model.summary() VGG-131234567891011121314151617181920212223242526272829303132#coding=utf-8from keras.models import Sequentialfrom keras.layers import Dense,Flatten,Dropoutfrom keras.layers.convolutional import Conv2D,MaxPooling2Dimport numpy as npseed = 7np.random.seed(seed) model = Sequential()model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Flatten())model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(1000,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])model.summary() VGG-161234567891011121314151617181920212223242526272829303132333435#coding=utf-8from keras.models import Sequentialfrom keras.layers import Dense,Flatten,Dropoutfrom keras.layers.convolutional import Conv2D,MaxPooling2Dimport numpy as npseed = 7np.random.seed(seed) model = Sequential()model.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(224,224,3),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))model.add(MaxPooling2D(pool_size=(2,2)))model.add(Flatten())model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(4096,activation='relu'))model.add(Dropout(0.5))model.add(Dense(1000,activation='softmax'))model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])model.summary() ResNet-34123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#coding=utf-8from keras.models import Modelfrom keras.layers import Input,Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,concatenate,Activation,ZeroPadding2Dfrom keras.layers import add,Flatten#from keras.layers.convolutional import Conv2D,MaxPooling2D,AveragePooling2Dimport numpy as npseed = 7np.random.seed(seed) def Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None): if name is not None: bn_name = name + '_bn' conv_name = name + '_conv' else: bn_name = None conv_name = None x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x) x = BatchNormalization(axis=3,name=bn_name)(x) return x def Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False): x = Conv2d_BN(inpt,nb_filter=nb_filter,kernel_size=kernel_size,strides=strides,padding='same') x = Conv2d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size,padding='same') if with_conv_shortcut: shortcut = Conv2d_BN(inpt,nb_filter=nb_filter,strides=strides,kernel_size=kernel_size) x = add([x,shortcut]) return x else: x = add([x,inpt]) return x inpt = Input(shape=(224,224,3))x = ZeroPadding2D((3,3))(inpt)x = Conv2d_BN(x,nb_filter=64,kernel_size=(7,7),strides=(2,2),padding='valid')x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x)#(56,56,64)x = Conv_Block(x,nb_filter=64,kernel_size=(3,3))x = Conv_Block(x,nb_filter=64,kernel_size=(3,3))x = Conv_Block(x,nb_filter=64,kernel_size=(3,3))#(28,28,128)x = Conv_Block(x,nb_filter=128,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)x = Conv_Block(x,nb_filter=128,kernel_size=(3,3))x = Conv_Block(x,nb_filter=128,kernel_size=(3,3))x = Conv_Block(x,nb_filter=128,kernel_size=(3,3))#(14,14,256)x = Conv_Block(x,nb_filter=256,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)x = Conv_Block(x,nb_filter=256,kernel_size=(3,3))x = Conv_Block(x,nb_filter=256,kernel_size=(3,3))x = Conv_Block(x,nb_filter=256,kernel_size=(3,3))x = Conv_Block(x,nb_filter=256,kernel_size=(3,3))x = Conv_Block(x,nb_filter=256,kernel_size=(3,3))#(7,7,512)x = Conv_Block(x,nb_filter=512,kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)x = Conv_Block(x,nb_filter=512,kernel_size=(3,3))x = Conv_Block(x,nb_filter=512,kernel_size=(3,3))x = AveragePooling2D(pool_size=(7,7))(x)x = Flatten()(x)x = Dense(1000,activation='softmax')(x) model = Model(inputs=inpt,outputs=x)model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])model.summary() ResNet-50123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#coding=utf-8from keras.models import Modelfrom keras.layers import Input,Dense,BatchNormalization,Conv2D,MaxPooling2D,AveragePooling2D,ZeroPadding2Dfrom keras.layers import add,Flatten#from keras.layers.convolutional import Conv2D,MaxPooling2D,AveragePooling2Dfrom keras.optimizers import SGDimport numpy as npseed = 7np.random.seed(seed) def Conv2d_BN(x, nb_filter,kernel_size, strides=(1,1), padding='same',name=None): if name is not None: bn_name = name + '_bn' conv_name = name + '_conv' else: bn_name = None conv_name = None x = Conv2D(nb_filter,kernel_size,padding=padding,strides=strides,activation='relu',name=conv_name)(x) x = BatchNormalization(axis=3,name=bn_name)(x) return x def Conv_Block(inpt,nb_filter,kernel_size,strides=(1,1), with_conv_shortcut=False): x = Conv2d_BN(inpt,nb_filter=nb_filter[0],kernel_size=(1,1),strides=strides,padding='same') x = Conv2d_BN(x, nb_filter=nb_filter[1], kernel_size=(3,3), padding='same') x = Conv2d_BN(x, nb_filter=nb_filter[2], kernel_size=(1,1), padding='same') if with_conv_shortcut: shortcut = Conv2d_BN(inpt,nb_filter=nb_filter[2],strides=strides,kernel_size=kernel_size) x = add([x,shortcut]) return x else: x = add([x,inpt]) return x inpt = Input(shape=(224,224,3))x = ZeroPadding2D((3,3))(inpt)x = Conv2d_BN(x,nb_filter=64,kernel_size=(7,7),strides=(2,2),padding='valid')x = MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='same')(x) x = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3),strides=(1,1),with_conv_shortcut=True)x = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[64,64,256],kernel_size=(3,3)) x = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)x = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[128,128,512],kernel_size=(3,3)) x = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)x = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[256,256,1024],kernel_size=(3,3)) x = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3),strides=(2,2),with_conv_shortcut=True)x = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))x = Conv_Block(x,nb_filter=[512,512,2048],kernel_size=(3,3))x = AveragePooling2D(pool_size=(7,7))(x)x = Flatten()(x)x = Dense(1000,activation='softmax')(x) model = Model(inputs=inpt,outputs=x)sgd = SGD(decay=0.0001,momentum=0.9)model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])model.summary() 参考： 1. 对比深度学习十大框架：TensorFlow 并非最好？ &#8617; 2. keras实现常用深度学习模型LeNet，AlexNet，ZFNet，VGGNet，GoogleNet，Resnet &#8617; 类似程序可参考： AlexNet的理解及其Keras实现 VGGNet的理解及其Keras实现 GoogLeNet的理解及其Keras实现 ResNet的理解及其Keras实现]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典CNN网络结构汇总]]></title>
    <url>%2Farticle%2FCNNArchitecture.html</url>
    <content type="text"><![CDATA[鉴于平时接触的知识、概念等太多，一些知识点时间长了也容易忘记，因此这里对之前所了解的各种卷积神经网络做一个归纳。 大名鼎鼎的LeNet5 诞生于1994年，是最早的深层卷积神经网络之一，并且推动了深度学习的发展。从1988年开始，在多次成功的迭代后，这项由Yann LeCun完成的开拓性成果被命名为LeNet5。LeCun认为，可训练参数的卷积层是一种用少量参数在图像的多个位置上提取相似特征的有效方式，这和直接把每个像素作为多层神经网络的输入不同。像素不应该被使用在输入层，因为图像具有很强的空间相关性，而使用图像中独立的像素直接作为输入则利用不到这些相关性。 LeNet - 1998 AlexNet - 2012 ZF-Net - 2013 GoogLeNet - 2014 VGG - 2014 ResNet - 2015 残差结构：]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Comprehensive Survey on Deep Learning Approaches]]></title>
    <url>%2Farticle%2FSurveyOnDL.html</url>
    <content type="text"><![CDATA[前段时间，一位天大的师兄分享了这篇文章，干货满满！ 感觉十分不错，自己也挂出来分享下。 The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches M. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, M. Hasan, B. C. Van Esesn, A. A. S. Awwal, and V. K. Asari, “The history began from alexnet: A comprehensive survey on deep learning approaches,” arXiv preprint arXiv:1803.01164, 2018. 论文地址：https://arxiv.org/abs/1803.01164 A. 深度学习方法的类型 监督学习 半监督学习 非监督学习 深度强化学习（DRL） B. 特征学习C. 应用深度学习的时机和领域D. 深度学习的前沿发展 ImageNet 数据集上的图像分类 自动语音识别 E. 为什么要使用深度学习 通用学习方法 鲁棒性 泛化 可扩展性 F. 深度学习面临的挑战后面的结构如下： 第二节讨论 DNN 的详细调查；第三节讨论 CNN；第四节介绍了不同的先进技术，以有效地训练深度学习模型； 第五节讨论 RNN； AE 和 RBM 在第六节中讨论； GAN 及其应用在第七节讨论；强化学习在第八节中介绍；第九节解释迁移学习； 第十节介绍了深度学习的高效应用方法和硬件； 第十一节讨论了深度学习框架和标准开发工具包（SDK）； 第十二节给出了不同应用领域的基准测试结果；第十三节为结论。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习常用算法总结]]></title>
    <url>%2Farticle%2FMachineLearningNote.html</url>
    <content type="text"><![CDATA[因为最近一段时间准备找工作的事情，所以对之前机器学习的内容做了一个梳理。 结合麦子学院里面大牛的教程，这里对机器学习中常用的算法进行归纳和总结。 机器学习总的可以分为以下四类： 监督学习（Supervised Learning） 非监督学习（Unsupervised Learning） 半监督学习（Semi-Supervised Learning） 强化学习（Reinforcement Learning） 监督学习（Supervised Learning）1.监督学习：分类（Classification） 决策树（Decision Tree） 【代码】 KNN（K Nearest Neighbor） 【代码】 支持向量机（Support Vector Machine） 线性可区分 【代码】 线性不可分 【代码】 神经网络（Neural Network） 【代码一】 【代码二】 注：神经网络既可以用来做分类（classification）问题，也可以用来解决回归（regression）问题。 2.监督学习：回归（Regression） 线性回归 简单线性回归 【代码】 多元线性回归 【代码】 非线性回归 【代码】 回归中的相关度和R平方值 【代码】 非监督学习（Unsupervised Learning）1.非监督学习：聚类（Clustering） 用K-mean算法聚类（Clustering） 【代码】 用Hierarchical clustering算法聚类 【代码】 2.非监督学习：降维（Dimensionality Reduction） PCA 流形学习 SIGAI算法地图 SK-Learn Exampleshttp://scikit-learn.org/stable/ SK-learn: Choosing the right estimator]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[成都参会之记]]></title>
    <url>%2Farticle%2FICAIBDJourney.html</url>
    <content type="text"><![CDATA[于5月26-28日去成都参加了「2018 International Conference on Artificial Intelligence and Big Data」，私认为收获颇多，特记录所想所感，方将来勉励自己，回过头来看看，结果强行写成了日记，手动捂脸。 5月26日 厦门，轻微雾霾 于26日早和「吴二狗」从学校一同出发，去机场的路上和滴滴司机吹逼厦门的房价，厦门的房价仿佛如伦敦的天气般，成了陌生人间寒暄的必聊话题。忙碌中，赶上了早班飞机。 成都，小雨初过 飞机下降的过程中，小雨滋润后的成都清晰的浮现在眼前，整座城市仿佛被洗涤了一般，秀色可餐。随人潮涌入了地铁中，既陌生也熟悉的方言，才把自己拉过来魂，意识到来了成都。和「吴二狗」在某地铁中转站挥手分道扬镳。 因早起，中午特在酒店补了个觉，虽然没睡着。2点左右便行至会场注册，领取了材料，被告知了会议相关细节。后，正好在地图上看到了不远处即有一所高庙「文殊院」，特步行于此，踱步直至傍晚，古色古香，庄严肃穆，真是个好去处。 做地铁参观了下人民公园，被成都城区绿意所感染，整个人都一直是处于很舒服的状态。饭点特品尝了下著名的担担面，不禁美味，食之两碗。 后又去附近的「宽窄巷子」逛了逛，独自一人，自己也整个洒脱了起来，穿街过巷，访店过坊，巴不得每个角落都看个遍，然而和西塘、鼓浪屿等并无两样，影响最深刻的就是有个印章篆刻的师傅，要价很贵。 晚10时左右，返回住处。 5月27日 成都，晴 9时抵达会场，大牛们的陆续汇报，自己震撼于科技的进步，传感器网络在大数据及人工智能中的应用让我很是感兴趣。私以为，未来是万物互联的世界，各类「智能」传感器必然担当起重要角色。 汇报前的等待总是充满了激动和紧张，当然，自我情绪的调节还是稍微有效的。下午4时后自己的汇报时间点，猛着一股劲便作了报告，概述了拙见，交换了意见，整个人也放松了起来。对智能的未来，充满期待。另外，深感稿子还是要牢牢熟背！ 自己的会议日程结束后，便早早的溜走了。大成都的美，还是要好好的花时间去品味。后便去「天府广场」溜达了下，整个广场被科技馆、图书馆、博物馆等包围，可以看出成都人民对文化、教育及科技的重视程度之深。 坐了约莫一个小时，为N个游人拍了照，便打算参观下成都的时尚之美，步行至春熙路。为方便深刻体验成都的时尚，也只好独身深入步行街去感受，成都妹子可以称之为秀色宜人。特去IFS拜访了下「熊猫爬墙」，感叹艺术的伟大！ 晚9时于太古里返回住处，特买了点瓜子，供晚上看电影消遣用。 5月28日 成都，晴 一觉睡到自然醒，本打算去省博物馆参观下，无奈于当天周一，博物馆闭馆而不得打消了此计划。后便在周边看了看，寄存了行李。午餐尝了下有名的「夫妻肺片」，四川的辣还是不同于两湘地区，实属麻辣。整个来说，物价还是比厦门低。 午餐后和「吴二狗」于熊猫大道地铁站汇合，意打算参观下成都大熊猫繁育研究基地。当时见「二狗」满面红光，甚不知发生了什么，涉及隐私，便不方便打问。 二人乘车至「熊猫基地」，半价票29元甚是优惠。园内途径「天鹅湖」，黑天鹅是相当的蠢萌，人走哪，其游哪，不知是因为游客经常投食，还是受熊猫影响，呆至如此。 听「吴二狗」说看熊猫全凭运气，我开始不信。后步至各熊猫园，譬如「成年大熊猫别墅」、「大熊幼稚园」等，告知因天气炎热，熊猫被收回房内，深感「吴二狗」所言极是。 虽有可惜，但也面睹了国宝之真容，仍感欣慰。在园区一直溜达到傍晚，最终依依不舍的离开。 后两人商议去体验下成都火锅，成都火锅品牌很多，在「吴二狗」强烈提议下去了「小龙坎火锅店」，口味确实正宗，两人吃了个大饱，也正儿八经的体验了下成都人的日常生活。 吃完火锅，步行至IFS，上顶层端详了下「熊猫爬墙」的正脸，对其忧郁的表情很是不解。因时间太晚，另第二天赶早班飞机，随返回住处。 5月29日 成都，晴 一早，酒店派车送至航站楼，感火锅后劲仍在，肠胃初次接触成都本地火锅，还是需要适应的过程。「吴二狗」声称川航的妹子国内顶尖，果不其然，另飞机餐也很多样。 厦门，爆晒 到厦门时，已是中午，爆晒，相比之下，成都更为宜居。 此次行程终结。 附图]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>成都</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow Object detection API教程之二：训练自己的模型]]></title>
    <url>%2Farticle%2FTensorFlowObjectDetectionAPITutorial2.html</url>
    <content type="text"><![CDATA[TensorFlow Object detection API 教程系列： TensorFlow Object detection API 教程之一：Object detection API安装 TensorFlow Object detection API 教程之二：训练自己的模型 TensorFlow Object detection API 教程之三：测试自己的模型 这一节，我们将对TensorFlow中的训练过程做一个介绍，训练模型的步骤可大体划分为以下几步： 收集数据去收集至少100张包含你需要检测目标的图像，理想的情况是数据越多越好，不过对下一步的打标签带来沉重的任务。 将数据安装9：1的比例划分为训练集和测试集，并根据要训练的数据集，创建.pbtxt文件。 打标签使用LabelImg对数据集打标签，可以生成Pascal VOC格式的xml文件。 关于LabelImg的相关教程请参考下方两个链接： 1.LabelImg介绍与安装教程 2.LabelImg使用教程 将数据转换为TF Records格式 借助Raccon_dataset中的xml_to_csv.py将数据由XML格式转为CSV格式。 12345678910111213# 其中def main(): image_path = os.path.join(os.getcwd(), 'annotations') xml_df = xml_to_csv(image_path) xml_df.to_csv('raccoon_labels.csv', index=None) print('Successfully converted xml to csv.')# 修改为：def main(): for directory in ['train','test']: image_path = os.path.join(os.getcwd(), 'images/&#123;&#125;'.format(directory)) xml_df = xml_to_csv(image_path) xml_df.to_csv('data/&#123;&#125;_labels.csv'.format(directory), index=None) print('Successfully converted xml to csv.') 此时目录譬如下方结构： 12345678910111213.└── Object-Detection/ ├── data/ │ └── test_labels.csv | └── train_labels.csv └── images/ | └── test/ | | └── testingimages.jpg | └── train/ | └── trainingimages.jpg | └── yourimages.jpg └── training/ └── xml_to_csv.py 借助Raccon_dataset中的generate_tfrecord.py将数据由CSV格式转为TF Records格式。 注意：generate_tfrecord.py的Todo部分需要与你的.pbtxt文件内的内容一致 1234567# TO-DO replace this with label mapdef class_text_to_int(row_label): if row_label == 'macncheese': return 1 else: None# 此处只有一类 执行： 123# 譬如python generate_tfrecord.py --csv_input=data/train_labels.csv --output_path=data/train.recordpython3 generate_tfrecord.py --csv_input=data/test_labels.csv --output_path=data/test.record 另外在models/research/object_detection/dataset_tools目录中，官方提供了一些数据转换工具。 配置模型参数Tensorflow Object Detection API中模型参数、训练参数、评估参数都是在一个config文件中配置。 在配置模型参数的时候，通常有两种方式，一是使用预训练的模型，通过迁移学习(Transfer learning )来学习一个新目标(Object)，这种训练方式可以大幅缩减训练的时间，使用少量的数据就可以达到较好的效果。另外一种是从头开始训练，end-to-end。 在models/research/object_detection/samples/configs/的路径下，官方提供了一些object_detection配置文件的结构。在.config中搜索所有的PATH_TO_BE_CONFIGURED，修改为自己数据所存放的路径。另外还有heckpoint的路径、名称，num_classes的数目，label_map_path的路径等，按需修改。 训练在tensorflow/models/research/路径下，执行： 1234567# From tensorflow/models/research/python object_detection/model_main.py --pipeline_config_path=$&#123;YOUR_DIRECTORY&#125;\object_detection\samples\configs\XXXXXXX.config --model_dir=$&#123;YOUR_DIRECTORY&#125;\object_detection\data --num_train_steps=50000 --num_eval_steps=2000 --alsologtostderr 其中：--pipeline_config_path,--model_dir,--num_train_steps等按需修改。 使用Tensorboard对过程进行监视 1tensorboard --logdir=$&#123;YOUR_DIRECTORY&#125;/model_dir 在浏览器中输入127.0.0.1:6006观察训练的过程。 参考文献： [1]https://pythonprogramming.net/custom-objects-tracking-tensorflow-object-detection-api-tutorial/[2]https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/[3]https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow Object detection API教程之一：TensorFlow及其Object detection API安装]]></title>
    <url>%2Farticle%2FTensorFlowObjectDetectionAPITutorial1.html</url>
    <content type="text"><![CDATA[之前一直在Caffe平台做object detection，后面查阅相关资料时，经常看到TensorFlow下的object detection API，怀着好奇心了解了下，发现效果很不错，总体而言比Caffe下要简单些。 Google niubility ! TensorFlow Object detection API 教程系列： TensorFlow Object detection API 教程之一：Object detection API安装 TensorFlow Object detection API 教程之二：训练自己的模型 TensorFlow Object detection API 教程之三：测试自己的模型 安装TensorFlow12345678910111213141516# 安装TensorFlowpip install tensorflow # For CPUpip install tensorflow-gpu # For GPU# 若安装不同版本的TensorFlow# pip install tensorflow==1.4.0# pip install tensorflow-gpu==1.4.0# 卸载TensorFlow# pip uninstall tensorflow# 验证TensorFlow是否安装成功# 输出Hello, TensorFlow! 则代表安装成功。&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; print(sess.run(hello)) 安装TensorFlow Object Detection API1234567891011121314151617181920212223242526272829303132333435363738# 安装依赖项sudo apt-get install protobuf-compiler python-pil python-lxmlsudo pip install jupytersudo pip install matplotlib# 下载TensorFlow Object Detection APImkdir ~/tensorflowcd ~/tensorflowgit clone https://github.com/tensorflow/models.git# 编译protobuf# 进入object_detection所在目录# 譬如：cd ~/tensorflow/models/research/# 编译成功时,界面无任何显示protoc object_detection/protos/*.proto --python_out=.# 添加环境变量# 1.进入object_detection所在目录# 譬如：cd ~/tensorflow/models/research/export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim# 这条命令在新打开的终端中需要重新执行一次才会在新终端中生效# 2.添加到~/.bashrcgedit ~/.bashrc# 将下面命令添加到最后，注意'pwd'更换为object_detection的路径# export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim# 进入object_detection所在目录，在终端输入pwd查询# 例如我的export PYTHONPATH=$PYTHONPATH:/root/tensorflow/models/research:/root/tensorflow/models/research/slim# Testpython object_detection/builders/model_builder_test.py# 成功则显示如下：.......----------------------------------------------------------------------Ran 7 tests in 0.026sOK Debugs123456# Bugillegal instruction (sore dumped)# Debugpip uninstall tensorflowpip install tensorflow==1.5 1234567891011121314# Bugobject_detection/protos/ssd.proto:11:3: Expected "required", "optional", or "repeated".# Debug# 使用高版本protoc#download protoc 3.3mkdir protoc_3.3cd protoc_3.3wget https://github.com/google/protobuf/releases/download/v3.3.0/protoc-3.3.0-linux-x86_64.zipchmod 775 protoc-3.3.0-linux-x86_64.zipunzip protoc-3.3.0-linux-x86_64.zip#compile proto filecd /usr/local/lib/python2.7/dist-packages/tensorflow/models/~/protoc_3.3/bin/protoc object_detection/protos/*.proto --python_out=.]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于机器学习和深度学习的一些思考]]></title>
    <url>%2Farticle%2FNotes01.html</url>
    <content type="text"><![CDATA[现在各种学习算法火的一塌糊涂，很多人进入这个圈子搞研究，但有多少人真正明白为什么要用这些学习算法，以及如何将算法落地应用到工程中？ 当前机器学习及深度学习训练模型都需要大量的数据或特征，公司或研究机构都需要花费大量人力物力去获取数据、标注数据，还需要高性能的计算机或服务器通过数据来训练模型，总所走着的是花费时间长，所需成本高，模型获取复杂。​由上述情况，个人认为将来机器学习及深度学习或可能向这么几个方向发展。一，采用大数据或GAN得到大量数据，采用某种方法（待查阅相关资料）对数据自行标注；二，学习算法或向数据需求少，计算复杂度低，可移植性高，向移动端或嵌入式设备靠拢。（由于个人层次比较低，眼界相对较窄，望阅读的同学不要因此随笔所局限住思维，还请不吝赐教）​从整个工程系统来看，机器学习、深度学习等只是一种工具，甚至可以说是一种可替代的工具，这个工具既可以被（改良的）传统算法所取代（毕竟发展时间长，应用成熟），也可以被未来更为优秀、高效的算法所取代。​个人认为做项目，更应该从宏观的角度来思考整个系统，而不应被某一算法或某一框架所局限。而且，要乐于去拥抱新技术，去探索新方向，去尝试别人未曾做过的事物。​至于学习算法将来会不会其他更优秀、更高效、更实际的算法取代，还有待观望。不过人工智能仍然将是未来社会的发展动力，仍是各国政府及企业所关注的重要方向，因此也勉励自己能够在这个道路上继续走下去，转化为自己的核心竞争力。]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chrome常用快捷键]]></title>
    <url>%2Farticle%2FChromeHotKey.html</url>
    <content type="text"><![CDATA[上一篇越扯越远，本想做一个快捷键归纳的日志，方便以后忘记的时候查阅。 没想到，硬生生写成了一篇吐槽文，新开一篇特贴上干货。 Mac 快捷键 功能 ⌘ + N 打开新窗口 ⌘+ T 打开新标签页 ⌘+ Shift + N 无痕模式下打开新窗口 ⌘+ 鼠标左击 链接新窗口打开 ⌘+ R 页面刷新 ⌘ + W 关闭网页 ⌘+ Shift + T 重新打开最后关闭的标签页 ⌘ + D 将网页保存为书签 ⌘ + Shift + B 显示或隐藏书签栏 ⌘ + Option + B 打开书签管理器 Win &amp; Linux 快捷键 功能 Ctrl + N 打开新窗口 Ctrl + T 打开新标签页 Ctrl + Shift + N 无痕模式下打开新窗口 Ctrl + 鼠标左击 链接新窗口打开 Ctrl + R 页面刷新 Ctrl + W 关闭网页 Ctrl + Shift + T 重新打开最后关闭的标签页 Ctrl + D 将网页保存为书签 Ctrl + Shift + B 显示或隐藏书签栏 Ctrl + Shift + O 打开书签管理器 更多快捷键参看官方介绍：Chrome 键盘快捷键]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[那些年我用过的搜索引擎]]></title>
    <url>%2Farticle%2FSearchEngines.html</url>
    <content type="text"><![CDATA[早先在Mac上一直用Safari和Chrome，后来转Win开始用UC和Firefox。 作为一个追求艺术和高效的汉子，使用浏览器则一直在路上。 最近使用UC，发现其貌似在Win10下启用了Edge内核，而之前其一直是使用Chrome内核，这种更换严重影响了我的网页体验，致使GitHub无故打不开。 迫不得己，又转回了Chrome，特此对Chrome的部分快捷键做个归纳。 PS: UC遇到的问题，熟悉的味道，熟悉的IE界面。 后来去UC官方论坛看了看，果真… 写着写着就跑远了，赶紧回头把快捷键贴上。 快捷键—跳转链接]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Search Engines</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS相关画图工具]]></title>
    <url>%2Farticle%2FROSPlotTools.html</url>
    <content type="text"><![CDATA[通过查阅很多资料，这里对ROS中的画图工具（或者称为：日志分析工具）作一个总结。 rxplot官方介绍：http://wiki.ros.org/rxplot Since Groovy, rxplot becomes DEPRECATED and is succeeded by rqt_plot. 说明的是rxplot被rqt_plot替代了。 rqt_plot官方介绍：http://wiki.ros.org/rqt_plot rqt_plot provides a GUI plugin visualizing numeric values in a 2D plot using different plotting backends. rqt_plot提供了一个GUI插件，使用不同的绘图后端在2D图中显示数值。 PlotJuggler介绍：https://github.com/facontidavide/PlotJuggler PlotJuggler is meant to be a better alternative to rqtplot and rqtbag, providing a more user friendly interface. PlotJuggler旨在成为rqtplot和rqtbag的更好替代品，提供更友好的用户界面。 Matlab另外一个方法就是，借助Matlab中进行画图，这种方式更适合搞研究，做仿真。 官方相关：Work with rosbag Logfiles, Control LBR Manipulator Motion Through Joint Torque Commands MATLAB® can read these rosbag files and help with filtering and extracting message data. 关于更多 ROS + Matlab 相关教程，参看：Robotics System Toolbox Examples]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ROS运行Caffe-SSD demo【通用】]]></title>
    <url>%2Farticle%2FCaffeSSDDemo.html</url>
    <content type="text"><![CDATA[最近仔细研究了下SSD相关的几个demo文件，在这里做一个简要的记录。 大神weiliu89提供的Caffe/examples/ssd下提供了很多demo程序，常用的有以下这么几个。 关于配置Caffe-SSD请参看官方文档或我之前的笔记，不过我之前的教程是基于ROS的。 使用ssd_detect.cpp生成可执行程序：注：C++程序不自动显示label 对ssd_detect.cpp进行编译，生成可执行二进制文件，使用ssd_detect.bin接口程序（路径：caffe/build/examples/ssd/），检测image或video； 1234567build/examples/ssd/ssd_detect.bin models/VGGNet/VOC0712/SSD_300x300/deploy.prototxt \models/VGGNet/VOC0712/SSD_300x300/VGG_VOC0712_SSD_300x300_iter_120000.caffemodel \examples/videos/test.txt --file_type video \--out_file output.txt \--confidence_threshold 0.4 #检测视频，阈值为0.4并保存结果# 一定要注意上述路径，最易出错# 注：若出现错误，请将换行符删除 使用examples/ssd/plot_detections.py，对ssd_detect.cpp的输出文件，绘制检测结果。 123456python examples/ssd/plot_detections.py examples/images/result.txt \/home/catkin_ws/src/roc_caffe/caffe \--labelmap-file data/VOC0712/labelmap_voc.prototxt \--save-dir examples/# 一定要注意上述路径，最易出错# 注：若出现错误，请将换行符删除 作为类来调用将ssd_detect.cpp改写为一个头文件（譬如：Detector.h），然后进行调用。 使用ssd_detect.py12345678# 先进入caffe主路径，譬如我的路径cd ~/catkin_ws/src/ros_caffe/caffe/# 检测单张图片python examples/ssd/score_ssd_pascal.py# 检测视频python examples/ssd/ssd_pascal_video.py # 通过摄像头，检测实时视频python examples/ssd/ssd_pascal_webcam.py Debugs1234567# BugImportError:No module named caffe# Debug# 在py代码中添加import sys,oscaffe_root = '/home/cong/catkin_ws/src/ros_caffe/caffe'sys.path.insert(0, caffe_root + 'python')]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>SSD</tag>
        <tag>Demo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人机相关坐标系的解算]]></title>
    <url>%2Farticle%2FUAVCoordinateSystemSolving.html</url>
    <content type="text"><![CDATA[通过小孔成像模型，来结算几个坐标系之间的关系。 分别为机身及摄像机坐标系，图像坐标系和世界坐标系。 摄像机坐标系与机身坐标系摄像机坐标系（光心坐标系） $C=[x_C, y_C, z_C]^T$以摄像机光心$O_C$为原点，其$z_C$轴与光轴重合，水平方向为$x_C$轴，$y_C$轴垂直于$X_C, O_c, Z_C$平面，焦距$f$为光心到像平面的距离。 机身坐标系$B=[x_B, y_B, z_B]^T$，即无人机最中心的坐标。 摄像机坐标系和机身坐标系之间存在一个安装误差，用$T=\begin{bmatrix} \alpha &amp; 0 &amp; 0 \\\ 0 &amp; \beta &amp; 0 \\\ 0 &amp; 0 &amp; \gamma \end{bmatrix}\begin{bmatrix} x_e &amp; 0 &amp; 0 \\\ 0 &amp; y_e &amp; 0 \\\ 0 &amp; 0 &amp; z_e \end{bmatrix}$表示，其中$[\alpha, \beta, \gamma]^T$为安装的误差角，$[x_e, y_e, z_e]^T$为摄像机到机身坐标原点的空间距离。 摄像机坐标系与机身坐标系之间的关系为： C=TB \tag{1}摄像机坐标系与世界坐标系世界坐标系：是客观三维世界的绝对坐标系，也称客观坐标系。对于空间中的一点$P_E=(x_E, y_E, z_E)$，其对应的摄像机坐标系的坐标$(x_C, y_C, z_C)$与摄像机的姿态角和GPS（所在的位置）有关。在无人机飞行的过程中，可以通过IMU实时获取其姿态角信息$(\phi, \theta, \varphi)$，以便对摄像头姿态校正。 摄像机坐标系与世界坐标系之间的关系为： \begin{bmatrix} x_C \\ y_C \\ z_C \end{bmatrix} =T \begin{bmatrix} R(\phi, \theta, \varphi) & \begin{bmatrix} x & 0 & 0 \\ 0 & y & 0 \\ 0 & 0 & z \end{bmatrix} \end{bmatrix} \begin{bmatrix} x_E \\ y_E \\ z_E \end{bmatrix} =T \begin{bmatrix} R & M \end{bmatrix} \begin{bmatrix} x_E \\ y_E \\ z_E \end{bmatrix} \tag{2}其中，$R(\phi, \theta, \varphi)$为机身坐标与地面坐标的转换矩阵，$M=\begin{bmatrix} x &amp; 0 &amp; 0 \\ 0 &amp; y &amp; 0 \\ 0 &amp; 0 &amp; z \end{bmatrix}$，其中$(x, y, z)$为无人机在空间中（世界坐标系）的位置信息，$z$即无人机飞行高度。 像素坐标系与物理坐标系图像像素坐标系$[u, v]$，该坐标系没有物理单位。像素坐标系中的坐标点$(u_1, v_1)$代表图像在存储数组中的行和列位置，该位置存储的是图像的颜色或灰度信息。 图像物理坐标系$I=[x_I, y_I]$，其中$x_I$，$y_I$轴分别和$x_C$，$y_C$轴平行。物理坐标系中的坐标点单位为毫米，如果一个像素点对应物理坐标系中$x_I$轴方向物理尺寸${\rm d}x$和$y_I$轴方向物理尺寸${\rm d}y$，${\rm d}x$和${\rm d}y$与摄像机焦距$f$有关。 物理坐标系上的一点$I_1=(x_1, y_1)$与其像素坐标系中的点$(u_1, v_1)$对应关系为： \begin{bmatrix} u_1 \\ v_1 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1}{dx} & 0 & u_0 \\ 0 & \frac{1}{dy} & v_0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ y_1 \\ 1 \end{bmatrix} =K \begin{bmatrix} x_1 \\ y_1 \\ 1 \end{bmatrix} \tag{3}其中，$(u_0, v_0)$为图像像素坐标系中的中心点，也即图像物理坐标系的原点所对应的像素点，$K= \begin{bmatrix} \frac{1}{dx} &amp; 0 &amp; u_0 \\ 0 &amp; \frac{1}{dy} &amp; v_0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}$，为相机的内参矩阵，包含是个与相机内部结构有关的参数。 摄像机坐标系与物理坐标系假设摄像机坐标系中一点$P=(x_C, y_C, z_C,)$，连接光心在$O_C$物理坐标系中的投影点为$P_{I1}=(x_I, y_I)$，则这两点之间的坐标转换关系为： \begin{cases} x_I=\frac{x_C}{z_C}\\ y_I=\frac{y_C}{z_C} \end{cases} \tag{4}转化为矩阵形式为： z_C\begin{bmatrix} x_I \\ y_I \\ 1 \end{bmatrix} = \begin{bmatrix} x_Cf \\ y_Cf \\ z_C \end{bmatrix} = \begin{bmatrix} f & 0 & 0 \\ 0 & f & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_C \\ y_C \\ z_C \end{bmatrix} \tag{5}像素坐标系与世界坐标系结合摄像机与世界坐标系 (2)，摄像机与物理坐标系 (5)，以及像素与物理坐标系 (3)，可得出像素坐标系与世界坐标系之间关系为： z_C \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = K \begin{bmatrix} f & 0 & 0 \\ 0 & f & 0 \\ 0 & 0 & 1 \end{bmatrix} T \begin{bmatrix} R & M \end{bmatrix} \begin{bmatrix} x_E \\ y_E \\ z_E \end{bmatrix} \tag{6}由上式可知，从空间中（世界坐标系）一点转换到像素坐标系中一点，只有一个位置变量（比例变量）$z_C$，即 (7) 式： z_C=\frac{fh\cos\varphi\cos\theta}{x_I\cos\phi} \tag{7}当得知摄像机高度 $h$，姿态角$(\phi, \theta, \varphi)$，以及空间（世界坐标系）中一点在物理坐标系中的投影$(x_I, y_I)$，即可通过上式来计算出空间该点的世界坐标系坐标$(x_E, y_E, z_E)$。 具体步骤： 由$(u_1, v_1)$通过式 (3) 求$x_I$； 由$x_I$通过式 (7) 求比例系数$z_C$； 由$(u_1, v_1)$通过式 (6) 求$(x_E, y_E, z_E)$]]></content>
      <categories>
        <category>UAV</category>
      </categories>
      <tags>
        <tag>UAV</tag>
        <tag>坐标系结算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标跟踪算法汇总]]></title>
    <url>%2Farticle%2FTargetTrackingAlgorithm.html</url>
    <content type="text"><![CDATA[目标视觉跟踪(Visual Object Tracking)，大家比较公认分为两大类：生成(generative)模型方法和判别(discriminative)模型方法，目前比较流行的是判别类方法，也叫检测跟踪tracking-by-detection，以下简单介绍。 生成类方法在当前帧对目标区域建模，下一帧寻找与模型最相似的区域就是预测位置，比较著名的有卡尔曼滤波，粒子滤波，mean-shift等。举个例子，从当前帧知道了目标区域80%是红色，20%是绿色，然后在下一帧，搜索算法就像无头苍蝇，到处去找最符合这个颜色比例的区域。 Camshift 算法全称为 Continuiusly Adaptive Mean-SHIFT，主要通过视频图像中运动目标的颜色特征达到跟踪目的。首先，利用鼠标响应函数手动选择出目标物体，然后提取目标物体的颜色直方图特征，再通过反向投影将目标颜色直方图转换为颜色概率分布图，初始化一个搜索窗大小和位置，并根据上一帧得到的结果自适应调整搜索窗口，进而求解目标在视频图像中的位置。 粒子滤波算法主要利用目标物体的颜色直方图特征，通过粒子预估、校正、重采样等步骤，最终提高目标跟踪算法的抗干扰能力，并保证目标跟踪的实时性。 粒子滤波又称为条件概率密度传播算法和序列蒙特卡罗方法，从本质上讲，粒子滤波是一种基于蒙特卡罗仿真的最优回归贝叶斯滤波算法，它将状态向量表示为一组带有相应权值的随机样本，并通过这些样本和权值计算出状态的估值。 光流指运动的物体经过相机成像后获得的像素在运动时的瞬时速度，光流的计算是通过图像序列中的像素灰度在相邻帧间的变化和相关性，来确定各像素位置的运动情况，即研究图像灰度在时间上的变化与空间上的运动关系。 判别类方法CV中的经典套路图像特征+机器学习， 当前帧以目标区域为正样本，背景区域为负样本，机器学习方法训练分类器，下一帧用训练好的分类器找最优区域。举个例子，在训练时告诉tracker目标80%是红色，20%是绿色，还告诉它背景中有橘红色，要格外注意别搞错了，这样的分类器知道更多信息，效果也相对更好。 经典判别类方法推荐Struck和TLD，都能实时性能还行，Struck是2012年之前最好的方法，TLD是经典long-term的代表，思想非常值得借鉴。 tracking-by-detection和检测算法非常相似，如经典行人检测用HOG+SVM，Struck用到了haar+structured output SVM，跟踪中为了尺度自适应也需要多尺度遍历搜索，区别仅在于跟踪算法对特征和在线机器学习的速度要求更高，检测范围和尺度更小而已。 相关滤波类方法correlation filter简称CF，最经典的高速相关滤波类跟踪算法CSK, KCF/DCF, CN。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Object</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo中LaTeX公式渲染]]></title>
    <url>%2Farticle%2FHexoWihLaTeX.html</url>
    <content type="text"><![CDATA[在 Hexo 中，无法显示LaTeX数学公式，这对于书写学术博客来说是很大的不便。 以下便是通过安装第三方库来解决这一问题。 安装Kramed代码Hexo 默认的渲染引擎是 Marked，但是 Marked 不支持 Mathjax。 Kramed 是在 Marked 的基础上进行修改。我们在工程目录下执行以下命令来安装 Kramed. 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 然后，更改renderer.js（路径：/node_modules/hexo-renderer-kramed/lib/renderer.js）， 更改： 12345// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');&#125; 为： 1234// Change inline math rulefunction formatText(text) &#123; return text;&#125; 停止使用 hexo-math首先，如果你已经安装 hexo-math, 请卸载它： 1npm uninstall hexo-math --save 然后安装 hexo-renderer-mathjax 包： 1npm install hexo-renderer-mathjax --save 更新 Mathjax 的 CDN 链接首先，打开mathjax.html（路径：/node_modules/hexo-renderer-mathjax/mathjax.html） 然后，把&lt;script&gt;更改为： 1&lt;script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; 在Marked中更改默认转义规则因为 Hexo 默认的转义规则会将一些字符进行转义，比如 _ 转为 &lt;em&gt;, 所以我们需要对默认的规则进行修改.首先， 打开inline.js（路径：node_modules\kramed\lib\rules\inline.js） 12# 第11行escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, 更改为: 1escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 把 12# 第20行em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 更改为: 1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 开启mathjax在主题 _config.yml (路径：/theme/next/_config.yml)中开启 Mathjax， 找到 mathjax 字段修改如下代码（false改为true）： 12mathjax: enable: true 在文章的Front-matter里打开Mathjax开关1234567---title: Hexo中LaTeX公式渲染date: 2018-03-25 16:10:25tags: [Markdown, LaTeX, Hexo]categories: Hexomathjax: true--- 测试LaTeX源码： 123456\begin&#123;eqnarray&#125;\nabla\cdot\vec&#123;E&#125; &amp;=&amp; \frac&#123;\rho&#125;&#123;\epsilon_0&#125; \\\nabla\cdot\vec&#123;B&#125; &amp;=&amp; 0 \\\nabla\times\vec&#123;E&#125; &amp;=&amp; -\frac&#123;\partial B&#125;&#123;\partial t&#125; \\\nabla\times\vec&#123;B&#125; &amp;=&amp; \mu_0\left(\vec&#123;J&#125;+\epsilon_0\frac&#123;\partial E&#125;&#123;\partial t&#125; \right)\end&#123;eqnarray&#125; 显示效果，如下所示： \begin{eqnarray} \nabla\cdot\vec{E} &=& \frac{\rho}{\epsilon_0} \\ \nabla\cdot\vec{B} &=& 0 \\ \nabla\times\vec{E} &=& -\frac{\partial B}{\partial t} \\ \nabla\times\vec{B} &=& \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right) \end{eqnarray}可以正常显示麦克斯韦方程组。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Markdown</tag>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LaTeX矩阵表示]]></title>
    <url>%2Farticle%2FLaTeXMatrix.html</url>
    <content type="text"><![CDATA[前一篇文章，对 LaTeX 中常用的数学符号及公式进行了总结，因研究中时常接触到矩阵，譬如空间坐标系的解算，或者无线通信中多天线发射和接收信号的计算，等等。因此本篇文章特别对常用的矩阵进行了总结。 简单Matrix使用$$\begin{matrix}…\end{matrix}$$来生成矩阵，其中... 表示的是LaTeX 的矩阵命令，矩阵命令中每一行以 \\ 结束，矩阵的元素之间用&amp;来分隔开。 例如： 12345678$$ \begin&#123;matrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;matrix&#125; \tag&#123;1&#125;$$ 示例： \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{matrix} \tag{1}带括号的Matrix1. […] 实例： 12345678$$ \begin&#123;bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;bmatrix&#125; \tag&#123;2&#125;$$ 示例： \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \tag{2}2. {…} 实例： 12345678$$ \begin&#123;Bmatrix&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end&#123;Bmatrix&#125; \tag&#123;3&#125;$$ 示例： \begin{Bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{Bmatrix} \tag{3}带省略符号的Matrix如果矩阵元素太多，可以使用\cdots ⋯⋯ \ddots ⋱⋱ \vdots ⋮⋮ 等省略符号来定义矩阵。 实例： 123456789$$\begin&#123;bmatrix&#125; 1 &amp; 2 &amp; \cdots &amp; 4 \\ 7 &amp; 6 &amp; \cdots &amp; 5 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 8 &amp; 9 &amp; \cdots &amp; 0 \\\end&#123;bmatrix&#125;\tag&#123;4&#125;$$ 示例： \begin{bmatrix} 1 & 2 & \cdots & 4 \\ 7 & 6 & \cdots & 5 \\ \vdots & \vdots & \ddots & \vdots \\ 8 & 9 & \cdots & 0 \\ \end{bmatrix} \tag{4}带参数的Matrix比如写增广矩阵，可能需要最右边一列单独考虑。可以用array命令来处理。 实例： 123456789$$ \left[ \begin&#123;array&#125;&#123;cc|c&#125; 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end&#123;array&#125;\right] \tag&#123;5&#125;$$ 示例： \left[ \begin{array}{cc|c} 1 & 2 & 3 \\ 4 & 5 & 6 \end{array} \right] \tag{5}其中\begin{array}{cc|c}中的c表示居中对齐元素,|用来作为分割列的符号。 行间矩阵可以使用\bigl(\begin{smallmatrix} ... \end{smallmatrix}\bigr)。 实例： 1我们使用矩阵 $\bigl( \begin&#123;smallmatrix&#125; a &amp; b \\ c &amp; d \end&#123;smallmatrix&#125; \bigr)$ 作为因子矩阵，将其... 示例： 我们使用矩阵 $\bigl( \begin{smallmatrix} a &amp; b \\ c &amp; d \end{smallmatrix} \bigr)$ 作为因子矩阵，将其…]]></content>
      <categories>
        <category>LaTeX</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>LaTeX</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown中LaTeX数学符号及公式]]></title>
    <url>%2Farticle%2FMarkdownWithMath.html</url>
    <content type="text"><![CDATA[前言整理笔记的时候经常性的输入公式，考虑过在LyX以及LaTeX里面编写之后截图或者复制过来，但总感觉效果不好并且比较费时，特整理符合Markdown中的公式源码，以便后续方便做笔记。 行间公式，在输入数学公式的时候，需要在数学公式的前后加入$$符号，将需要输入的公式加入到$$与$$中间。 行内公式，在输入数学公式的时候，需要在数学公式的前后加入$符号，将需要输入的公式加入到$与$中间。 在Typora中，输入$$然后回车，直接将TeX代码输入进去。 上下标 名称 数学表达式 Markdown源码 上标 $a^b$ a^b 下标 $a_b$ a_b 双下标 $a_{b1}$ a_{b1} 破折号 or 连字符 名称 数学表达式 Markdown源码 连字符 $F\verb - Score$ F\verb\ -\ Score 破折号 $F\verb — Score$ F\verb\ --\ Score 分数 第一个{ }写分子，第二个{ }写分母。 名称 数学表达式 Markdown源码 分数 $\frac{3+8a}{5b+6}$ \frac{3+8a}{5b+6} 累加 累加号的上标下标的前后顺序可以互换。 名称 数学表达式 Markdown源码 求和号 $\sum{3x^n}$ \sum{3x^n} 带范围求和 $\sum_{n=1}^N{3x^n}$ \sum_{n=1}^N{3x^n} 带求和上下标 \sum\limits_{n=1}^N{3x^n} \sum\limits_{n=1}^N{3x^n} 累乘 累加号的上标下标的前后顺序可以互换。 名称 数学表达式 Markdown源码 求和号 $\prod{3x^n}$ \prod{3x^n} 带范围求乘 $\prod_{n=1}^N{3x^n}$ \prod_{n=1}^N{3x^n} 开方[ ]中写的是开几次方，{ }中写的是需要开方的数值。 名称 数学表达式 Markdown源码 开方号 $\sqrt[5]{100}$ \sqrt[5]{100} 积分 名称 数学表达式 Markdown源码 积分 $\int^5_1{f(x)}{\rm d}x$ \int^5_1{f(x)}{\rm d}x 二重积分 $\iint^5_1{f(x)}{\rm d}x$ \iint^5_1{f(x)}{\rm d}x 三重积分 $\iiint^5_1{f(x)}{\rm d}x$ \iiint^5_1{f(x)}{\rm d}x 正无穷、负无穷 名称 数学表达式 Markdown源码 正无穷 $+\infty$ +\infty 负无穷 $-\infty$ -\infty 极限 名称 数学表达式 Markdown源码 左箭头 $\lim_{n\rightarrow+\infty} n$ \lim_{n\rightarrow+\infty} n 关系运算符 名称 数学表达式 Markdown源码 大于等于 $\geq$ \geq 小于等于 $\leq$ \leq 包含于 $\subset$ \subset 包含 $\supset$ \supset 属于 $\in$ \in 二元运算符 名称 数学表达式 Markdown源码 加减 $\pm$ \pm 点乘 $\cdot$ \cdot 乘 $\times$ \times 除 $\div$ \div 否定关系运算符 名称 数学表达式 Markdown源码 不等于 $\not=$ \not= 不小于 $\not&lt;$ \not&lt; 不包含 $\not\supset$ \not\supset 对数运算符 名称 数学表达式 Markdown源码 对数 $\log$ \log 对数 $\log_2{18}$ \log_2{18} 对数 $\ln$ \ln 三角运算符 名称 数学表达式 Markdown源码 垂直 $\bot$ \bot 角 $\angle $ \angle 30度角 $30^\circ$ 30^\circ 正弦 $\sin$ \sin 余弦 $\cos$ \cos 正切 $\tan$ \tan 箭头 名称 数学表达式 Markdown源码 左箭头 $\leftarrow$ \leftarrow 右箭头 $\rightarrow$ \rightarrow 长箭头 $\longrightarrow$ \longrightarrow 上箭头 $\uparrow$ \uparrow 下箭头 $\downarrow$ \downarrow 矢量 名称 数学表达式 Markdown源码 矢量 $\vec{a}$ \vec{a} 省略号 名称 数学表达式 Markdown源码 省略号 $\cdots$ \cdots 希腊字母 字母名称 大写 Markdown源码 小写 Markdown源码 alpha $A$ A $\alpha$ \alpha beta $B$ B $\beta$ \beta gamma $\Gamma$ \Gamma $\gamma$ \gamma delta $\Delta$ \Delta $\delta$ \delta epsilon $E$ E $\epsilon$ \epsilon $\varepsilon$ \varepsilon zeta $Z$ Z $\zeta$ \zeta eta $E$ E $\eta $ \eta theta $\Theta$ \Theta $\theta$ \theta iota $I$ I $\iota$ \iota kappa $K$ K $\kappa$ \kappa lambda $\Lambda$ \Lambda $\lambda$ \lambda Mu $M$ M $\mu$ \mu nu $N$ N $\nu$ \nu xi $\Xi$ \Xi $\xi$ \xi omicron $O$ O $\omicron$ \omicron pi $\Pi$ \Pi $\pi$ \pi rho $P$ P $\rho$ \rho sigma $\Sigma $ \Sigma $\sigma$ \sigma tau $T$ T $\tau$ \tau upsilon $\Upsilon$ \Upsilon $\upsilon$ \upsilon phi $\Phi$ \Phi $\phi$ \phi $\varphi$ \varphi chi $X$ X $\chi$ \chi psi $\Psi$ \Psi $\psi$ \psi sigma $\Sigma$ \Sigma $\sigma$ \sigma 空心字母与Fraktur字母A-Z皆可用 名称 符号 Markdown源码 空心字母 $\mathbb{A} $ \mathbb{A} Fraktur字母 $\mathfrak{B}$ \mathfrak{B} 段内公式 名称 示例 Markdown源码 行内公式 段内公式：$x+y=z$ \$x+y=z\$ 行间公式 x+y=z \$\$x+y=z\$\$ 分段函数示例： P_{r-j}= \begin{cases} 0 &\mbox{if r-j is odd}\\ r!\,(-1)^{(r-j)/2} &\mbox{if r-j is even} \end{cases}实例： 1234567$$P_&#123;r-j&#125;= \begin&#123;cases&#125; 0 &amp;\mbox&#123;if $r-j$ is odd&#125;\\ r!\,(-1)^&#123;(r-j)/2&#125; &amp;\mbox&#123;if $r-j$ is even&#125; \end&#123;cases&#125;$$ 公式推导过程有时一行放不下所有的推导过程，放到多行并使得每行的等号对齐可以大大增加可读性。下面这个例子原始形式是A，然后经过三步推导最终得到了D。以符号 &amp; 的下一个字符进行对齐，末尾的两个反斜杠 用来分割行。这里是由于Markdown与mathjax的渲染有冲突才需要用三个反斜杠。 示例： \begin {aligned} A&=B \\ &=C \\ &=D \end {aligned}实例： 1234567$$\begin &#123;aligned&#125;A&amp;=B \\&amp;=C \\&amp;=D\end &#123;aligned&#125;$$ 多行公式align环境可以用来对齐公式，使用&amp;符号来标记对齐的位置。 实例： 123456$$\begin&#123;align&#125;h(x) =&amp; \frac&#123;1&#125;&#123;\int_xt(x)\mathrm&#123;d&#125;x&#125; \tag&#123;1&#125;\\f(x) =&amp; \frac&#123;1&#125;&#123;\int_x\eta(x)\mathrm&#123;d&#125;x&#125;g(x)\tag&#123;2&#125;\end&#123;align&#125;$$ 示例： \begin{align} h(x) =& \frac{1}{\int_xt(x)\mathrm{d}x} \tag{1}\\ f(x) =& \frac{1}{\int_x\eta(x)\mathrm{d}x}g(x)\tag{2} \end{align}持续整理中。。。]]></content>
      <categories>
        <category>LaTeX</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
        <tag>LaTeX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++与ROS 回调函数解析]]></title>
    <url>%2Farticle%2FCplusplusAndROSCallbackFunction.html</url>
    <content type="text"><![CDATA[C++中的回调函数： A “callback” is any function that is called by another function which takes the first function as a parameter. 直白点说，就是“函数#”的参数是另一个函数，通过“函数#”调用另一个函数，这个“函数#”就是回调函数。以数学形式来看（虽然不太恰当）：Function(y)和Function(g(x))。Function(y)是一个函数，g(x)也是一个函数，那么Function(g(x))就可以看成是一个回调过程，g(x)就是回调函数。 更直接地说，我们一般都是调用OpenCV里面的库函数，现在关系反过来了，我们要让OpenCV调用一个我们自己写的函数，这个过程就是回调。那个被OpenCV调用的（我们自己写的）函数就是回调函数。接下來举例规范地说一下： 不带参数的回调函数： 12345678910111213141516171819//回调函数 void wordsCallback() &#123; std::cout&lt;&lt;"Hello World!"&lt;&lt;std::endl; &#125; //实现回调函数的"调用函数" void wods(void (*callfuction)()) &#123; callfuction(); &#125; int main(int argc,char* argv[]) &#123; words(wodsCallback); return 0; &#125; 程序的正确输出结果是：Hello World! 带参数的回调函数： 12345678910111213141516171819//回调函数 void wordsCallback(char* s) &#123; std::cout&lt;&lt;s&lt;&lt;std::endl; &#125; //实现带参回调函数的"调用函数" void words(void (*callfuction)(char*),char* s) &#123; callfuction(s); &#125; int main(int argc,char* argv[]) &#123; words(wordsCallback,"Hello World!"); return 0; &#125; 程序的正确输出结果：Hello World! ROS中的回调函数一般来说这个回调函数会用一些比较显著且比较统一的名字：**Callback（如ScanCallback/CameraCallback），它是在订阅话题的时候使用的。所以在使用时，我们需要声明订阅话题的名称，然后选择话题，最后调用Callback函数。） 举个栗子： 1234567891011void imageCallback(const sensor_msgs::ImageConstPtr&amp; msg)&#123; try &#123; cv::imshow("view", cv_bridge::toCvShare(msg, "bgr8")-&gt;image); &#125; catch (cv_bridge::Exception&amp; e) &#123; ROS_ERROR("Could not convert from '%s' to 'bgr8'.", msg-&gt;encoding.c_str()); &#125;&#125; 1image_transport::Subscriber sub_raw_image = it.subscribe("camera/rgb/image_raw", 1, imageCallback); 先声明订阅的话题的名称： 1sub_raw_image 然后，选择我们需要订阅的话题。 1"camera/rgb/image_raw" 题外话，ROS中有很多话题，有的是自己写的，有的来自其他节点发布的。 1代表我们一次性可以缓存多少消息，最后那个就是回调函数了。 1imageCallback ROS中，imageCallback的参数是与话题息息相关的，要跟据订阅的话题来确定参数。 一般来说，程序的主要功能也都是在回调函数中实现的。 或者说，将来自【订阅话题】中的数据，传递给【目标函数】进行处理。 需要的数据，都是要通过话题订阅的，而订阅了就肯定有回调函数。可以把一个个回调函数看成是一个个单独的线程。 只要订阅的消息一更新（当有消息到达topic时），回调函数就会被调用（ROS就会调用imageCallback ()函数），对新的（到达的）数据进行处理，程序就这么不断的进行下去了。 这里我们仍旧举例： 123void forwardCallback(const nav_msgs::Odometry::ConstPtr &amp;forward_msg)&#123;&#125; void imageCallback(const sensor_msgs::ImageConstPtr &amp;image_msg)&#123;&#125; 需要确定的就是这个sensor_msgs::ImageConstPtr和nav_msgs::Odometry::ConstPtr. 使用类作为回调函数： 举个栗子： 12345class Listener&#123;public: void callback(const std_msgs::String::ConstPtr&amp; msg);&#125;; 12Listener listen;ros::Subscriber sub = n.subscribe("chatter", 1000, &amp;Listener::callback, &amp;listen); 如果订阅在Listener内部，你替换最后的参数为关键词this，它意味着订阅会引用类的一部分。 以上！ 附： &amp; 有 “引用” 和 “ 取地址”的含义 int a; int &amp;b = a;上述两行语句执行完之后，a、b表示同一个变量，对其中一个的操作相当于对另一个的操作。 int a; int *b = &amp;a;则b中储存的是a的地址。对*b的任何操作相当于对a的操作。]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ cout控制输出\流控制]]></title>
    <url>%2Farticle%2FCplusplusCout.html</url>
    <content type="text"><![CDATA[I/O的书写格式I/0流是输入或输出的一系列字节，当程序需要在屏幕上显示输出时，可以使用插入操 作符“&lt;&lt;”向cout输出流中插入字符。例如： 1cout&lt;&lt;”This is a program.\n"； 当程序需要执行键盘输入时，可以使用抽取操作符 “&gt;&gt;”从cin输人流中抽取字符。例如： 12int myAge；cin&gt;&gt;myAge； 不管把什么基本数据类型的名字或值传给流，它都能懂。例如，下面的函数输出字符串和整数： 12345678#include &lt;iostream.h&gt;void main()&#123; cout &lt;&lt; "My name is Jone\n"; cout "the ID is"; cout &lt;&lt; 2; cout &lt;&lt; endl;&#125; 上面的输出也可以在同一行中串连，下面的输出语句与上例输出同样 内容： 1cout &lt;&lt;"My name is Jone\n" &lt;&lt;”the ID is"&lt;&lt; 2 &lt;&lt;endl； 也可以分在几行，提高可读性，下列语句与上例输出同样结果： 1234cout&lt;&lt;"My name is Jonen" //行末无分号&lt;&lt;"the ID is"&lt;&lt;2&lt;&lt;endl； cin可以和cout一样的方式调整行，它自动 识别变量位置和类型。例如： 12int i；float f；long l；cin &gt;&gt;i &gt;&gt;f &gt;&gt;l； cin能够知道抽取的变量之类型，它将对i，f，l分别给出一个整型、浮点型和长整型数。 使用控制符流的默认格式输出有时不能满足特殊要求，如： 12double average=9.400067；cout&lt;&lt;average&lt;&lt;endl； 希望显示的是9.40，即保留两位小数，可是却显示了9.40007；默认显示6位有效位。 用控制符(manipulators)可以对I/O流的格式进行控制。控制符是在头文件iomanip.h中定义的对象。可以直接将控制符插入流中。常用 控制符如表2-4所列。 表 I／O流的常用控制符 控制符 描 述 dec hex oct setfill(c) setdivcision(n) setw(n) setiosflags(ios::fixed) setiosflags(ios::scientific) setiosflags(ios::left) setiosflags(ios::right) setiosflags(ios::skipws) setiosflags(ios::uppercase) setiosflags(ios::lowercase) 置基数为10 置基数为16 置基数为8 没填充字符为c 没显示小数精度为n位 设域宽为n个字符 固定的浮点显示 指数表示 左对齐 右对齐 忽略前导空白 16进制数大写输出 16 进制数小写输出 使用控制符时，要在程序的头上加头文件iomanip.h。 控制浮点数值显示使用setdivcision(n)可控制输出流显示浮点数的数字个 数。C++默认的流输出数值有效位是6。如果setdivcision(n)与setiosflags(ios::fixed)合用，可以控制小数点右边的数字个数。setiosflags(ios::fixed)是用定点方式表示实数。如果与 setiosnags(ios::scientific)合用， 可以控制指数表示法的小数位数。setiosflags(ios::scientific)是用指数方式表示实数。例如，下面的代码分别用浮 点、定点和指数方式表示一个实数： 1234567891011121314151617181920212223//*********************//** ch2_1.cpp **//*********************#include &lt;iostream.h&gt;#include &lt;iomanip.h&gt; //要用到格式控制符void main()&#123; double amount = 22.0/7; cout &lt;&lt;amount &lt;&lt;endl; cout &lt;&lt;setdivcision(0) &lt;&lt;amount &lt;&lt;endl &lt;&lt;setdivcision(1) &lt;&lt;amount &lt;&lt;endl &lt;&lt;setdivcision(2) &lt;&lt;amount &lt;&lt;endl &lt;&lt;setdivcision(3) &lt;&lt;amount &lt;&lt;endl &lt;&lt;setdivcision(4) &lt;&lt;amount &lt;&lt;endl; cout&lt;&lt;setiosflags(ios::fixed); cout &lt;&lt;setdivcision(8) &lt;&lt;amount &lt;&lt;endl; cout&lt;&lt;setiosflags(ios::scientific)&lt;&lt;amount&lt;&lt;endl; cout &lt;&lt;setdivcision(6);//重新设置成原默认设置&#125; 运行结果为： 123456783.14286333.13.143.1433.142857143.14285714e+00 该程序在32位机器上运行通过。在用浮点表示的输出中，setdivcision(n)表示有效位数。第1行输出数值之前没有设置有效位数，所以用流的有效位数默认设置值6：第2个输出设置了有效位数0，C++最小的有效位数为1，所以作为有效位数设置 为1来看待：第3～6行输出按设置的有效位数输出。在用定点表示的输出中，setdivcision(n)表示小数位数。第7行输出是与setiosflags(ios::fixed)合用。所以setdivcision(8)设置的是小数点后面的位数，而非全部数字个 数。在用指数形式输出时，setdivcision(n)表示小数位数。第8行输出用 setiosflags(ios::scientific)来表示指数表示的输出形式。其有效位数沿用上次的设置值8。小数位数截短显示时，进行4舍5入处理。 设置值的输出宽度除了使用空格来强行控制输出间隔外，还可以用 setw(n)控制符。如果一个值需要比setw(n)确定的字符数更多的字符，则该值将使用它所需要的所有字符。例如： 12float amount=3.14159；cout &lt;&lt;setw(4) &lt;&lt;amount &lt;&lt;endl； 其运行结果为：3.14159。它并不按4位宽度，而是按实际宽度输出。如果一个值的字符数比setw(n)确定的字符个数更少，则在数字 字符前显示空白，不同于其他控制符，setw(n)仅仅影响下一个数值输出，换句话说，使用setw设置的间隔方式并不保留其效力。例如： 123456cout&lt;&lt;setw(8)&lt;&lt;10&lt;&lt;20&lt;&lt;endl；//运行结果为：-------1020 运行结果中的下横线表示空 格。整数20并没有按宽度8输出。setw()的默认值为宽度0，即setw(0)，意思是按输出数值的表示宽度输出， 所以20就紧挨10了。若要每个数值都有宽度8，则每个值都要设置： 12cout&lt;&lt;setw(8)&lt;&lt;10&lt;&lt;setw(8) &lt;&lt;20 &lt;&lt;endl； 输出8进制和16进制数3个常用的控制符是hex，oct和dec，它们分别对应16 进制、8进制和10进制数的显示。这3个控制符在iostream.h头文件中定义。例如：12345678910111213//*********************//** ch2_2.cpp **//*********************#include &lt;iostream.h&gt;void main()&#123;int number=1001;cout &lt;&lt;"Decimal:" &lt;&lt;dec &lt;&lt;number &lt;&lt;endl&lt;&lt;"Hexadecimal:" &lt;&lt;hex &lt;&lt;number &lt;&lt;endl&lt;&lt;"Octal:" &lt;&lt;oct &lt;&lt;number &lt;&lt;endl;&#125; 运行结果为：123Decimal：1001Hexadecimal：3e9Octal：1751 1001是一个10进制数，不能把它理解成16进制或8进制数，因为它不是以0x或0开头。但 在输出时， 流根据控制符进行过滤， 使其按一定的进制来显示。用头文件iomanip.h中的 setiosflags(ios::uppercase)可以控制16进制数大写输出。例如,上例中增加一个头文件,对16进制数进行大写控制，即：1234567#include&lt;iostream.h&gt;#include&lt;iomanip.h&gt;//...cout&lt;&lt;"Hexadecimal:"&lt;&lt;hex &lt;&lt;setiosftags(ios::uppercase) &lt;&lt;number&lt;&lt;endl； 便能得到16进制数的大写表示：Hexadecimal：3E9。 设置填充字符setw可以用来确定显示的宽度。默认时，流使用空格符来保证字符间的正确间隔。用setfill控制符可以确定一个非空格的别的字符。Setfill在头文件iomanip·h中定义。例如：12345678910111213141516//*********************//** ch2_3.cpp **//*********************#include &lt;iostream.h&gt;#include &lt;iomanip.h&gt;void main()&#123; cout &lt;&lt;setfill('*') &lt;&lt;setw(2) &lt;&lt;21 &lt;&lt;endl &lt;&lt;setw(3) &lt;&lt;21 &lt;&lt;endl &lt;&lt;setw(4) &lt;&lt;21 &lt;&lt;endl; cout &lt;&lt;setfill(' '); // 恢复默认设置&#125; 运行结果为：12321*21**21 左右对齐输出默认时，I/O流左对齐显示的内容。使用头文件iomanip.h中的 setiosflags(ios::left)和(ios::right)标志，可以控制输出对齐。例如：12345678910111213141516171819//*********************//** ch2_4.cpp **//*********************#include &lt;iostream.h&gt;#include &lt;iomanip.h&gt;void main()&#123;cout &lt;&lt;setiosflags(ios::right) &lt;&lt;setw(5) &lt;&lt;1 &lt;&lt;setw(5) &lt;&lt;2 &lt;&lt;setw(5) &lt;&lt;3 &lt;&lt;endl;cout&lt;&lt;setiosflags(ios::left) &lt;&lt;setw(5) &lt;&lt;1 &lt;&lt;setw(5) &lt;&lt;2 &lt;&lt;setw(5) &lt;&lt;3 &lt;&lt;endl;&#125; 运行结果为：12-----1-----2-----31-----2-----3----- 强制显示小数点和符号当程序输出下面的代码时： 1cout&lt;&lt;10.0/5&lt;&lt;endl； 默认的I/0流会简单地显示2，而非2.0，因为除法的结果是精确的。当 需要显示小数点时，可以用iso::showpoint标志。例如：1234567891011121314//*********************//** ch2_5.cpp **//*********************#include &lt;iostream.h&gt;#include &lt;iomanip.h&gt;void main()&#123;cout &lt;&lt;10.0/5 &lt;&lt;endl;cout&lt;&lt;setiosflags(ios::showpoint) &lt;&lt;10.0/5 &lt;&lt;endl;&#125; 运行结果为：1222.00000 默认时，I/O流 仅在负数之前显示值的符号，根据程序的用途，有时也需要在正数之前加上正号，可以用ios::showpos标志。例如：1234567891011121314//*********************//** ch2_6.cpp **//*********************#include &lt;iostream.h&gt;#include &lt;iomanip.h&gt;void main()&#123;cout &lt;&lt;10 &lt;&lt;" " &lt;&lt;-20 &lt;&lt;endl;cout&lt;&lt;setiosflags(ios::showpos) &lt;&lt;10 &lt;&lt;" " &lt;&lt;-20 &lt;&lt;endl;&#125; 运行结果为：1210 -20+10 -20 小 结变量是程序分配给某个内存位置的名字，它可以存放信息。程序在使用变量前，必须先说明变量名和变量 类型。不同的变量不能同名，变量名应该尽量反映出变量的用途，以增强程序的可读性。在程序运行中，常量的值不可改变。常量也有各 种数据类型，也占有存储空间。各种数据类型的数据表示有一定的范围，越过了该范围， C++就要对该数据进行截取，使得数据不再正确。利用cout可以输出各种数据类型的数据，可以多种方式在屏幕上显示输出信息(包括特殊符号)。C++兼容C的库函数，所以printf()和 scanf()也可照常使用。]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Companion computer 对UAV的外部控制]]></title>
    <url>%2Farticle%2FCompanionComputer2Pixhawk.html</url>
    <content type="text"><![CDATA[Enter password to read. Incorrect Password! No content to display! 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]]></content>
      <categories>
        <category>UAV</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>MAVROS</tag>
        <tag>Offboard</tag>
        <tag>Pixhawk</tag>
        <tag>UAV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark部署架构方式]]></title>
    <url>%2Farticle%2FSparkArchitecturalSchemes.html</url>
    <content type="text"><![CDATA[Spark架构 用Spark架构具有如下优点： 实现一键式安装和配置、线程级别的任务监控和告警 降低硬件集群、软件维护、任务监控和应用开发的难度 便于做成统一的硬件、计算平台资源池 注：Spark Streaming无法实现毫秒级的流计算，因此，对于需要毫秒级实时响应的企业应用而言，仍然需要采用流计算框架（如Storm） Spark+Hadoop架构Hadoop和Spark的统一部署缘由： Hadoop生态系统中的一些组件所实现的功能，目前还是无法由Spark取代的，比如，Storm 现有的Hadoop组件开发的应用完全转移到Spark上需要一定的成本 不同的计算框架统一运行在YARN中，可以带来如下好处： 计算资源按需伸缩 不用负载应用混搭，集群利用率高 共享底层存储，避免数据跨集群迁移]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL、Streaming and MLlib]]></title>
    <url>%2Farticle%2FSparkSQLStreamingandMLlib.html</url>
    <content type="text"><![CDATA[Spark SQL 支持多种结构化数据，如JSON, Hive, Parquet等 Can be created from external data sources, from the results of queries, or from regular RDDs Spark Streaming Spark MLlib支持多类机器学习算法。]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark RDD学习笔记]]></title>
    <url>%2Farticle%2FSparkRDD.html</url>
    <content type="text"><![CDATA[RDD：Resilient Distributed Dataset（弹性分布式数据集），是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。 RDD分为Action（动作）和Transformation（转换）两种操作类型。 RDD的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作。 执行方式Lazy Fashion（惰性调用） ​ DAG：Directed Acyclic Graph（有向无环图），反映RDD之间的依赖关系。 RDD运行过程 创建RDD对象； SparkContext负责计算RDD之间的依赖关系，构建DAG； DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。 参考示例12345text_file = sc.textFile("hdfs://skn-0exybxsb-hadoop-master:9000/use/ubuntu/sample1")counts = text_file.flatMap(lambda line: line.split(" "))\ .map(lambda word: (word, 1))\ .reduceByKey(lambda a, b: a + b)counts.collect()]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark集群管理架构]]></title>
    <url>%2Farticle%2FSparkCluster.html</url>
    <content type="text"><![CDATA[Spark集群管理架构包括： 集群资源管理器（ClusterManager） 运行作业任务的工作节点（Worker Node） 每个应用的任务控制节点（Driver） 每个工作节点上负责具体任务的执行进程（Executor） where 资源管理器可以自带或Mesos或YARN Spark集群管理流程 首先为应用构建起基本的运行环境，即由Driver创建一个SparkContext，进行资源的申请、任务的分配和监控。 资源管理器为Executor分配资源，并启动Executor进程。 SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，TaskScheduler将Task发放给Executor运行，并提供应用程序代码。 Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源 。 优点与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点： 利用多线程来执行具体的任务，减少任务的启动开销。 Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销。]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是Spark？]]></title>
    <url>%2Farticle%2FWhatsSpark.html</url>
    <content type="text"><![CDATA[简介：一个围绕速度、易用性和复杂分析构建的通用大数据计算框架。 来源：Spark最初由美国加州伯克利大学（UCBerkeley）的AMP实验室于2009年开发，是基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程序。 ​ 2013年Spark加入Apache孵化器项目后发展迅猛，如今已成为Apache软件基金会最重要的三大分布式计算系统开源项目之一（Hadoop、Spark、Storm）。 运行速度快，消耗资源少 基于内存计算，全局优化的工作流编排 比MapReduce快100倍（内存），或10倍（磁盘） 计算跟着数据走 简单易用，支持交互式查询 易用的APIs（Python, JAVA, Scala, and SQL） 丰富的内置库函数 包含多个库的全栈式平台，良好的生态系统 减轻运维，且各组件之间易于集成 支持所有Hadoop生态系统中的数据源 可以基于云计算 Spark生态系统组件应用场景]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce On Yarn工作原理]]></title>
    <url>%2Farticle%2FMapReduceWihtYARN.html</url>
    <content type="text"><![CDATA[用户向Yarn提交一个MapReduce应用，Yarn会分两个阶段运行： ResourceManager启动MRAppMaster 其中，MapReduce的ApplicationMaster负责管理作业生命周期 MRAppMaster创建应用程序，申请资源，并监控其运行过程，直到运行成功 MRAppMaster的3种运行模式：本地，uber，非uber； MapReduce on Yarn工作流程： 用户向YARN中提交应用程序/作业，包括MRAppMaster程序、启动MRAppMaster的命令、用户程序等； ResourceManager为作业分配第一个Container，并与对应的NodeManager通信，要求它在这个Containter中启动该作业的MRAppMaster； MRAppMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查询作业的运行状态；然后它将为各个任务申请资源并监控任务的运行状态，直到运行结束。即重复步骤4-7； MRAppMaster采用轮询方式通过RPC请求向RM申请和领取资源； MRAppMaster申请到资源后，便与对应的NM通信，要求它启动任务； NodeManager使用脚本启动任务（环境变量，jar包等）； 各个任务通过RPC协议向MRAppMaster汇报自己的状态和进度，以让MRAppMaster 随时掌握各任务的运行状态，从而可以在任务失败时重新启动任务；在作业运行过程中，用户可随时通过RPC向ApplicationMaster查询作业当前运行状态； 作业完成后，MRAppMaster向ResourceManager注销并关闭自己；]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是大数据？]]></title>
    <url>%2Farticle%2FWhatsBigData.html</url>
    <content type="text"><![CDATA[什么是大数据？四个“V”： 海量的数据规模（Volume） 快速的数据流转和动态的数据体系（Velocity） 多样的数据类型（Variety） 巨大的数据价值（Value） Wikipedia：大数据是指用常用的软件获取、管理和处理数据所耗时间超过可容忍时间的数据集； 数据仓库or生命周期]]></content>
      <categories>
        <category>Big Data</category>
      </categories>
      <tags>
        <tag>Big Data</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开机出现“CPU fan error”错误的解决方案]]></title>
    <url>%2Farticle%2FCPUFanError.html</url>
    <content type="text"><![CDATA[问题描述：开机出现“CPU fan error”错误的解决方案 原因分析： CPU风扇线接触不良 系统BIOS检测不到CPU风扇的转速 风扇工作不正常或转速太低 风扇电源线接错了 主板电池没电了 解决方案： CPU风扇线接触不良 重新将CPU风扇线拔插下 系统BIOS检测不到CPU风扇的转速 开机后按【Del】进入【BIOS】选项，进入【Power→Hardware monitor】，你会看到有三个在一起的分别是CPU FAN SPEED;CHASSIS FANSPEED;POWER FAN SPEED，将第一项改成【IGNORED】，第二项改成【N/A】，第三项改成【IGNORED】。 风扇工作不正常或转速太低 风扇工作时间过长，里面的润滑油干了，导致风扇转速变慢了，在风扇的内部加点润滑油，加大风扇的转速会降低您机箱内部的温度，以至于不会使【CPU】烧坏。 风扇电源线接错了 一般主板上都有多个风扇的插口，【CPU】的风扇应该插在【CPU_F】这个上面，如果不插在这里虽然风扇正常，但是主板会提示错误。将【CPU】风扇的电源插头插到相应的位置，一般问题可以解决。 主板电池没电了 将主板上的【COMS】电池拆下来，然后等几分钟后装回去，就可以将主板【COMS】放电恢复默认;最后在【BIOS】里面使用默认设置。 以上几点就是开机出现【CPU fan error】的提示的原因和解决办法，CPU和风扇之间硅胶没了，导致热量散发不出去，机箱内灰尘太多也会导致开机出现【CPU fan error】的提示]]></content>
      <categories>
        <category>Issues</category>
      </categories>
      <tags>
        <tag>Issues</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ROS测试Caffe MobileNet-SSD【通用】]]></title>
    <url>%2Farticle%2FTestCaffeMobileNetSSD.html</url>
    <content type="text"><![CDATA[在测试的过程中和群友们仔细的探讨了下，很快的解决了测试中遇见的一些细节问题，学东西还是尽量和周围的人一起学，毕竟老话说过“人多力量大！”。 自己一个人也不是不可以，但是往往要比很多人一起探讨花费两倍或者五六倍的时间，所以强烈建议要有那么几个共同学习的小圈子。 修改demo.py12# 设置成绝对路径caffe_root = '/home/cong/catkin_ws/src/ros_caffe/caffe/' 修改solver_test.prototxt有GPU，则不需要修改配置文件，如果没有GPU，则需要修改配置文件 solver_test.prototxt 123# 将solver_mode:GPU 改为 solver_mode:CPU# solver_mode:GPUsolver_mode:CPU 运行demo.py12cd catkin_ws/src/ros_caffe/caffe/examples/MobileNet-SSD/python demp.py 在图像检测输出的界面，按回车或者其他按键，会检测下一张。 Debugs1234# BugImportError: No module named skimage.io# Debugpip install scikit-image 1234# BugImportError: No module google.protobuf.internal# Debugpip install protobuf 完成上面安装步骤后，执行以下命令重新编译： 1234make cleanmake -j8make pymake test -j8]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>SSD</tag>
        <tag>ROS</tag>
        <tag>MobileNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ROS安装Caffe和SSD【通用】]]></title>
    <url>%2Farticle%2FROSCaffeSSDMobileNet.html</url>
    <content type="text"><![CDATA[安装ROS Caffe SSD MobileNet1234567891011# 安装Caffe_ROScd ~/catkin_ws/srcgit clone https://github.com/tzutalin/ros_caffe.git# 安装Caffe_SSDcd ~/catkin_ws/src/ros_caffegit clone https://github.com/weiliu89/caffe.gitcd ~/catkin_ws/src/ros_caffe/caffegit chechout ssd# 安装MobileNet_SSDcd ~/catkin_ws/src/ros_caffe/caffe/examplesgit clone https://github.com/chuanqi305/MobileNet-SSD.git 安装依赖包1234567891011sudo apt-get install libatlas-base-devsudo apt-get install libprotobuf-devsudo apt-get install libleveldb-devsudo apt-get install libsnappy-devsudo apt-get install libopencv-devsudo apt-get install libboost-all-devsudo apt-get install libhdf5-serial-devsudo apt-get install libgflags-devsudo apt-get install libgoogle-glog-devsudo apt-get install liblmdb-devsudo apt-get install protobuf-compiler 安装OpenCV123456cd caffesudo git clone https://github.com/jayrambhia/Install-OpenCVcd Install-OpenCV/Ubuntusudo sh dependencies.shcd 2.4sudo sh opencv2_4_10.sh 编译Caffe123456789cd catkin_ws/src/ros_caffe/caffe# 修改配置文件cp Makefile.config.example Makefile.configsudo gedit Makefile.config #参考文末链接# 编译make all # make all = make = make bzImage + make modulesmake -j8make pymake test -j8 安装usb_cam12cd catkin_ws/srcgit clone https://github.com/bosch-ros-pkg/usb_cam.git 编译ROS12cd catkin_wscatkin_make 运行外部程序123456# 启动ROSroscore# 开启摄像头节点# 提示找不到节点，参考：http://wangcong.info/article/ROSGazeboPixhawk.html#仿真roslaunch usb_cam usb_cam-test.launch# 运动外部程序 Debugs1234567# Bug/usr/bin/ld: cannot find -lopenblascollect2: error: ld return 1 eixt statuesmakefile: 566: recipe for target 'build_rekease/lib/libcaffe.so.1.0.0-rc3' failed# Debugsudo apt-get install libopenblas-dev 1234567# Bugfatal error: caffe/proto/caffe.pb.h: No such file or directory# Debugprotoc src/caffe/proto/caffe.proto --cpp_out=.sudo mkdir include/caffe/protosudo mv src/caffe/proto/caffe.pb.h include/caffe/proto 参考：安装Caffe]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>SSD</tag>
        <tag>ROS</tag>
        <tag>MobileNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ros::spin()和ros::spinOnce()解析]]></title>
    <url>%2Farticle%2FROSSpinAndSpinOnce.html</url>
    <content type="text"><![CDATA[ros::spin()这句话的意思是循环且监听反馈函数（callback）。循环就是指程序运行到这里，就会一直在这里循环了。监听反馈函数的意思是，如果这个节点有callback函数，那写一句ros::spin()在这里，就可以在有对应消息到来的时候，运行callback函数里面的内容。就目前而言，以我愚见，我觉得写这句话适用于写在程序的末尾（因为写在这句话后面的代码不会被执行），适用于订阅节点，且订阅速度没有限制的情况。 ros::spinOnce()这句话的意思是监听反馈函数（callback）。只能监听反馈，不能循环。所以当你需要监听一下的时候，就调用一下这个函数。这个函数比较灵活，尤其是我想控制接收速度的时候。配合ros::ok()效果极佳。例如 123456ros::Rate loop_rate(10);while(ros::ok())&#123; ros::spinOnce(); loop_rate.sleep();&#125; 可以控制10Hz速度，运行callback函数，非常方便。如果只有 1234while(ros::ok())&#123; ros::spinOnce();&#125; 这就等于ros::spin()。 注意这两个函数只和接收回调函数（callback）有关，和发布并没有关系。如果想循环发布，只能循环写publish()。见参考文献[1]的发布节点示例。我这次出问题就是和发布弄混了。我以为写了spin()可以循环发布的，后来发现并不是。 参考文献 Writing a Simple Publisher and Subscriber (C++) Significance of ros::spinOnce()]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ROS与Gazebo的Pixhawk仿真]]></title>
    <url>%2Farticle%2FROSGazeboPixhawk.html</url>
    <content type="text"><![CDATA[心路历程 因为在之前搭建深度学习Caffe框架的过程中安装了Anaconda，因此后面搭建Pixhawk+Gazebo仿真环境时，不断出错（错误如下方所示）。 反反复复卸载安装相关组件，尝试了各种办法，花了至少4天时间，最终还是没能解决问题。 之后，为了排除其他因素影响，我在Windows下面直接搭了虚拟机，尝试着一步步搭建，终于可以跑通了。 在这几天的摸爬滚打的过程中，过程虽然煎熬，但自己也学习了不少Linux相关知识，对这个系统了解的程度也比以前深了些。 123456789&gt;/home/cong/src/Firmware/build/posix_sitl_default/build_gazebo/Float.pb.h:12:2: error: &gt;#error This file was generated by a newer version of protoc which is&gt; ^&gt;/home/cong/src/Firmware/build/posix_sitl_default/build_gazebo/Float.pb.h:13:2: error: &gt;#error incompatible with your Protocol Buffer headers. Please update&gt; ^&gt;/home/cong/src/Firmware/build/posix_sitl_default/build_gazebo/Float.pb.h:14:2: error: &gt;#error your headers.&gt; 安装ROS一定要确定Ubuntu与ROS的版本对应 如果安装的是ros-kinetic-desktop-full，其自带gazebo7 ROS发布日期 ROS版本 对应Ubutnu版本 2016.3 ROS Kinetic Kame Ubuntu 16.04 (Xenial) / Ubuntu 15.10 (Wily) 2015.3 ROS Jade Turtle Ubuntu 15.04 (Wily) / Ubuntu LTS 14.04 (Trusty) 2014.7 ROS Indigo Igloo Ubuntu 14.04 (Trusty) 2013.9 ROS Hydro Medusa Ubuntu 12.04 LTS (Precise) 2012.12 ROS Groovy Galapagos Ubuntu 12.04 (Precise) … … … 添加源： 1sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/ros-latest.list' 或添加国内源： 1sudo sh -c '. /etc/lsb-release &amp;&amp; echo "deb http://mirrors.ustc.edu.cn/ros/ubuntu/ $DISTRIB_CODENAME main" &gt; /etc/apt/sources.list.d/ros-latest.list' 设置秘钥： 1sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 0xB01FA116 安装最新桌面版： 1234# 更新源sudo apt-get update# 安装 ros-kinetic-desktopsudo apt-get install ros-kinetic-desktop-full 初始化ROS初始化rosdep： 12sudo rosdep initrosdep update 初始化环境变量: 12echo "source /opt/ros/kinetic/setup.bash" &gt;&gt; ~/.bashrcsource ~/.bashrc 上面两句非常非常重要，很多小伙伴在日常的开发过程中，有的找不到 Package, 找不到node, 很多情况下都是没有添加source。 创建catkin工作空间： 1234mkdir -p ~/catkin_ws/srccd ~/catkin_ws#可以测试下catkin_make 安装MAVROS1sudo apt-get install ros-kinetic-mavros ros-kinetic-mavros-extras ros-kinetic-control-toolbox 下载一些Geoid Model dataset： 1sudo /opt/ros/kinetic/lib/mavros/install_geographiclib_datasets.sh 外部控制程序创建外部控制程序包 12cd ~/catkin_ws/srccatkin_create_pkg offboard roscpp mavros geometry_msgs 这一步会创建一个名为offboard的新程序包，这个程序包依赖于roscpp 、mavros以及geometry_msgs。 在offboard目录下生自动成两个文件夹include和src。 将外部控制例程offboard_node.cpp放入刚刚生成的src目录下。然后修改/catkin_ws/src/offboard目录下的CMakeLists.txt文件。取消掉一些注释，生成相应节点（否则会出现找不到节点的错误）。 编译 12cd ~/catkin_wscatkin_make 仿真1234567891011# Terminal 1 切换到固件目录cd ~/src/Firmware# 启动gazebo仿真make posix_sitl_default gazebo# Terminal 2 启动MAVROS,链接到本地ROSroslaunch mavros px4.launch fcu_url:="udp://:14540@127.0.0.1:14557"# Terminal 3 运行外部控制程序rosrun offboard offboard_node# 若出现 [rospack] Error: package 'offboard' not found 执行下面两行命令# echo "source ~/catkin_ws/devel/setup.bash" &gt;&gt; ~/.bashrc# source ~/.bashrc Debugs123456789101112# Bug[FATAL] [1524021465.593366921]: UAS: GeographicLib exception: File not readable /usr/share/GeographicLib/geoids/egm96-5.pgm | Run install_geographiclib_dataset.sh script in order to install Geoid Model dataset!===============================================================================REQUIRED process [mavros-2] has died!process has finished cleanlylog file: /home/cong/.ros/log/0f7b4d4e-42b7-11e8-b72f-3464a913a149/mavros-2*.logInitiating shutdown!===============================================================================# Debugsudo /opt/ros/kinetic/lib/mavros/install_geographiclib_datasets.sh 123456# Bug找不到empy或toml# Debugsudo apt-get install python-pippip install toml]]></content>
      <categories>
        <category>UAV</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>Pixhawk</tag>
        <tag>Gazebo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下Pixhawk环境搭建]]></title>
    <url>%2Farticle%2FPixhawkOnUbuntu.html</url>
    <content type="text"><![CDATA[网上很多博客中介绍的方法大多比较久远的了，自己综合参考了不少博客的过程中，还是踩了很多坑，特总结下来，以便后面重搭环境时之需。 官方的配置教程 前期准备 设置权限 1sudo usermod -a -G dialout $USER 安装相关工具 123456789sudo add-apt-repository ppa:george-edison55/cmake-3.x -ysudo apt-get update# 必备软件 python git qtsudo apt-get install python-argparse git-core wget zip python-empy qtcreator cmake build-essential genromfs -y# 仿真工具sudo add-apt-repository ppa:openjdk-r/ppasudo apt-get updatesudo apt-get install openjdk-8-jresudo apt-get install ant protobuf-compiler libeigen3-dev libopencv-dev openjdk-8-jdk openjdk-8-jre clang-3.5 lldb-3.5 -y 卸载模式管理器 1sudo apt-get remove modemmanager 更新包列表和安装下面的依赖包 1234567sudo add-apt-repository ppa:terry.guo/gcc-arm-embedded -ysudo add-apt-repository ppa:team-gcc-arm-embedded/ppasudo apt-get updatesudo apt-get install python-serial openocd \ flex bison libncurses5-dev autoconf texinfo build-essential \ libftdi-dev libtool zlib1g-dev \ python-empy -y 工具链安装（若gcc-arm-none-eabi版本不对）（非必须） 如果以前装过工具链，删除残余 12sudo apt-get remove gcc-arm-none-eabi gdb-arm-none-eabi binutils-arm-none-eabi gcc-arm-embeddedsudo add-apt-repository --remove ppa:team-gcc-arm-embedded/ppa 安装工具链 特殊原因，推荐使用浏览器下载：https://launchpad.net/gcc-arm-embedded/5.0/5-2016-q2-update/+download/gcc-arm-none-eabi-5_4-2016q2-20160622-linux.tar.bz2 1234567pushd .cd ~tar -jxf gcc-arm-none-eabi-5_4-2016q2-20160622-linux.tar.bz2exportline="export PATH=$HOME/gcc-arm-none-eabi-5_4-2016q2/bin:\$PATH"if grep -Fxq "$exportline" ~/.profile; then echo nothing to do ; else echo $exportline &gt;&gt; ~/.profile; fi. ~/.profilepopd 这是安装最新的工具链，版本是5.4。 安装过程 下载代码 123mkdir -p ~/srccd ~/srcgit clone https://github.com/PX4/Firmware.git 初始化 123cd Firmware# 下载相关子模块git submodule update --init --recursive 编译 1make px4fmu-v2_default 成功后显示： 12345[100%] Built target nuttx_px4fmu-v2_default.elfScanning dependencies of target px4[100%] Generating px4fmu-v2.bin[100%] Creating /home/cong/src/Firmware/build/px4fmu-v2_default/px4fmu-v2_default.px4[100%] Built target px4 若报错： 1234-- Configuring incomplete, errors occurred!make[1]: *** /root/src/Firmware/build/nuttx_px4fmu-v2_default: No such file or directory. Stop.Makefile:154: recipe for target 'px4fmu-v2_default' failedmake: *** [px4fmu-v2_default] Error 2 解决办法：安装最新gcc-arm-none-eabi版本 上传Pixhawk1sudo make px4fmu-v2_default upload Debug:内存溢出： https://dev.px4.io/en/debug/faq.html 12345678910region `flash' overflowed by 24405 bytescollect2: error: ld returned 1 exit statusplatforms/nuttx/CMakeFiles/nuttx_px4fmu-v2_default.elf.dir/build.make:187: recipe for target 'nuttx_px4fmu-v2_default.elf' failedmake[3]: *** [nuttx_px4fmu-v2_default.elf] Error 1CMakeFiles/Makefile2:5057: recipe for target 'platforms/nuttx/CMakeFiles/nuttx_px4fmu-v2_default.elf.dir/all' failedmake[2]: *** [platforms/nuttx/CMakeFiles/nuttx_px4fmu-v2_default.elf.dir/all] Error 2Makefile:105: recipe for target 'all' failedmake[1]: *** [all] Error 2Makefile:153: recipe for target 'px4fmu-v2_default' failedmake: *** [px4fmu-v2_default] Error 2 解决方法： 编译器版本太低。将arm-none-eabi升级到最新版本 参考本文上面的教程 减少不必要的模块 /Firmware/cmake/configs/nuttx_px4fmu-v2_default.cmake 注释： 123456789#drivers/px4flow#drivers/distance_sensor/ll40ls#drivers/distance_sensor/mb12xx#drivers/distance_sensor/sf0x#drivers/distance_sensor/sf1xx#drivers/distance_sensor/srf02#drivers/distance_sensor/srf02_i2c#drivers/distance_sensor/teraranger#drivers/distance_sensor/tfmini ​]]></content>
      <categories>
        <category>UAV</category>
      </categories>
      <tags>
        <tag>Pixhawk</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ROS和 MAVROS对Pixhawk进行外部控制]]></title>
    <url>%2Farticle%2FUsingROSAndMAVROSOffboardControlPixhawk.html</url>
    <content type="text"><![CDATA[概述PIxhawk（中文译为PX4）是一个软、硬件开源项目（遵守BSD协议），目的在于为学术、爱好和工业团体提供一款低成本高性能的高端的自驾仪。这个项目源于 ETH Zurich (苏黎世联邦理工大学)的计算机视觉与几何实验室的PIXHAWK项目、并得到了自主系统实验室和 自动控制实验室的支持 ，以及一些出色的个人(Contact and Credits)也参与其中，包括 3D Robotics 和 international 3DR distributors的成员。 协作计算机（Companion computer）可以与Pixhawk飞控相连，读取飞控中的状态信息，同时对飞控发送指令。并且可以作为一个更高性能的计算平台，可以运行图像识别、机器学习、实时路径规划等程序，为开源无人机带来了无限可能。 本文将介绍如何使用ROS控制无人机和协作计算机（Companion computer），以便轻松地连接电脑。PX4网站上有原始的教程，但是逻辑上有点混乱，以下是我查阅了很多资料做出的一个归档和总结，以及自己的一下思路。 什么是ROS？ROS（Robot Operating System，简称“ROS”）是一个适用于机器人的开源的元操作系统。 ROS 的主要目标是为机器人研究和开发提供代码复用的支持。它提供类似操作系统所提供的功能，包含硬件抽象描述、底层驱动程序管理、共用功能的执行、程序间的消息传递、程序发行包管理，它也提供一些工具程序和库用于获取、建立、编写和运行多机整合的程序。 学习ROS相关链接：ROS探索总结 什么是 MAVROS？MAVROS是ROS的一个软件包，允许在运行ROS的计算机、支持MAVLink的飞控板以及支持MAVLink的地面站之间通讯。 MAVlink由17个字节组成，包括消息ID、目标ID和数据。其中消息ID显示是什么数据，可以在messageID命令集中看到消息ID。 这使得MAVLink能够从同一个通道传输信息，从多个无人机获取信息。另外，消息也可以通过无线信号进行传输。]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>MAVROS</tag>
        <tag>Offboard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jetson开发板刷机及部署教程]]></title>
    <url>%2Farticle%2FJetsonPlatformWithAI.html</url>
    <content type="text"><![CDATA[NVIDIA® Jetson™ 系统所提供的性能和能效可提高自主机器软件的运行速度，而且功耗更低。每个系统都是一个完备的模块化系统 (SOM)，具备 CPU、GPU、PMIC、DRAM 和闪存，可节省开发时间和资金。Jetson 还具备可扩展性。只需选择适合应用场合的 SOM，即能够以此为基础构建自定义系统，满足特定的应用需求。 前期准备 JetPack刷机需要以下设备： Ubuntu主机：因为刷机是通过Ubuntu主机安装 JetPack 来给Jetson开发板安装开发套餐，所以需要一台装有Ubuntu的主机。 网线：用于给Jetson开发板联网。 键盘，键鼠：用于操控Jetson开发板。 显示器： 建议参考官方刷机教程：Download and Install JetPack L4T 下载 JetPack在 NVIDIA 官网 下载最新的JetPack文件，文件名大致为 JetPack-${VERSION}.run ，其中${VERSION}为版本号。 安装 JetPack 为 JetPack-${VERSION}.run 增加执行权限 1chmod +x JetPack-$&#123;VERSION&#125;.run 执行 JetPack-${VERSION}.run.run ，点击 Next 1sudo ./JetPack-$&#123;VERSION&#125;.run 此处选择 Jetson TX1，同样也支持 TK 1的部署 弹出 JetPack L4T Component Manager 对话框 推荐进行标准Standard安装的开发包， Ubuntu主机与Jetson开发板从机建立连接 之后输入用户名和密码，默认都是Ubuntu。IP的话如果不填，之后程序会自动计算出，请确保此时你有一根网线和你的宿主机相连，或者他们两者连接在同一个路由器上。 网络配置和端口选择 安装完成后进入网络配置选项，可以选择通过路由器连接互联网，同时将Jetson开发板连接到与主机同一个路由器上。 如果通过路由器连接，选择eth0端口，点击Next 如果通过WIFI连接，分别选择eth0和eth2端口，点击Next 将Jetson开发板设置为 Force USB Recovery Mode (强制USB恢复模式) 弹出如图所示终端窗口： 按照提示，将Jetson开发板设置为强制USB恢复模式 关闭设备，移除电源适配器 用自带的Micro USB 转 USB 数据线连接主机与Jetson开发板 连接电源适配器 按下POWER按钮 按住FORCE RECOVERY 按钮不放开，同时按一下RESET按钮，等待两秒钟，松开FORCE RECOVERY 按钮 完成上述步骤后，可以在主机上通过 lsusb命令查看是否含有Nvidia Corp设备，确认存在后敲击Enter，开始将操作系统以及开发包拷贝到Jetson开发板，中途连接Jetson开发板的显示器会显示Ubuntu桌面系统 待安装部署完成后，在主机上退出 JetPack即可，以上就完成了 JetPack的配置 运行示例我们可以在Jetson开发板上运行几个示例，验证下 CUDA是否安装成功 1/home/ubuntu/NVIDIA_CUDA-&lt;version&gt;_Samples/bin/armv7l/linux/release/gnueabihf/ 运行标检测Demo重启Jetson开发板，键盘、鼠标、显示器。 一般刷机之后，自带CUDA9.0和OPENCV3.3.1，并且已经装过CUDNN 1234# 进入backend目录cd home/nvidia/tegra_multimedia_api/samples/backend# 运行nvidia@tegra-ubuntu:~/tegra_multimedia_api/samples/backend$ ./backend 1 ../../data/Video/sample_outdoor_car_1080p_10fps.h264 H264 --trt-deployfile ../../data/Model/GoogleNet_one_class/GoogleNet_modified_oneClass_halfHD.prototxt --trt-modelfile ../../data/Model/GoogleNet_one_class/GoogleNet_modified_oneClass_halfHD.caffemodel --trt-forcefp32 0 --trt-proc-interval 1 -fps 10 后续则可以安装Python、TensorFlow及PyTorch等网络框架。 注：在JetPack开发板上，无论是执行编译还是执行代码，都必须加上sudo，否则会出现编译失败或者cuda error的情况。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Caffe</tag>
        <tag>深度学习</tag>
        <tag>Jetson TK1</tag>
        <tag>Jetson TX1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV 3.3 DNN模块的认识]]></title>
    <url>%2Farticle%2FOpenCVWithDNN.html</url>
    <content type="text"><![CDATA[在OpenCV3.3版本发布中把DNN模块从扩展模块移到了OpenCV正式发布模块中，当前DNN模块最早来自Tiny-dnn，可以加载预先训练好的Caffe模型数据，OpenCV做了近一步扩展支持所有主流的深度学习框架训练生成与导出模型数据加载。 按照官方介绍，DNN现在有下面几点特性： 无需任何依赖新加入的DNN模块不需要任何依赖，除了protobuf……而protobuf被加入到OpenCV的thirdparty了。简直是贴心至极有没有？ 支持以下框架 Caffe 1 TensorFlow Torch/PyTorch 虽然还没有支持caffe2，不过我现在就已经很满足了。 支持很多种类的层 AbsVal AveragePooling BatchNormalization Concatenation Convolution (including dilated convolution) Crop Deconvolution, a.k.a. transposed convolution or full convolution DetectionOutput (SSD-specific layer) Dropout Eltwise (+, *, max) Flatten FullyConnected …… 还有很多，就不一一列举了， 估计绝大部分人也用不上。。。 以下网络经过了测试且可用 AlexNet GoogLeNet v1 (also referred to as Inception-5h) ResNet-34/50/… SqueezeNet v1.1 VGG-based FCN (semantical segmentation network) ENet (lightweight semantical segmentation network) VGG-based SSD (object detection network) MobileNet-based SSD (light-weight object detection network) 但是现在OpenCV貌似只能加载训练好的网络，caffe的，TF的，Torch的，只能训练好之后拿来用，但是不能自己训练网络。 现在看来加入DNN模块算是众望所归，虽然有点晚，虽然功能还不够完善，但是仍然值得期待。 以GoogleNet Caffe模型举例说明OpenCV通过支持加载这些预先训练好的模型，实现图像分类、对象检测、语义分割、风格迁移等功能。支持Android/iOS等移动端平台开发。下面我们就以OpenCV3.3 使用Caffe的GoogleNet数据模型为例，实现对图像常见分类，OpenCV3.3的DNN模块使用的模型支持1000种常见图像分类、googlenet深度学习网络模型是2014图像分类比赛的冠军、首先是下载相关的数据模型文件 bvlc_googlenet.caffemodel bvlc_googlenet.prototxt 其中：prototxt定义了网络结构，caffemodel是训练后的权重。 文本文件只有你下载了OpenCV3.3解压缩之后就会在对应的目录发现。模型文件需要从以下地址下载即可： http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>Computer Vision</tag>
        <tag>OpenCV</tag>
        <tag>DNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量网页图片下载器 NeoDownloader]]></title>
    <url>%2Farticle%2FNeoDownloader.html</url>
    <content type="text"><![CDATA[NeoDownloader功能介绍：它还可以即时预览图片、幻灯显示、桌布设置…… 等功能，配合 Express Thumbnail Creator 软件更是可以快捷地制作网络相簿！推荐使用！NeoDownloader可以批量从任何网站下载任何文件，它主要是为了帮助您自动下载和查看您最喜爱的图片，照片，壁纸，视频，MP3音乐，和任何其他文件。 NeoDownloader配备了一个大型的在线数据库，随时可以下载的项目：数以千计的各种壁纸，高品质的照片，名人和美丽的女孩，由著名艺术家和摄影师，有趣的图片和GIF动画的作品。 下载地址：链接：https://pan.baidu.com/s/1jIH334U 密码：0z64 NeoDownloader怎么用？ 新建方案： 设置下载图片保存目录地址，以及勾上复制网站的文件夹结构！ 开始方案： 文件规律重命名下载地址：链接：https://pan.baidu.com/s/1gfs7hLL 密码：lnhp]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>爬图</tag>
        <tag>NeoDownloader</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测方法汇总]]></title>
    <url>%2Farticle%2FTargetDetectionAlgorithm.html</url>
    <content type="text"><![CDATA[目标检测方法分类第一，已知目标的先验知识。 在这种情况下检测目标有两类方法，第一类方法是用目标的先验知识训练一堆弱分类器，然后这些弱分类器一起投票来检测目标，如Boosting, Random forest 都是这个思路，大家熟知的Adaboost人脸检测也是如此。第二类方法是根据先验知识找到目标和非目标的最佳划分线，如SVM.这两类方法各成一家，各有所长，都有着不错的表现。 第二，未知目标的先验知识。 此时不知道要检测的目标是什么，于是什么是目标就有了不同的定义。一种方法是检测场景中的显著目标，如通过一些特征表达出场景中每个像素的显著性概率，然后找到显著目标。另一种方法就是检测场景当中的运动目标了。 经典目标检测方法1、背景差分法 在检测运动目标时，如果背景是静止的，利用当前图像与预存的背景图像作差分，再利用阈值来检测运动区域的一种动态目标识别技术。 背景差分算法适用于背景已知的情况，但难点是如何自动获得长久的静态背景模型。 Matlab中单纯的背景差分直接是函数imabsdiff（X,Y）就可以。 2、帧差分法 利用视频序列中连续的两帧或几帧图像的差来进行目标检测和提取。在运动的检测过程中，该方法利用时间信息，通过比较图像中若干连续帧获得对应像素点的灰度差值，如果均大于一定的阈值T2，则可以判断该位置存在运动的目标。适合于动态变化场景。 3、光流场法 利用相邻两帧中对应像素的灰度保持原理来评估二维图像的变化。能够较好的从背景中检测到相关前景目标，甚至是运动屋里中的部分运动目标，适用于摄像机运动过程中相对运动目标的检测。 开口问题、光流场约束方程的解的不唯一性问题，不能正确的表示实际的运动场。 例子如下： 首先，在一帧图像内随机均匀选取k个点，并滤除那些邻域纹理太光滑的点，因为这些点不利于计算光流。 然后，计算这些点与上一帧图像的光流矢量，如上右图，此时已经可以看出背景运动的大概方向了。 接下来的这一步方法因人而异了。 2007年CVPR的一篇文章Detection and segmentation of moving objects in highly dynamic scenes的方法是把这些光流点的（x, y, dx, dy, Y, U, V)7个特征通过Meanshift聚类来聚合到一起，最后形成运动目标轮廓。 新目标检测方法其实写到这里想了想到底能不能叫目标检测，博主认为图像的前背景分离也是目标检测的一种（博主才疏学浅，求赐教）。 1、像素点操作 对每个像素点进行操作，判别为前景或者背景两类。如下面的图片所示： 2、低秩矩阵应用 背景建模是从拍摄的视频中分离出背景和前景。下面的例子就是将背景与前景分离开。使用的方法是RPCA的方法。 其网址以及效果如下： http://perception.csl.illinois.edu/matrix-rank/introduction.html 3、深度学习 FCN + DenseCRF精确分割+语义标签。图像中的前景目标检测分割做的很好，下面还能做出语义检测，判断出图中的东西属于什么。ICCV 2015 paper ：Conditional Random Fields as Recurrent Neural Networks 测试网址以及测试图像如下： http://www.robots.ox.ac.uk/~szheng/crfasrnndemo]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>Computer Vision</tag>
        <tag>Object</tag>
        <tag>Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04安装L2TP Client]]></title>
    <url>%2Farticle%2FUnderUbuntuInstallL2TP.html</url>
    <content type="text"><![CDATA[添加源1sudo add-apt-repository ppa:nm-l2tp/network-manager-l2tp Update1sudo apt update 安装network-manager-l2tp1sudo apt install network-manager-l2tp 安装UI1sudo apt install network-manager-l2tp-gnome]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>VPN</tag>
        <tag>L2TP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LabelImg使用教程]]></title>
    <url>%2Farticle%2FlabelImgUsing.html</url>
    <content type="text"><![CDATA[Steps 首先新建一个存放标定结果xml的目标文件夹（命名不要有中文）。 点击 file，选择 Change default saved annotation folder，选择存放xml文件的目标文件夹。 点击 Open Dir，打开图像数据目录（不要有中文） 点击 Create RectBox，光标置于目标左上角，按住左键向右下角拖动。方框绘制完成后，双击选择弹窗中相应的类别名称。 标注完成后，光标放至方框内，该方框可移动位置以及右键删除. 标注完成后，点击左边栏的 save进行保存（务必保存） Hotkeys Ctrl + u Load all of the images from a directory Ctrl + r Change the default annotation target dir Ctrl + s Save Ctrl + d Copy the current label and rect box Space Flag the current image as verified w Create a rect box d Next image a Previous image del Delete the selected rect box Ctrl++ Zoom in Ctrl— Zoom out ↑→↓← Keyboard arrows to move selected rect box]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
        <tag>LabelImg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LabelImg介绍与安装教程]]></title>
    <url>%2Farticle%2FLabelImgInstall.html</url>
    <content type="text"><![CDATA[介绍对图片标注进而创建自己的数据集，以便在Deep neural networks中进行训练。 本文推荐一款十分好用的图片标注工具LabelImg，重点介绍其安装以及使用的过程。 在此感谢原作者在Github所做的贡献，博主发现软件一直在更新，各位小伙伴可以关注其最新版本。这款工具是全图形界面，用Python和Qt写的，最厉害的是其标注信息可以直接转化成为XML文件，与PASCAL VOC以及ImageNet用的XML是一样的。 PS.作者在5月更新了代码，现在最新的版本号是1.3.0，博主亲测，源代码在Windows 10和Ubuntu 16.04上运行正常。 Ubuntu 源码安装由于Ubuntu系统自带python，这款软件在Ubuntu环境下的安装是最方便的。软件要求python版本在2.6以上，同时需要PyQt和lxml的支持。 12345678910sudo apt-get install pyqt4-dev-tools # 安装PyQt4sudo pip install lxml # 安装lxml，如果报错，可以试试下面语句sudo apt-get install python-lxmlgit clone https://github.com/tzutalin/labelImg.gitcd labelImgmake all./labelImg.py # 或者 python labelImg.py# 作者新加的命令模式（应该只适用于最新github代码）./labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE] Ubuntu pip安装作者最近（2017.05）增添的一种安装方式，使用Python的pip工具安装，感觉更方便了。 123pip install labelImglabelImglabelImg [IMAGE_PATH] [PRE-DEFINED CLASS FILE] Windows平台安装 安装Anaconda Anaconda是一款十分好用的python集成安装环境，主要是方便扩展包管理。 具体介绍可参考这篇博客 。首先从官网下载最新的版本，这里我下载的是64位的python2.7版本。然后双击安装，一般点选“for all person”，然后安装到系统默认位置即可。安装成功后建议在环境变量path里面增加一条“C:\Program Files\Anaconda2” 安装PyQt 打开命令行窗口，输入conda list ，会列出所有预装的python扩展包，可以看到里面已经有了lxml，但是缺少PyQt。命令行输入conda install pyqt=4 ,等待一会即可完成PyQt4的安装。 下载源码并运行 在原作者的github下载源码压缩包，解压可得到名为labelImg-master的文件夹，进入该文件夹，在空白处使用“Shift+鼠标右键”，进入当前目录的命令行窗口，依次输入下面语句即可打开软件。 123pyrcc4 -o resources.py resources.qrcpython labelImg.pypython labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
        <tag>LabelImg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉学习笔记]]></title>
    <url>%2Farticle%2FComputerVisionStudyNotes.html</url>
    <content type="text"><![CDATA[概念计算机视觉（Computer Vision）就是指用摄影机和计算机代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图像处理，用计算机处理成更适合人眼观察或进行仪器检测的图像。 图1 计算机视觉介绍 图2 计算机视觉、机器视觉和图像处理之间的关系和区别]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大疆Maniflod妙算 -个人理解]]></title>
    <url>%2Farticle%2FDJIManiflodUsing.html</url>
    <content type="text"><![CDATA[妙算的实质官方宣称：大疆直连计算平台个人理解：英伟达TK1+自定义接口+大疆的SDK 注意虽然长得像个微型电脑，但妙算内部的CPU、GPU是嵌入式（NVIDIA Jetson TK1）的，所以默认搭载的也是嵌入式Ubuntu系统（大疆可能在这个基础上做了一些优化）一般我们装Ubuntu后会因为apt-get的下载源网速比较慢而换成国内的源（阿里之类的），千万要记得别选成i386的安装源（那是给Intel芯片的机子用的） 安装后的系统默认是全英文的 OpenCV妙算支持两种（还是三种？我记不清了）OpenCV 桌面系统的OpenCV（2，3应该都可以） 针对Jetson TK1专门优化过的OpenCV（版本是2.x.x，不支持3以上的版本，大疆官方手册里有讲具体的安装办法） 安装教程具体过程请看官方教程 重装系统（制作和恢复镜像）需要连接到装有Linux系统的电脑，刷入系统具体过程请看官方教程 最后如果你不是做嵌入式开发的，接触妙算后会不熟悉，特别是你把机子搞到要重装时会很无语，不过也放宽心态，哈哈哈… 详细过程后续文章中继续更新……]]></content>
      <categories>
        <category>UAV</category>
      </categories>
      <tags>
        <tag>UAV</tag>
        <tag>无人机</tag>
        <tag>大疆</tag>
        <tag>妙算</tag>
        <tag>Maniflod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大疆Maniflod妙算详解]]></title>
    <url>%2Farticle%2FDJIManiflodIntroduce.html</url>
    <content type="text"><![CDATA[基本配置Manifold 妙算采用NVIDIA Tegra K1处理器，它由NVIDIA 4-加-1 四核ARM - Cortex A15 CPU和包含192个NVIDIA CUDA 核心的 NVIDIA Kepler GPU组成，最高主频达2.2GHz。GPU核心不仅能实现强大的图像处理能力，还能高效地处理并行任务，浮点运算能力达326GFLOPS。 图像处理功能Manifold 妙算 拥有PC独立显卡级别的绘图能力，支持DirectX 11、OpenGL 4.4，可让机器人实时进行复杂的图像处理。Manifold还支持NVIDIA CUDA，用于开发最前沿的 GPU 加速应用，可将程序性能提升数倍。Manifold能广泛应用于计算机视觉、深度学习等人工智能领域，让你的设备具备环境感知、物体辨识和实时反应能力。 开发环境Manifold 妙算搭载了Ubuntu操作系统，可便捷安装运行Linux软件，支持CUDA、OpenCV及ROS等。全面支持DJI Onboard SDK，可轻松获取飞行数据，并进行控制和数据分析。 接口Manifold 妙算具备USB、Ethernet、Mini-PCIe、HDMI、UART、SPI和I2C等一系列接口，可连接传感器、显示屏等多种扩展设备。Manifold还能通过自定义接口与经纬M100 、M600连接，实时接收并处理所搭载云台相机的图像信息。 功耗其峰值功耗约15W，仅为普通笔记本电脑的四分之一，配备的Tegra K1 CPU共有4个A15核心，可从容应对复杂的计算任务， 1个附属核心负责简单任务的处理。其中，4个A15核心中的每一个核心均可根据工作负荷的繁重程度，独立而自动地启用和关闭，从而降低整体功耗。]]></content>
      <categories>
        <category>UAV</category>
      </categories>
      <tags>
        <tag>UAV</tag>
        <tag>无人机</tag>
        <tag>大疆</tag>
        <tag>妙算</tag>
        <tag>Maniflod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习与深度学习资料推荐]]></title>
    <url>%2Farticle%2FDeepLearningMaterials.html</url>
    <content type="text"><![CDATA[持续更新中... 深度学习圣经级教科书 摆书房，信仰用 公开课Machine learning： Andrew Ng (吴恩达): 斯坦福大学CS229 《 Mchine learning》推荐 Andrew Ng (吴恩达): Coursera2012推荐 Hung-yi Lee: 台湾大学 Machine Learning (2017,Fall)推荐 Geoffrey Hinton: Coursera 《Neural Networks For Machine Learning》 Yaser Abu-Mostafa: 加州理工学院 《Learnning from data》 Deep learning： Andrew Ng (吴恩达): DeeplearniNg.AI Deep Learning推荐 Fei-Fei Li: 斯坦福大学CS231n 《Convolutional Neural Networks for Visual Recognition》推荐 Chris Manning: 斯坦福大学CS224n 《Natural Language Processing with Deep Learning》 Google: Udacity 《Deep Learning by Google》 Reinforcement Learning： Hung-yi Lee: 台湾大学 深度强化学习推荐 Emma Brunskill: 斯坦福大学CS234 《Reinforcement Learning Winter 2019》 教材 Yoshua Bengio. 《Deep Learning》 Gene H. Golub. 《Matrix Computations》 李航. 《统计学习方法》 周志华. 《机器学习》 Li Y. Deep Reinforcement Learning: An Overview[J]. 2017. 郑哲宇. 《TensorFlow 实战Google深度学习框架 (第2版)》 诸葛越. 《百面机器学习》 深度学习框架 Tensorflow初学者篇：https://www.tensorflow.org/get_started/mnist/beginners进阶篇：https://www.tensorflow.org/get_started/mnist/pros PyTorch参考：https://pytorch.apachecn.org/docs/1.0/#/ Caffe参考：http://www.cnblogs.com/denny402/tag/caffe/default.html?page=2 Keras参考：http://keras-cn.readthedocs.io/en/latest/getting_started/sequential_model/]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster-RCNN的搭建和测试]]></title>
    <url>%2Farticle%2FFasterRCNNBulidAndTest.html</url>
    <content type="text"><![CDATA[下载Faster-RCNN12cd ~git clone --recursive https://github.com/rbgirshick/py-faster-rcnn.git 安装Python依赖包1234sudo apt-get install python-pipsudo pip install cythonsudo apt-get install python-opencvsudo pip install easydic 生成Cython模块12cd ~/py-faster-rcnn/lib/make 安装Atlas1sudo apt-get install libatlas-base-dev 生成Caffe和pycaffe12cd ~/py-faster-rcnn/caffe-fast-rcnncp Makefile.config.example Makefile.config 编辑Makefile.config，根据自己的需要修改相应模式，其中重点关注CPU_ONLY 和WITH_PYTHON_LAYER以及CUDNN等处，若是在CPU的情况下，请务必修改为CPU_ONLY ：= 1 编译环境12345678cd ~/py-faster-rcnn/caffe-fast-rcnnmkdir buildcd buildcmake ..make all -j16（"‐j16"是使用 CPU 的多核进行编译,可以极大地加速编译的速度）make installmake runtest -j16（如果出错没有关系，直接进行下一步）make pycaffe（编译pycaffe） 下载fetch_fast_rcnn_models下载网址：（链接：http://pan.baidu.com/s/1pJVburD 密码：11m0） 将下载好的faster_rcnn_models.tgz放到/py-faster-rcnn/data这个目录下，右键提取到此处，即是解压。 CPU环境下所需要的操作 1vim ~/py-faster-rcnn/lib/fast_rcnn/config.py 将 __C.USE_GPU_NMS = False 1vim ~/py-faster-rcnn/tools/test_net.py 注释掉：caffe.set_mode_gpu() ，其下方添加： caffe.set_mode_cpu() 1vim ~/py-faster-rcnn/tools/train_net.py 注释掉：caffe.set_mode_gpu() ，其下方添加： caffe.set_mode_cpu() 1vim ~/py-faster-rcnn/lib/setup.py 注释掉GPU相关代码，如下 1#CUDA = locate_cuda() 1234567891011121314151617# Extension('nms.gpu_nms',# ['nms/nms_kernel.cu', 'nms/gpu_nms.pyx'],# library_dirs=[CUDA['lib64']],# libraries=['cudart'],# language='c++',# runtime_library_dirs=[CUDA['lib64']],# # this syntax is specific to this build system# # we're only going to use certain compiler args with nvcc and not with# # gcc the implementation of this trick is in customize_compiler() below# extra_compile_args=&#123;'gcc': ["-Wno-unused-function"],# 'nvcc': ['-arch=sm_35',# '--ptxas-options=-v',# '-c',# '--compiler-options',# "'-fPIC'"]&#125;,# include_dirs = [numpy_include, CUDA['include']]# ), 1vim ~/py-faster-rcnn/lib/fast_rcnn/nms_wrapper.py 注释掉 #from nms.gpu_nms import gpu_nms 运行Demo1234cd ~/py-faster-rcnn/tools./demo.py 或python demo.py #当此处报错时请尝试python2 demo.py#CPU环境下运行话：python2 demo.py --cpu 若提示缺少某个模块(module)，网上搜索下，安装给出的教程安装相应模块，建议尽量终端中命令安装． 效果图如下(仅贴了一张图)：]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Caffe</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast-RCNN的搭建和测试]]></title>
    <url>%2Farticle%2FFastRCNNBulidAndTest.html</url>
    <content type="text"><![CDATA[下载Fast-RCNN12cd ~git clone --recursive https://github.com/rbgirshick/fast-rcnn.git 安装Python依赖包1234sudo apt-get install python-pipsudo pip install cythonsudo apt-get install python-opencvsudo pip install easydic 生成Cython模块12cd ~/fast-rcnn/lib/make 安装Atlas1sudo apt-get install libatlas-base-dev 生成Caffe和pycaffe12cd ~/fast-rcnn/caffe-fast-rcncp Makefile.config.example Makefile.config 编辑Makefile.config，根据自己的需要修改相应模式，其中重点关注CPU_ONLY 和WITH_PYTHON_LAYER以及CUDNN等处，若是在CPU的情况下，请务必修改为CPU_ONLY ：= 1 编译环境12345678cd ~/fast-rcnn/caffe-fast-rcnnmkdir buildcd buildcmake ..make all -j16（"‐j16"是使用 CPU 的多核进行编译,可以极大地加速编译的速度）make installmake runtestmake pycaffe（编译pycaffe） 下载fetch_fast_rcnn_models下载网址：（链接：http://pan.baidu.com/s/1pJVburD 密码：11m0） 将下载好的fetch_fast_rcnn_models.tgz放到/fast-rcnn/data这个目录下，右键提取到此处，即是解压。 运行Demo1234cd ~/fast-rcnn/tools./demo.py 或python demo.py #当此处报错时请尝试python2 demo.py#CPU环境下运行话：python2 demo.py --cpu 若提示缺少某个模块(module)，网上搜索下，安装给出的教程安装相应模块，建议尽量终端中命令安装． 效果图如下(仅贴了一张图)：]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Caffe</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测试Caffe]]></title>
    <url>%2Farticle%2FTestCaffe.html</url>
    <content type="text"><![CDATA[下载Mnist123cd ~/caffesudo sh data/mnist/get_mnist.shsudo sh examples/mnist/create_mnist.sh ​ 有GPU，则不需要修改配置文件，如果没有GPU，则需要修改配置文件 lenet_solver.prototxt 1vim examples/mnist/lenet_solver.prototxt 将最后一行的solver_mode:GPU 改为 solver_mode:CPU 运行1sudo sh examples/mnist/train_lenet.sh 注意，运行caffe程序时，必须在caffe的根目录下，不然会出错 ​​]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Caffe</tag>
        <tag>深度学习</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Caffe]]></title>
    <url>%2Farticle%2FInstallCaffe.html</url>
    <content type="text"><![CDATA[下载Caffe直接使用Git下载caffe，如果没有Git，请先安装Git：1sudo apt-get install git ​ 安装Caffe： 1git clone git://github.com/BVLC/caffe.git 安装依赖包1234567891011sudo apt-get install libatlas-base-devsudo apt-get install libprotobuf-devsudo apt-get install libleveldb-devsudo apt-get install libsnappy-devsudo apt-get install libopencv-devsudo apt-get install libboost-all-devsudo apt-get install libhdf5-serial-devsudo apt-get install libgflags-devsudo apt-get install libgoogle-glog-devsudo apt-get install liblmdb-devsudo apt-get install protobuf-compiler 安装OpenCV（可选）123456cd caffesudo git clone https://github.com/jayrambhia/Install-OpenCVcd Install-OpenCV/Ubuntusudo sh dependencies.shcd 2.4sudo sh opencv2_4_10.sh 修改Caffe配置文件12cd ~/caffesudo cp Makefile.config.example Makefile.config 修改配置文件，本人使用vim，若果没有，请先安装： 1sudo apt-get install vim 打开文件： 1vim Makefile.config 修改如下： 12345678910111213141516171819202122232425262728//如果你不使用GPU的话，就将# CPU_ONLY := 1修改成：CPU_ONLY := 1//若使用cudnn，则将# USE_CUDNN := 1修改成：USE_CUDNN := 1//若使用的opencv版本是3的，则将# OPENCV_VERSION := 3修改为：OPENCV_VERSION := 3//若要使用python来编写layer，则需要将# WITH_PYTHON_LAYER := 1修改为WITH_PYTHON_LAYER := 1//重要的一项！！！将# Whatever else you find you need goes here.下面的INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includeLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib修改为：INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serialLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial//这是因为ubuntu16.04的文件包含位置发生了变化，尤其是需要用到的hdf5的位置，所以需要更改这一路径 编译123make all -j16（"‐j16"是使用 CPU 的多核进行编译,可以极大地加速编译的速度）make test -j16make runtest -j16]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Caffe</tag>
        <tag>深度学习</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记]]></title>
    <url>%2Farticle%2FLinuxStudyNote.html</url>
    <content type="text"><![CDATA[绝对路径： cd /home/python相对路径： cd Desktop . : 表示当前目录.. : 表示当前目录的上一层目录../.. : 表示当前目录的上上层目录没有…或以上 ls : 123456789101112ls #查看当前目录下的文件或文件夹ls /bin #查看根目录下bin文件夹下的东西ls Document #查看当前路径下Document文件夹中的东西ls -a #显示隐藏文件ls -l #以列详细表格式显示文件ls -l -h #以列详细表格式显示文件，文件大小以合适的单位进行显示 (ls -lh 或 ls -hl 效果同上）ls *.txt #显示以txt为后缀的文件ls *.t?t #显示txt或tnt等为后缀的文件ls *.t[xn]t #只显示txt或tnt为后缀的文件ls *.t[a-f]t #只显示tat -&gt; tft为后缀的文件，必须是连续的ls \*a #显示 *a 文件ls -alh | more #显示，并分页显示，利用了管道“|”命令进行暂时存储 cd : 1234cd Desktop #进入Desktop这个文件夹cd .. #跳转到当前目录的上一层目录cd - #跳转到上一次所在的路径(类似遥控器中的回看)cd ~ #跳转到当前用户的家目录 clear : 清屏 pwd : 显示路径 tab : 用来自动补全 touch : 创建一个文件，Linux中没有后缀的说法，可以起各种名字1touch a/b/xxx.txt #在a中的b文件夹中创建 xxx.txt文件 > #重定向 ，例如 ls &gt; test.txt 或 ls -alh &gt; test.txt >&gt; #重定向，不过不代替原数据，添加数据 more ：分屏显示cat ：全部显示 12cat helloworld.txt &gt; xyz.txt #起到到复制的功能cat a.txt b.txt &gt; xxx.txt #将a.txt和b.txt合并到xxx.txt，但是要注意前后顺序 mkdir : 创建文件夹 12mkdir a #在当前文件夹创建mkdir a/b/c -p #在a文件夹中创建b,并在b文件夹中创建c tree : 以目录树的方式显示 rmdir : 删除空文件夹 1例: rmdir test.txt rm ： 删除即无 1rm hello_python -r #删除文件夹 ln : 创建；链接 123ln -s 源文件 链接文件 #表示创建软链接ln 源文件 链接文件 #表示创建硬链接注：硬链接把源文件删除，链接文件仍然可以打开；软连接则不行 grep : 查找文件内容 1234其中 -n 代表行号； -i 忽略大小写；-v 代表取反，除了x，留下剩下的grep -n 'a' greptest.txt #查找任意位置的agrep -n '^a' greptest.txt #查找a开头grep -n 'a$' greptest.txt #查找a结尾 ls —help : 帮助man ls : 帮助 history : 历史命令 find : 查找文件 123find ./ -name '*.sh' #即在当前目录查找find / -name '*xyz*' #根目录下查找fine /tep -size -2M #在该目录下查找小于2M的文件, +2M :大于2M的文件 cp : 拷贝 12345-v 显示进度；-r 拷贝文件夹;-i 交互，让用户确认；-f 强制，禁止交互；cp 原文件名 复制的文件名cp *0 ./acp b a/ -r #即将文件夹b拷贝到文件夹a中cp b/* a/ -r #即将文件夹b中的内容拷贝到文件夹a中 mv : 移动、剪切 tar : 打包 1234tar -cvf test.tar * #即把当前所有文件打包tar -xvf test.tar #即把tar文件还原tar -zcvf test.tar.gz * #即把当前文件打包并压缩tar -zxvf test.tar.gz gzip : gzip压缩 123gzip test.tar #即压缩文件gzip -d test.tar #即解压缩流程：打包-&gt;压缩—&gt; -&gt;解压-&gt;解包 bz2 : bizp2压缩 12tar -jcvf test.tar.bz2 test.txt #即把当前文件打包并压缩tar -jcvf test.tar.bz2 -C test/ #即解压到test文件夹下 zip : zip压缩 123zip myzip * #即把当前文件夹中的文件压缩为myzipuzip -d ./ myzip.zip #即解压到当前文件夹中uzip -d ./aa myzip.zip #即创建aa,并解压到aa文件夹中 which : 显示命令的位置 1which ls #显示ls命令的位置 ifconfig : 查看ip地址，Windows中使用ipconfig 123ifconfig ens33 down #把网卡关闭ifconfig ens33 up #把网卡打开ifconfig ens33 178.12.12.23 #设置ip地址 ping : 测试网络链接是否正常 ssh : 远程登录 1ssh 用户@ip地址 who : 当前有几个用户正在使用 12who -q #哪些用户登录了电脑whoami #查看当前用户名 exit : 退出 useradd : 添加用户账号 12-m ：自动创建家目录文件夹； -d :指定家目录路径 ；-g : 指定用户组sudo useradd yonghuming -m -d /home/hahaha #创建用户 yonghuming control + a : 快速跳到行头control + e : 快速跳到行尾 passwd : 设置密码 1sudo yonghuming #设置yonghuming的密码 su : 切换用户 12su yonghuming #切换到用户 yonghumingsu - yonghuming #切换到用户 yonghuming ，并且跳转到yonghuming其家目录 userdel : 删除用户 12userdel abc(用户) #删除用户，不删除其主目录userdel -r abc(用户) #删除用户，并删除其主目录 sudo : 获取管理员权限 1sudo -s #把当前用户切换到root用户 group ：用户组 12345cat etc/group/ #查看用户组gropumod + 2次tab键groupadd xyz(用户) #创建用户组groupdel xyz(用户) #删除用户组groups xyz(用户) #查看用户所在的用户组 usermod : 修改用户所在的组 1234格式：usermod -g 用户组 用户名 #用户名默认的主组usermod -a -G XXX xyz(用户) #把用户xyz添加到其他组XXX，副组usermod -a -G adm xyz(用户)usermod -a -G sudo xyz(用户) #给用户xyz sudo的权限 chmod : 修改权限 12345678格式：chmod u/g/o/a +/-/= rwx test.txt其中 u = user,g = group, o = other, a = all; + 增加权限，- 减少权限，= 赋予权限chmod u=rwx,o=rwx test.py #赋予user、others可读可写可执行权限chomod a= test.py #清空权限数字对应法：r--&gt;4, w--&gt;2, x--&gt;1;chmod 761 test.py chmod 777 123(文件夹) #只会修改文件夹123的权限为777chmod 777 123(文件夹) -R #修改文件夹123及其内部所有文件的权限为777 chown : 修改所有者 chgrp : 修改组别 cal : 系统日历 ps : 查看进程设置 12psps -aux : top : 动态显示进程所在的内存的状态 kill ： 杀进程 1kill (PID) reboot : 重启 shutdown : 重启 1shutdown -h now #立即关机 df : 检查磁盘空间 du : 检测目录所占磁盘空间vi 3种模式： vim xxx.py +22 ：打开并跳转到第22行； 1、命令模式；i、o、a-&gt;插入模式；: -&gt;末行模式； i光标左侧输入，a光标右侧输入，o光标下侧输入，O上侧输入； I最左侧输入，A最右侧输入; hjkl : 控制左下上右；M : 中间位置；L : 屏幕最后一行； G : 跳到最后一行；15 G ：跳转到第15行，1 G : 跳到第一行；gg : 跳转到第一行； w : 向右跳一个Word；b : 向左跳一个Word； } ： 向下跳一段；{ ：向上调一段； ctr + d ：向下翻半屏；ctr + u : 向上翻半屏； ctr + f : 向下翻一屏；ctr + b : 向上翻一屏； 8yy : 表示从当前光标所在的行开始复制8行； p 粘贴； 2dd : 表示从当前光标所在的行开始剪切2行； D ：删除本行光标后所有内容；d0 : 删除本行光标前所有内容； dw : 删除光标开始位置的Word； x : 删除光标后面；X ： 删除光标前面； u 撤销；control + r : 反撤销； &gt;&gt; 文本行右移；&lt;&lt; 文本行左移； . 表示重复上一次操作的命令； v ：可视，选中一片区域； r : 替换，按 r 后，然后替换； / : 按 / 后，输入要搜索的，按 n 进行下一个查找 2、编辑模式；esc-&gt;命令模式; 3、末行模式；wq 保存退出；q! 即为强制退出； x 保存退出；shift + 2个z 保存退出 X 进行加密； 全部替换： :%s/被替换内容/替换成的内容/g 部分替换： :1,10s/被替换内容/替换成的内容/g 在末行模式下，不退出而进行其他操作： :!ls 回车 apt-get install packge : 安装包apt-get update : 刷新源]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu VIM+Terminal 配色方案]]></title>
    <url>%2Farticle%2FUbuntuVIMAndTerminalHarmonizeColors.html</url>
    <content type="text"><![CDATA[修改终端的配色：Solarized现在基本用 ubuntu 做开发，直接在终端(gnome-terminal)里面 vim，但配色效果不甚满意，因为 gvim 的配色是 Solarized，google告诉我 Gnome-Terminal 也可以这样配。可以先围观下效果图： 安装 git： 1sudo apt-get install git-core 下载Solarized Theme 1git clone git://github.com/seebi/dircolors-solarized.git dircolor-solarized 配色之一： dark256 12cp ~/dircolors-solarized/dircolors.256dark ~/.dircolorseval 'dircolors .dircolors' 设置 Terminal 支持 256 色 1vim .bashrc 添加： 1export TERM=xterm-256color 最后： 1source .bashrc 下载 Solarized 的 Gnome-Terminal 配色 1git clone git://github.com/Anthony25/gnome-terminal-colors-solarized.git 进入目录，配置运行配置脚本： 12cd gnome-terminal-colors-solarizedsudo sh set_dark.sh 或者：sudo sh set_light.sh 修改VIM配色只要把 solarized.vim 复制到 ~/.vim/colors/ 目录下就可以了 下载 Solarized 的 VIM 配色 1234git clone git://github.com/altercation/vim-colors-solarized.gitcd vim-colors-solarized/colorsmv solarized.vim ~/.vim/colors/#注：没有 .vim/ 或者 .vim/colors/ 使用 mkdir 在目录 ～/ 下创建 创建（修改）VIM配色文件 .vimrc 1vim .vimrc 添加： 123syntax enableset background=darkcolorscheme solarized]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>配色</tag>
        <tag>VIM</tag>
        <tag>Terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WIN10下安装Ubuntu 16.04（双硬盘：SDD+HDD）]]></title>
    <url>%2Farticle%2FUnderWIN10InstallUbuntuSDDAndHDD.html</url>
    <content type="text"><![CDATA[安装双系统时踩了不少坑，网上能查到的攻略和安装教程基本都看的差不多了，有些教程版本太老或解释得不很明确，特此记录。 数据备份 重要的事情说三遍，数据备份，数据备份，数据备份，数据丢失，本人概不负责。 磁盘分区（重要） 在磁盘A中（就是第一个系统的启动分区所在的磁盘），单独给出500M用于/boot分区（安装的时候把这500M free space作为/boot)，其他的/home 和/ 什么的装到磁盘B或者磁盘A上都可以（/home 和/可任意选择案安装位置）； 在磁盘B中，根据自己的需求进行分区； 安装推荐教程：Windows10+Ubuntu双系统安装进行安装 注意： 在选择分区类型时，可能会出现“安装系统时空闲分区不可用”状况，为了解决问题，所有添加的新分区类型一律选择“逻辑分区”； 选择/boot对应的盘符作为“安装启动引导器的设备”，务必保证一致；在安装过程中，为/boot分区时，一定注意添加到磁盘A中！！！]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>双系统</tag>
        <tag>双硬盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text Personalization]]></title>
    <url>%2Farticle%2FSublimeTextSetting.html</url>
    <content type="text"><![CDATA[Fonts：Source Code Pro Source Code Pro: A perfect programming font https://github.com/adobe/source-code-pro For Mac: Double-click the font (SourceCodeVariable.ttf) in the Finder, then click Install Font in the font preview window that opens. After your Mac validates the font and opens the Font Book app, the font is installed and available for use. For Linux: copy the fonts to /usr/share/fonts and run sudo fc-cache to rebuild the font cache For Win: copy the fonts to C:\Windows\Fonts and Double-click the font Color scheme：Solarized https://github.com/braver/Solarized The easiest way to install is using Sublime Package Control, where Solarized is listed as Solarized Color Scheme. Open Command Palette using menu item Tools -&gt; Command Palette... (⇧⌘P on Mac) Choose Package Control: Install Package Find Solarized Color Scheme and hit Enter How to ActivateActivate the UI theme and color scheme by modifying your user preferences file, which you can find using the menu item Sublime Text -&gt; Preferences -&gt; Settings - User (⌘, on Mac). Indentation123456// The number of spaces a tab is considered equal to "tab_size": 4, // Set to true to insert spaces when tab is pressed "translate_tabs_to_spaces": true, //Set automatically when saving "expand_tabs_on_save": true Mine123456789101112131415&#123; "color_scheme": "Packages/Solarized Color Scheme/Solarized (light).tmTheme", "font_face": "Source Code Pro", "font_size": 11, "ignored_packages": [ "Vintage" ], "theme": "Default.sublime-theme", "tab_size": 4, "translate_tabs_to_spaces": true, "expand_tabs_on_save": true&#125;]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Sublime Text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text快捷键]]></title>
    <url>%2Farticle%2FSunlimeTestHotKey.html</url>
    <content type="text"><![CDATA[常用Tab 自动补全 Ctrl + Z 撤销 Ctrl + Y 前进 Ｃtrl+[对齐 Ｃtrl+]对齐返回 Ctrl+/ 注释单行 Ctrl+Shift+/ 注释多行。 2空格转4空格的操作：View-&gt;Indentation-&gt;Convert Indentation to Tabs-&gt;Tab width 4-&gt;Convert Indentation to Spaces SublimeText自带格式化: Edit -&gt; Line -&gt; Reindent 选择类Ctrl+D 选中光标所占的文本，继续操作则会选中下一个相同的文本。 Alt+F3 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑。举个栗子：快速选中并更改所有相同的变量名、函数名等。 Ctrl+L 选中整行，继续操作则继续选择下一行，效果和 Shift+↓ 效果一样。 Ctrl+Shift+L 先选中多行，再按下快捷键，会在每行行尾插入光标，即可同时编辑这些行。 Ctrl+Shift+M 选择括号内的内容（继续选择父括号）。举个栗子：快速选中删除函数中的代码，重写函数体代码或重写括号内里的内容。 Ctrl+M 光标移动至括号内结束或开始的位置。 Ctrl+Enter 在下一行插入新行。举个栗子：即使光标不在行尾，也能快速向下插入一行。 Ctrl+Shift+Enter 在上一行插入新行。举个栗子：即使光标不在行首，也能快速向上插入一行。 Ctrl+Shift+[ 选中代码，按下快捷键，折叠代码。 Ctrl+Shift+] 选中代码，按下快捷键，展开代码。 Ctrl+K+0 展开所有折叠代码。 Ctrl+← 向左单位性地移动光标，快速移动光标。 Ctrl+→ 向右单位性地移动光标，快速移动光标。 shift+↑ 向上选中多行。 shift+↓ 向下选中多行。 Shift+← 向左选中文本。 Shift+→ 向右选中文本。 Ctrl+Shift+← 向左单位性地选中文本。 Ctrl+Shift+→ 向右单位性地选中文本。 Ctrl+Shift+↑ 将光标所在行和上一行代码互换（将光标所在行插入到上一行之前）。 Ctrl+Shift+↓ 将光标所在行和下一行代码互换（将光标所在行插入到下一行之后）。 Ctrl+Alt+↑ 向上添加多行光标，可同时编辑多行。 Ctrl+Alt+↓ 向下添加多行光标，可同时编辑多行。 编辑类Ctrl+J 合并选中的多行代码为一行。举个栗子：将多行格式的CSS属性合并为一行。 Ctrl+Shift+D 复制光标所在整行，插入到下一行。 Tab 向右缩进。 Shift+Tab 向左缩进。 Ctrl+K+K 从光标处开始删除代码至行尾。 Ctrl+Shift+K 删除整行。 Ctrl+/ 注释单行。 Ctrl+Shift+/ 注释多行。 Ctrl+K+U 转换大写。 Ctrl+K+L 转换小写。 Ctrl+Z 撤销。 Ctrl+Y 恢复撤销。 Ctrl+U 软撤销，感觉和 Gtrl+Z 一样。 Ctrl+F2 设置书签 Ctrl+T 左右字母互换。 F6 单词检测拼写 搜索类Ctrl+F 打开底部搜索框，查找关键字。 Ctrl+shift+F 在文件夹内查找，与普通编辑器不同的地方是sublime允许添加多个文件夹进行查找，略高端，未研究。 Ctrl+P 打开搜索框。举个栗子：1、输入当前项目中的文件名，快速搜索文件，2、输入@和关键字，查找文件中函数名，3、输入：和数字，跳转到文件中该行代码，4、输入#和关键字，查找变量名。 Ctrl+G 打开搜索框，自动带：，输入数字跳转到该行代码。举个栗子：在页面代码比较长的文件中快速定位。 Ctrl+R 打开搜索框，自动带@，输入关键字，查找文件中的函数名。举个栗子：在函数较多的页面快速查找某个函数。 Ctrl+： 打开搜索框，自动带#，输入关键字，查找文件中的变量名、属性名等。 Ctrl+Shift+P 打开命令框。场景栗子：打开命名框，输入关键字，调用sublime text或插件的功能，例如使用package安装插件。 Esc 退出光标多行选择，退出搜索框，命令框等。 显示类Ctrl+Tab 按文件浏览过的顺序，切换当前窗口的标签页。 Ctrl+PageDown 向左切换当前窗口的标签页。 Ctrl+PageUp 向右切换当前窗口的标签页。 Alt+Shift+1 窗口分屏，恢复默认1屏（非小键盘的数字） Alt+Shift+2 左右分屏-2列 Alt+Shift+3 左右分屏-3列 Alt+Shift+4 左右分屏-4列 Alt+Shift+5 等分4屏 Alt+Shift+8 垂直分屏-2屏 Alt+Shift+9 垂直分屏-3屏 Ctrl+K+B 开启/关闭侧边栏。 F11 全屏模式 Shift+F11 免打扰模式]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Sublime Text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Debug of " /usr/bin/env, 'python\r', No such file or directory "]]></title>
    <url>%2Farticle%2FPythonBugs01.html</url>
    <content type="text"><![CDATA[在Win下使用Sublime Text编写的Python 转移到Linux下运行报错： 1/usr/bin/env: 'python\r': No such file or directory 原因是：linux下的文本文件以ascii码为10的字符\n表示换行，而windows下则使用连续的13和10两个字符\r\n表示换行。 注：在python和c语言里使用\r代表ascii符为13的字符，叫做回车符，而\n代表ascii码为10的字符，叫做换行符。 在Win下编写的脚本中#!/usr/bin/env python在Linux下会被认为含有CR（carriage return ）字符!/usr/bin/env python\r 最简洁解决方法：使用 vim 打开脚本，命令模式下输入： :set ff=unix 回车 :wq 即可解决问题！ PS：如果之前没有安装过vim的话，使用命令： 1sudo apt-get install vim]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python--目录间.py文件调用]]></title>
    <url>%2Farticle%2FPythonNotes05.html</url>
    <content type="text"><![CDATA[这里对Python各种目录之间的.py文件调用做一个归纳。 123456789.└── folder ├── data │ └── data.txt | └── abc.py └── test | └── a.py | └── b.py └── hello.py 123# a.pydef show(): print("this is a.py") 12345678910# b.pydef add(x,y): print("sum = "%(x+y)) class B: def __init__(self, xx, yy): self.x = xx self.y = yy def plus(self): print("sum = "%(self.x + self.y)) 123# hello.pydef showHello(): print("this is hello.py") 主目录调用子目录.py文件譬如：hello.py调用b.py，需要test目录下创建__init__.py文件（该文件可以什么都不写） 1234567# 方式一：import test.btest.b.add(2,3)# 方式二：from test.b import addadd(2,3) 同目录调用.py文件譬如： a.py调用b.py 调用函数： 1234567# 方式一：import bb.add(2,3)# 方式二：from b import addadd(2,3) 调用类： 123456789# 方式一：import bxyz = b.B(2,3)xyz.plus()# 方式二：from b import B; xyz = B(2,3)xyz.plus() 跨目录读取.py文件abc.py调用hello.py： 1234567891011# 方式一：import syssys.path.append("..")import hellohello.showHello()# 方式二：import syssys.path.append("..")from hello import showHelloshowHello() abc.py调用a.py： 1234567891011# 方式一：import syssys.path.append("../test")import aa.show()# 方式二：import syssys.path.append("../test")from a import showshow()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python--获取当前或上级目录路径]]></title>
    <url>%2Farticle%2FPythonNotes06.html</url>
    <content type="text"><![CDATA[Python脚本中常常需要目录间文件的互相调用，这里分别对获取当前目录路径和获取上级目录路径常用方法进行总结。 获取当前目录路径123456789# 方式一import osprint(os.getcwd())# 方式二import sysprint(sys.path[0])# 方式三import osprint(os.path.abspath(os.path.dirname(__file__))) 获取上级目录路径123456789101112方式一：(推荐)import osprint(os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir)))方式二：（推荐）import sys, ospwd = sys.path[0] # 获取当前执行脚本的位置print(os.path.abspath(os.path.join(pwd, os.pardir)))方式三、四、五：import osprint(os.path.abspath(os.path.dirname(os.getcwd())))print(os.path.abspath(os.path.join(os.getcwd(), "..")))print(os.path.abspath(os.path.dirname(os.path.abspath(os.path.dirname(__file__)))) #不推荐，太丑 获取上上级目录路径123456方式一：(推荐)import osprint(os.path.abspath(os.path.join(os.getcwd(), "../..")))方式二：（推荐）import osprint(os.path.abspath(os.path.join(os.getcwd(), '..', '..'))) 获取上上上级目录路径123方式一：(推荐)import osprint(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..'))) PS 解释： 12345__file__：当前文件路径os.path.dirname(file): 某个文件所在的目录路径os.path.join(a, b, c,....): 路径构造 a/b/cos.path.abspath(path): 将path从相对路径转成绝对路径os.pardir: Linux下相当于"../"]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无人机开发资料汇总]]></title>
    <url>%2Farticle%2FUAVDevelopMaterials.html</url>
    <content type="text"><![CDATA[国内外知名论坛 无人机开源基金会(Dronecode) https://www.dronecode.org APM/PIX研发没有不知道这个的PX4官网 https://pixhawk.org/choice APM官方网站(美国3DR公司) http://ardupilot.com/ 国外DIY无人机专业论坛(DIY DRONES) http://diydrones.com/ 开源Linux自动驾驶仪(Erle-BRAIN)官网 http://erlerobotics.com/blog/home-creative/ 爱数码(LoveDigt)社区 专业数码技术交流社区 http://www.lovedigit.com APM/PIX相关调试软件： MISSION PLANNER 最新地面站下载地址 http://firmware.ap.ardupilot.org/Tools/MissionPlanner/ MISSION PLANNER 历史版本百度网盘下载地址 http://pan.baidu.com/s/1ntNUIox#path=%252FMission%2520Planner-APM%25E4%25B8%25AD%25E6%2596%2587%25E7%25BD%2591 QGROUNDCONTROL(QT版地面站下载地址，支持linux,Windows, Mac OS) http://www.qgroundcontrol.org APM安卓地面站下载1地址 http://diydrones.com/profiles/blogs/droidplanner-ground-control-station-for-android-devices (这个版本比较好用，但是需要安卓手机安装谷歌server,也可以寻找国内团队的改写版) APM安卓地面站下载2地址 http://diydrones.com/group/andropilot-users-group andropilot也是APM安卓地面站，可以下载参考。 国内爱好者改编的APM安卓地面站，比较适合国内网络受限的条件：http://fir.im/fishDroneGCS 飞控开发人员相关必备网站 APM自动驾驶仪源码 https://github.com/diydrones/ardupilot MISSION PLANNER 源码下载地址 https://github.com/diydrones/MissionPlanner PXHAWK原生固件源码下载地址 https://github.com/PX4/Firmware PXHAWK ardupilot固件下载地址：https://github.com/ArduPilot/ardupilot GQC 地面站源码下载地址 ：https://github.com/mavlink/qgroundcontrol PIXHAWK原生固件中文翻译开发指导手册：https://px4.osdrone.net/ PIXHAWK ardupilot固件编译指导手册：http://ardupilot.org/dev/docs/building-px4-for-linux-with-make.html#building-px4-for-linux-with-make PIXHAWK光流资料手册：https://dev.px4.io]]></content>
      <categories>
        <category>UAV</category>
      </categories>
      <tags>
        <tag>UAV</tag>
        <tag>无人机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[儿童自我保护教育]]></title>
    <url>%2Farticle%2FChildEducation.html</url>
    <content type="text"><![CDATA[因为最近的携程幼儿园和红黄蓝幼儿园被爆虐童的事件，特整理儿童自我保护教育的资料。 持续更新中]]></content>
      <categories>
        <category>Child Education</category>
      </categories>
      <tags>
        <tag>儿童</tag>
        <tag>自我保护</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab常用命令速查]]></title>
    <url>%2Farticle%2FMatlabFunctionl.html</url>
    <content type="text"><![CDATA[Matlab常用命令速查表 这里只给出了函数的名字及其功能，至于其格式及其调用方法，请参阅MATLAB的在线帮助（只需要在命令窗口输入“help fun_name”） 我觉得学习Matlab（包括任何语言）需注意的是： 必须要有毅力。贵在坚持和积累！就像学习英语那样。 要敢于开口请教别人，向你的师兄，向网络上的Matlab高手咨询，共同讨论。 勤于思考，勤于记忆，勤于动手。对许多问题的探索一定要用自己的大脑去想，直到明白了为止，和其他其他程序设计一样，需要记忆的东西还是记忆，这样可以节省时间。程序设计是实践性和操作性很强的事情，需要你自己亲自动手。 最为重要的就是会在线帮助，但是上面写的都是英语，需要一定的英语基础。实在不懂就安装一个Google翻译之类的软件。 该表主要是为了便于应用时查阅，限于个人水平和资料，错误之处，望批评指正，不胜感激！**]]></content>
      <categories>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab画图函数]]></title>
    <url>%2Farticle%2FMatlabPlotFunction.html</url>
    <content type="text"><![CDATA[Matlab 在线帮助文档，]“MATLAB/Graphics”。 类别 Function 图 维度 描述 曲线 plot 2 绘制曲线，相邻点之间被插值 fplot 输入函数或函数句柄、自变量取值区间，绘制曲线 plotyy 2 双纵坐标图，两个纵坐标的数量级不同 plot3 3 绘制3D曲线 loglog 2 X,Y坐标都按对数缩放 semilogx 2 仅X坐标按对数缩放 semilogy 2 仅Y左边按对数缩放 errorbar 2 误差条形图，见wikipedia“Error bar” 条形直方面域 bar 2 条形图（垂直），分为grouped和stacked风格 bar3 3 3D条形图（垂直） barh 2 水平条形图，分为grouped和stacked风格 bar3h 3 3D水平条形图 hist 2 频数直方图 histc 输入数据和区间，返回数据落在每个区间的频数 pareto 2 帕累托图（柏拉图），见wikipedia“Pareto Chart” area 2 填充区域图，曲线和X轴之间被填充 pie 2 饼图，用于表示比例 pie3 3 3D饼图 极坐标 polar 2 极坐标图，以极坐标绘制曲线 rose 2 角直方图（频数扇形图） 离散数据 stem 2 杆图，对每个数据，从X轴伸出一条垂直线，顶端画圆圈 stem3 3 3D杆图 stairs 2 阶梯图，相邻点间不进行插值 scatter 2 散点图（气泡图），绘制一系列散点 scatter3 3 3D散点图 spy 2 稀疏模式（sparsity pattern）图，对矩阵非0的地方绘制散点 plotmatrix 2 将矩阵绘制为散点图或散点图和直方图 等高线 contour 2 等高线图，二维函数的等值线 contour3 3 3D等高线图，三维函数（空间函数）的等值线 contourf 2 填充的等高线图 contourc 等高线计算 曲面网格 surf 3 曲面图，和mesh的区别是，surf在小矩形上做颜色插值 surfl 3 在surf基础上，加入光照 surfc 3 在surf基础上，在底部绘制等高线图 surfnorm 3 在surf基础上，每个面绘制法线 surface 低层次曲面绘制函数 mesh 3 网格图，在行和列上绘制一系列曲线，构成网格 meshc 3 在mesh基础上，在底部绘制等高线图 meshz 3 在mesh基础上，在网格四周绘制“帘子” waterfall 3 瀑布图，类似于meshz函数，但在矩阵的列之间不生成线 ribbon 3 带图，绘制一定宽度的带，相当于将二维曲线沿着垂直平面方向拉开一定宽度形成三维图形 pcolor 2 伪彩图，根据矩阵的“相邻四个点”的值对应颜色插值得到小矩形颜色 peaks Example function of two variables cylinder Generate cylinder ellipsoid Generate ellipsoid sphere Generate sphere surf2patch Convert surface data to patch data 标量场体数据 slice 3 体积切片图，对体数据进行切片观察 contour-slice 3 切片等高线图，体数据在切片平面中的等值线 flow Simple function of three variables isosurface Extract isosurface data from volume data isocaps Compute isosurface end-cap geometry isocolors Calculate isosurface and patch colors isonormals Compute normals of isosurface vertices reduce-patch Reduce number of patch faces reduce-volume Reduce number of elements in volume data set shrinkfaces Reduce size of patch faces smooth3 Smooth 3-D data subvolume Extract subset of volume data set volume bounds Coordinate and color limits for volume data 向量场体向量数据 feather 2 羽状图，以X轴上的点为起点绘制一系列向量 compass 2 射线图，以原点为起点绘制一系列向量 quiver 2 矢量场图，以采样点为起点绘制一系列向量 quiver3 3 3D矢量场图 streamslice 3 绘制流场（三维向量函数）在切片平面中的流线 streamline 3 绘制流场的流线（类似于磁感线），起点由数据指定 coneplot 3 绘制三维圆锥，圆锥的起点由数据指定，方向和大小由流场指定 stream-particles 3 绘制流场marker粒子 stream-ribbon 3 绘制流场ribbon图 streamtube 3 绘制流场流管 curl Compute curl and angular velocity of vector field divergence Compute divergence of vector field interp-stream-speed Interpolate stream-line vertices from flow speed stream2 Compute 2-D streamline data stream3 Compute 3-D streamline data 多边形 fill 2 绘制填充的多边形 fill3 3 3D填充多边形 patch 2,3 绘制一个或多个填充多边形 Easy-to-use ezplot 2 Easy-to-use版绘图函数，这类函数传入要绘制的函数或函数句柄，以及自变量的定义域，调用具体函数绘图。例如ezplot的一种调用方式是：ezplot(fun,[xmin,xmax]) ezplot3 3 ezpolar 2 ezcontour 2 ezcontourf 2 ezsurf 3 ezsurfc 3 ezmesh 3 ezmeshc 3 动画 comet 2 彗星图，绘制一个类似彗星运动的动画，头部是圆圈，运动轨迹遍历数据 comet3 3 3D彗星图 image 2 将矩阵绘制成图像 movie Play recorded movie frames noanimate Change EraseMode of all objects to normal drawnow Update figure window and execute pending callbacks refreshdata Refresh data in graph when data source is specified frame2im Return image data associated with movie frame getframe Capture movie frame im2frame Convert image to movie frame]]></content>
      <categories>
        <category>Matlab</category>
      </categories>
      <tags>
        <tag>Matlab</tag>
        <tag>画图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2Farticle%2FMarkdownGrammar.html</url>
    <content type="text"><![CDATA[Markdown 是一种轻量级标记语言，它用简洁的语法代替排版，使我们专心于码字。它的目标是实现易读易写，成为一种适用于网络的书写语言。同时，Markdown支持嵌入html标签。 个人使用的编辑器为：Typora，轻量、跨平台、方便！ Markdown的常用语法标题Markdown 标题支持两种形式： 1、用#标记 在 标题开头 加上1~6个#，依次代表一级标题、二级标题….六级标题 123456# 一级标题## 二级标题### 三级标题##### 四级标题###### 五级标题###### 六级标题 2、用=和-标记 在 标题底下 加上任意个=代表一级标题，-代表二级标题 12345一级标题======二级标题---------- 列表Markdown 支持有序列表和无序列表。 无序列表使用-、+和*作为列表标记： 1234567891011- Red- Green- Blue* Red* Green* Blue+ Red+ Green+ Blue 效果如下： Red Green Blue 有序列表则使用数字加英文句点.来表示： 1231. Red2. Green3. Blue 效果如下： Red Green Blue 注意：Markdown使用#、+、*、. 等符号来标记， 符号后面必须跟上 至少1个 空格才有效！ 引用引用以&gt;来表示，引用中支持多级引用、标题、列表、代码块、分割线等常规语法。 常见的引用写法： 123456789101112131415161718&gt; 这是一段引用 //在`&gt;`后面有 1 个空格&gt; &gt; 这是引用的代码块形式 //在`&gt;`后面有 5 个空格&gt; &gt; 代码例子：&gt; protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); &#125;&gt; 一级引用&gt; &gt; 二级引用&gt; &gt; &gt; 三级引用&gt; &gt; 1. 这是第一行列表项&gt; 2. 这是第二行列表项 效果如下： 这是一段引用 12&gt; 这是引用的代码块形式 //在`&gt;`后面有 5 个空格&gt; &gt; 代码例子： 1234protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main);&#125; 一级引用 二级引用 三级引用 这是第一行列表项 这是第二行列表项 以下是分割线 强调两个*或-代表加粗，一个*或-代表斜体，~~代表删除。 12345**加粗文本** 或者 __加粗文本__*斜体文本* 或者_斜体文本_~~删除文本~~ 效果如下： 加粗文本 或者 加粗文本 斜体文本 或者 斜体文本 删除文本 图片与链接图片与链接的语法很像，区别在一个 ! 号。二者格式： 123图片：![]() ![图片文本(可忽略)](图片地址)链接：[]() [链接文本](链接地址) 插入视频注：Markdown 语法是不支持直接插入视频的普遍的做法是 插入HTML的 iframe 框架，通过网站自带的分享功能获取，如果没有可以尝试第二种方法第二是伪造播放界面，实质是插入视频图片，然后通过点击跳转到相关页面. 代码1注：多数第三方平台不支持插入&lt;iframe&gt;视频 12&lt;iframe height=498 width=510 src=&apos;http://player.youku.com/embed/XMzUxNDI3MTI3Mg==&apos; frameborder=0 &apos;allowfullscreen&apos;&gt;&lt;/iframe&gt;&lt;iframe src=&quot;http://open.iqiyi.com/developer/player_js/coopPlayerIndex.html?vid=fdf51f4dd55ed6ccf7b01e2cc8ecb40b&amp;tvId=849094800&amp;accessToken=2.f22860a2479ad60d8da7697274de9346&amp;appKey=3955c3425820435e86d0f4cdfe56f5e7&amp;appId=1368&amp;height=100%&amp;width=100%&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt;&lt;/iframe&gt; 演示 代码代码分为行内代码和代码块。 行内代码使用 代码 标识，可嵌入文字中，使用``标识 代码块使用4个空格或3个 `标识 1print "hello world" 表格表格对齐格式 居左：:---- 居中：:----:或----- 居右：----: 例子： 123456|标题|标题|标题||:---|:---:|---:||居左测试文本|居中测试文本|居右测试文本||居左测试文本1|居中测试文本2|居右测试文本3||居左测试文本11|居中测试文本22|居右测试文本33||居左测试文本111|居中测试文本222|居右测试文本333| 效果如下： 标题 标题 标题 居左测试文本 居中测试文本 居右测试文本 居左测试文本1 居中测试文本2 居右测试文本3 居左测试文本11 居中测试文本22 居右测试文本33 居左测试文本111 居中测试文本222 居右测试文本333 分隔线在一行中用三个以上的*、-、_来建立一个分隔线，行内不能有其他东西。也可以在符号间插入空格。 12345***---___* * * 效果均为一条分割线： 脚注(注解)使用[^]来定义脚注： 123这是一个脚注的例子[^1][^1]: 这里是脚注 效果如下： 这是一个脚注的例子[1] 常用弥补Markdown的Html标签字体12&lt;font face=&quot;微软雅黑&quot; color=&quot;red&quot; size=&quot;6&quot;&gt;字体及字体颜色和大小&lt;/font&gt;&lt;font color=&quot;#0000ff&quot;&gt;字体颜色&lt;/font&gt; 效果如下： 字体及字体颜色和大小 字体颜色 换行1使用html标签`&lt;br/&gt;`&lt;br/&gt;换行 效果如下： 使用html标签&lt;br/&gt;换行 文本对齐方式123&lt;p align=&quot;left&quot;&gt;居左文本&lt;/p&gt;&lt;p align=&quot;center&quot;&gt;居中文本&lt;/p&gt;&lt;p align=&quot;right&quot;&gt;居右文本&lt;/p&gt; 效果如下： 居左文本 居中文本 居右文本 下划线1&lt;u&gt;下划线文本&lt;/u&gt; 效果如下： 下划线文本 That’s all, Enjoy it! 这里是脚注 ↩]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu平台Hexo安装]]></title>
    <url>%2Farticle%2FUnderUbuntuInstallHexo.html</url>
    <content type="text"><![CDATA[安装node.js12curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -sudo apt-get install nodejs 将官方源替换为淘宝源12sudo npm install -g cnpm --registry=https://registry.npm.taobao.orgsudo cnpm install -g hexo-cli 初始化123hexo init 文件夹的名字(假设为BLOG)cd BLOGnpm install //安装依赖 Git配置发布123456hexo new post "newPost"hexo c //clean, 清空generate生成器的文件hexo g //generate ,编译成静态文件hexo d //deploy, 部署网站 hexo s //server, 本地运行]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Ubuntu</tag>
        <tag>GitHub</tag>
      </tags>
  </entry>
</search>
